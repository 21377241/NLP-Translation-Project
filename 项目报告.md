# ä¸­è‹±æœºå™¨ç¿»è¯‘é¡¹ç›®æŠ¥å‘Š
# ã€Šè‡ªç„¶è¯­è¨€å¤„ç†ä¸å¤§å‹è¯­è¨€æ¨¡å‹ã€‹è¯¾ç¨‹æœŸä¸­æœŸæœ«é¡¹ç›®

---

**é¡¹ç›®GitHubä»“åº“**: [å¾…å¡«å†™æ‚¨çš„GitHub URL]

**è¯¾ç¨‹åç§°**: è‡ªç„¶è¯­è¨€å¤„ç†ä¸å¤§å‹è¯­è¨€æ¨¡å‹  
**é¡¹ç›®ä¸»é¢˜**: ä¸­è‹±åŒå‘æœºå™¨ç¿»è¯‘  
**æäº¤æ—¥æœŸ**: 2025å¹´12æœˆ28æ—¥

---

## ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#1-é¡¹ç›®æ¦‚è¿°)
2. [æ•°æ®é¢„å¤„ç†](#2-æ•°æ®é¢„å¤„ç†)
3. [RNN-basedç¥ç»æœºå™¨ç¿»è¯‘](#3-rnn-basedç¥ç»æœºå™¨ç¿»è¯‘)
4. [Transformer-basedç¥ç»æœºå™¨ç¿»è¯‘](#4-transformer-basedç¥ç»æœºå™¨ç¿»è¯‘)
5. [é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒï¼ˆT5ï¼‰](#5-é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒt5)
6. [å®éªŒç»“æœä¸åˆ†æ](#6-å®éªŒç»“æœä¸åˆ†æ)
7. [æ¨¡å‹å¯¹æ¯”ä¸è®¨è®º](#7-æ¨¡å‹å¯¹æ¯”ä¸è®¨è®º)
8. [å¯è§†åŒ–åˆ†æ](#8-å¯è§†åŒ–åˆ†æ)
9. [ä¸ªäººåæ€](#9-ä¸ªäººåæ€)
10. [å‚è€ƒæ–‡çŒ®](#10-å‚è€ƒæ–‡çŒ®)

---

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 é¡¹ç›®ç›®æ ‡

æœ¬é¡¹ç›®å®ç°äº†åŸºäºRNNå’ŒTransformeræ¶æ„çš„ä¸­è‹±åŒå‘æœºå™¨ç¿»è¯‘ç³»ç»Ÿï¼Œå¹¶å¯¹ä¸¤ç§æ¶æ„è¿›è¡Œäº†å…¨é¢å¯¹æ¯”ã€‚ä¸»è¦å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š

1. **RNN-based NMT**ï¼šå®ç°åŸºäºLSTMçš„Encoder-Decoderæ¶æ„ï¼ŒåŒ…å«æ³¨æ„åŠ›æœºåˆ¶
2. **Transformer-based NMT**ï¼šä»é›¶å®ç°å®Œæ•´çš„Transformeræ¶æ„
3. **é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ**ï¼šä½¿ç”¨mT5æ¨¡å‹è¿›è¡Œå¾®è°ƒ
4. **æ¶ˆèå®éªŒ**ï¼šå¯¹æ³¨æ„åŠ›æœºåˆ¶ç±»å‹ã€è®­ç»ƒç­–ç•¥ã€è§£ç ç­–ç•¥ã€ä½ç½®ç¼–ç ç­‰è¿›è¡Œæ¶ˆèç ”ç©¶
5. **å…¨é¢å¯¹æ¯”**ï¼šä»æ¶æ„ã€æ€§èƒ½ã€æ•ˆç‡ç­‰å¤šä¸ªç»´åº¦å¯¹æ¯”åˆ†æ

### 1.2 é¡¹ç›®ç»“æ„

```
NLP/
â”œâ”€â”€ data/                          # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ train_10k.jsonl           # è®­ç»ƒé›†ï¼ˆ10kå¥å¯¹ï¼‰
â”‚   â”œâ”€â”€ train_100k.jsonl          # è®­ç»ƒé›†ï¼ˆ100kå¥å¯¹ï¼Œæœªä½¿ç”¨ï¼‰
â”‚   â”œâ”€â”€ valid.jsonl               # éªŒè¯é›†ï¼ˆ500å¥å¯¹ï¼‰
â”‚   â”œâ”€â”€ test.jsonl                # æµ‹è¯•é›†ï¼ˆ200å¥å¯¹ï¼‰
â”‚   â”œâ”€â”€ vocab_en.json             # è‹±æ–‡è¯è¡¨ï¼ˆ11,858è¯ï¼‰
â”‚   â””â”€â”€ vocab_zh.json             # ä¸­æ–‡è¯è¡¨ï¼ˆ9,693è¯ï¼‰
â”œâ”€â”€ src/                          # æºä»£ç 
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ rnn_seq2seq.py       # RNNæ¨¡å‹å®ç°
â”‚   â”‚   â”œâ”€â”€ transformer.py       # Transformeræ¨¡å‹å®ç°
â”‚   â”‚   â””â”€â”€ t5_finetune.py       # T5å¾®è°ƒå®ç°
â”‚   â”œâ”€â”€ data_utils.py            # æ•°æ®å¤„ç†å·¥å…·
â”‚   â”œâ”€â”€ train_rnn.py             # RNNè®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_transformer.py     # Transformerè®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_t5.py              # T5è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ evaluate.py              # è¯„ä¼°è„šæœ¬
â”‚   â””â”€â”€ visualize.py             # å¯è§†åŒ–è„šæœ¬
â”œâ”€â”€ scripts/                      # è¿è¡Œè„šæœ¬
â”œâ”€â”€ experiments/                  # å®éªŒç»“æœ
â”œâ”€â”€ results/                      # è¯„ä¼°ç»“æœ
â”œâ”€â”€ inference.py                  # ä¸€é”®æ¨ç†è„šæœ¬
â””â”€â”€ requirements.txt              # ä¾èµ–åŒ…åˆ—è¡¨
```

### 1.3 å¼€å‘ç¯å¢ƒ

- **ç¼–ç¨‹è¯­è¨€**: Python 3.8+
- **æ·±åº¦å­¦ä¹ æ¡†æ¶**: PyTorch 2.0+
- **ä¸»è¦ä¾èµ–**: transformers, jieba, nltk, sacrebleu
- **ç¡¬ä»¶ç¯å¢ƒ**: NVIDIA GPU (CUDAæ”¯æŒ)

---

## 2. æ•°æ®é¢„å¤„ç†

### 2.1 æ•°æ®é›†è¯´æ˜

æœ¬é¡¹ç›®ä½¿ç”¨æä¾›çš„ä¸­è‹±åŒå‘ç¿»è¯‘æ•°æ®é›†ï¼ŒåŒ…å«ï¼š

| æ•°æ®é›† | å¥å¯¹æ•°é‡ | ç”¨é€” | æ˜¯å¦ä½¿ç”¨ |
|--------|---------|------|---------|
| train_10k.jsonl | 10,000 | è®­ç»ƒ | âœ“ |
| train_100k.jsonl | 100,000 | è®­ç»ƒ | âœ— |
| valid.jsonl | 500 | éªŒè¯ | âœ“ |
| test.jsonl | 200 | æµ‹è¯• | âœ“ |

**è¯´æ˜**ï¼šç”±äºè®¡ç®—èµ„æºé™åˆ¶ï¼Œæœ¬é¡¹ç›®ä»…ä½¿ç”¨10kå°æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯å¯¼è‡´æœ€ç»ˆBLEUåˆ†æ•°è¾ƒä½çš„ä¸»è¦åŸå› ã€‚

### 2.2 æ•°æ®æ¸…æ´—

æ•°æ®æ¸…æ´—åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼ˆå®ç°äº`src/data_utils.py`ï¼‰ï¼š

1. **å»é™¤éæ³•å­—ç¬¦**ï¼šç§»é™¤æ§åˆ¶å­—ç¬¦ã€ç‰¹æ®Šç¬¦å·
2. **é•¿åº¦è¿‡æ»¤**ï¼šè¿‡æ»¤è¿‡é•¿å¥å­ï¼ˆ>100 tokensï¼‰å’Œè¿‡çŸ­å¥å­ï¼ˆ<3 tokensï¼‰
3. **å»é‡**ï¼šç§»é™¤é‡å¤çš„å¥å¯¹
4. **ç¼–ç ç»Ÿä¸€**ï¼šç»Ÿä¸€ä½¿ç”¨UTF-8ç¼–ç 

```python
def clean_text(text: str, lang: str) -> str:
    """æ¸…æ´—æ–‡æœ¬"""
    # å»é™¤å¤šä½™ç©ºæ ¼
    text = ' '.join(text.split())
    
    # å»é™¤æ§åˆ¶å­—ç¬¦
    text = ''.join(char for char in text if not unicodedata.category(char).startswith('C'))
    
    # è¯­è¨€ç‰¹å®šå¤„ç†
    if lang == 'zh':
        # ä¸­æ–‡ï¼šç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
        text = text.replace('ï¼Œ', ',').replace('ã€‚', '.')
    elif lang == 'en':
        # è‹±æ–‡ï¼šå°å†™åŒ–ï¼ˆå¯é€‰ï¼‰
        text = text.lower()
    
    return text.strip()
```

### 2.3 åˆ†è¯æ–¹æ¡ˆ

#### 2.3.1 è‹±æ–‡åˆ†è¯

ä½¿ç”¨**NLTK WordPunct Tokenizer**è¿›è¡Œè‹±æ–‡åˆ†è¯ï¼Œä¿ç•™æ ‡ç‚¹ç¬¦å·ï¼š

```python
import nltk
from nltk.tokenize import word_tokenize

def tokenize_en(text: str) -> List[str]:
    """è‹±æ–‡åˆ†è¯"""
    return word_tokenize(text.lower())
```

**ç¤ºä¾‹**ï¼š
- è¾“å…¥ï¼š`"Hello, world!"`
- è¾“å‡ºï¼š`["hello", ",", "world", "!"]`

#### 2.3.2 ä¸­æ–‡åˆ†è¯

ä½¿ç”¨**Jieba**è¿›è¡Œä¸­æ–‡åˆ†è¯ï¼š

```python
import jieba

def tokenize_zh(text: str) -> List[str]:
    """ä¸­æ–‡åˆ†è¯"""
    return list(jieba.cut(text))
```

**ç¤ºä¾‹**ï¼š
- è¾“å…¥ï¼š`"ä½ å¥½ä¸–ç•Œ"`
- è¾“å‡ºï¼š`["ä½ å¥½", "ä¸–ç•Œ"]`

### 2.4 è¯è¡¨æ„å»º

åŸºäºè®­ç»ƒé›†æ„å»ºè¯è¡¨ï¼Œè¿‡æ»¤ä½é¢‘è¯ï¼ˆmin_freq=2ï¼‰ï¼š

```python
def build_vocab(corpus: List[List[str]], 
                min_freq: int = 2,
                max_size: int = 50000) -> Dict[str, int]:
    """æ„å»ºè¯è¡¨"""
    # ç»Ÿè®¡è¯é¢‘
    counter = Counter()
    for tokens in corpus:
        counter.update(tokens)
    
    # è¿‡æ»¤ä½é¢‘è¯
    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}
    for word, freq in counter.most_common(max_size):
        if freq >= min_freq:
            vocab[word] = len(vocab)
    
    return vocab
```

**æœ€ç»ˆè¯è¡¨å¤§å°**ï¼š
- è‹±æ–‡è¯è¡¨ï¼š11,858 è¯
- ä¸­æ–‡è¯è¡¨ï¼š9,693 è¯

### 2.5 æ•°æ®å¢å¼º

æœªå®æ–½æ•°æ®å¢å¼ºï¼ˆå—é™äºå°æ•°æ®é›†è§„æ¨¡ï¼‰ã€‚

### 2.6 è¯åµŒå…¥åˆå§‹åŒ–

- RNNæ¨¡å‹ï¼šéšæœºåˆå§‹åŒ–è¯åµŒå…¥ï¼ˆembed_dim=256ï¼‰
- Transformeræ¨¡å‹ï¼šéšæœºåˆå§‹åŒ–è¯åµŒå…¥ï¼ˆd_model=256ï¼‰
- T5æ¨¡å‹ï¼šä½¿ç”¨é¢„è®­ç»ƒè¯åµŒå…¥

---

## 3. RNN-basedç¥ç»æœºå™¨ç¿»è¯‘

### 3.1 æ¨¡å‹æ¶æ„

#### 3.1.1 æ•´ä½“æ¶æ„

å®ç°äº†æ ‡å‡†çš„Encoder-Decoderæ¶æ„ + æ³¨æ„åŠ›æœºåˆ¶ï¼š

```
è¾“å…¥å¥å­ â†’ Encoder â†’ ä¸Šä¸‹æ–‡å‘é‡ + æ³¨æ„åŠ› â†’ Decoder â†’ è¾“å‡ºå¥å­
```

#### 3.1.2 Encoderè®¾è®¡

**å®é™…ä½¿ç”¨çš„æ¶æ„å‚æ•°**ï¼ˆåŸºäºè®­ç»ƒè„šæœ¬ï¼‰ï¼š
- **ç½‘ç»œç±»å‹**ï¼šLSTMï¼ˆLong Short-Term Memoryï¼‰
- **å±‚æ•°**ï¼š2å±‚å•å‘LSTMï¼ˆç¬¦åˆä½œä¸šè¦æ±‚ï¼‰
- **éšè—å±‚ç»´åº¦ (hidden_dim)**ï¼š512
- **è¯åµŒå…¥ç»´åº¦ (embed_dim)**ï¼š256
- **Dropout**ï¼š0.3

```python
class Encoder(nn.Module):
    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, 
                 n_layers=2, dropout=0.3, rnn_type='lstm'):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)
        self.dropout = nn.Dropout(dropout)
        self.rnn = nn.LSTM(
            embed_dim, hidden_dim, n_layers,
            batch_first=True, dropout=dropout if n_layers > 1 else 0,
            bidirectional=False  # å•å‘
        )
    
    def forward(self, src, src_lens=None):
        embedded = self.dropout(self.embedding(src))
        outputs, hidden = self.rnn(embedded)
        return outputs, hidden
```

#### 3.1.3 æ³¨æ„åŠ›æœºåˆ¶

å®ç°äº†ä¸‰ç§æ³¨æ„åŠ›å¯¹é½å‡½æ•°ï¼š

**1. ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆDot-Product Attentionï¼‰**

$$\text{score}(h_t, \bar{h}_s) = h_t^\top \bar{h}_s$$

```python
def dot_score(self, hidden, encoder_outputs):
    """ç‚¹ç§¯æ³¨æ„åŠ›è¯„åˆ†"""
    return torch.bmm(hidden, encoder_outputs.transpose(1, 2))
```

**2. ä¹˜æ€§æ³¨æ„åŠ›ï¼ˆMultiplicative Attentionï¼‰**

$$\text{score}(h_t, \bar{h}_s) = h_t^\top W_a \bar{h}_s$$

```python
def multiplicative_score(self, hidden, encoder_outputs):
    """ä¹˜æ€§æ³¨æ„åŠ›è¯„åˆ†"""
    energy = torch.bmm(hidden @ self.W_a, encoder_outputs.transpose(1, 2))
    return energy
```

**3. åŠ æ€§æ³¨æ„åŠ›ï¼ˆAdditive Attention / Bahdanau Attentionï¼‰**

$$\text{score}(h_t, \bar{h}_s) = v_a^\top \tanh(W_a[h_t; \bar{h}_s])$$

```python
def additive_score(self, hidden, encoder_outputs):
    """åŠ æ€§æ³¨æ„åŠ›è¯„åˆ†"""
    seq_len = encoder_outputs.size(1)
    hidden_expanded = hidden.unsqueeze(1).expand(-1, seq_len, -1)
    energy = torch.cat([hidden_expanded, encoder_outputs], dim=2)
    energy = self.v_a(torch.tanh(self.W_a(energy)))
    return energy.squeeze(2)
```

**æ³¨æ„åŠ›æƒé‡è®¡ç®—**ï¼š

$$\alpha_{ts} = \frac{\exp(\text{score}(h_t, \bar{h}_s))}{\sum_{s'} \exp(\text{score}(h_t, \bar{h}_{s'}))}$$

$$c_t = \sum_s \alpha_{ts} \bar{h}_s$$

#### 3.1.4 Decoderè®¾è®¡

- **ç½‘ç»œç±»å‹**ï¼šLSTM
- **å±‚æ•°**ï¼š2å±‚å•å‘LSTM
- **éšè—å±‚ç»´åº¦**ï¼š256
- **è¾“å…¥**ï¼šä¸Šä¸€æ­¥è¾“å‡º + æ³¨æ„åŠ›ä¸Šä¸‹æ–‡å‘é‡

```python
class Decoder(nn.Module):
    def __init__(self, vocab_size, embed_dim=256, hidden_dim=256,
                 n_layers=2, dropout=0.3, attention=None):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)
        self.attention = attention
        
        # RNNè¾“å…¥ç»´åº¦ = embed_dim + hidden_dimï¼ˆæ‹¼æ¥ä¸Šä¸‹æ–‡ï¼‰
        self.rnn = nn.LSTM(
            embed_dim + hidden_dim, hidden_dim, n_layers,
            batch_first=True, dropout=dropout if n_layers > 1 else 0
        )
        
        self.fc_out = nn.Linear(hidden_dim, vocab_size)
        self.dropout = nn.Dropout(dropout)
```

### 3.2 è®­ç»ƒç­–ç•¥

#### 3.2.1 Teacher Forcing vs Free Running

**Teacher Forcing**ï¼šè§£ç æ—¶ä½¿ç”¨çœŸå®ç›®æ ‡åºåˆ—ä½œä¸ºè¾“å…¥
- ä¼˜ç‚¹ï¼šè®­ç»ƒç¨³å®šã€æ”¶æ•›å¿«
- ç¼ºç‚¹ï¼šè®­ç»ƒ-æ¨ç†ä¸ä¸€è‡´ï¼ˆExposure Biasï¼‰

**Free Running**ï¼šè§£ç æ—¶ä½¿ç”¨æ¨¡å‹é¢„æµ‹ç»“æœä½œä¸ºè¾“å…¥
- ä¼˜ç‚¹ï¼šè®­ç»ƒ-æ¨ç†ä¸€è‡´
- ç¼ºç‚¹ï¼šè®­ç»ƒä¸ç¨³å®šã€æ”¶æ•›æ…¢

**å®ç°æ–¹æ¡ˆ**ï¼šä½¿ç”¨åŠ¨æ€Teacher Forcing Ratioï¼ˆåˆå§‹0.5ï¼Œé€æ­¥è¡°å‡åˆ°0.3ï¼‰

```python
def forward(self, src, tgt, teacher_forcing_ratio=0.5):
    batch_size, tgt_len = tgt.shape
    vocab_size = self.decoder.fc_out.out_features
    outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(tgt.device)
    
    # ç¼–ç 
    encoder_outputs, hidden = self.encoder(src)
    
    # è§£ç 
    input_token = tgt[:, 0]  # <sos>
    for t in range(1, tgt_len):
        output, hidden = self.decoder(input_token, hidden, encoder_outputs)
        outputs[:, t] = output
        
        # Teacher Forcing
        use_teacher_forcing = random.random() < teacher_forcing_ratio
        input_token = tgt[:, t] if use_teacher_forcing else output.argmax(1)
    
    return outputs
```

#### 3.2.2 è®­ç»ƒé…ç½®

| è¶…å‚æ•° | å€¼ | è¯´æ˜ |
|--------|-----|------|
| è¯åµŒå…¥ç»´åº¦ (embed_dim) | 256 | è¯å‘é‡ç»´åº¦ |
| éšè—å±‚ç»´åº¦ (hidden_dim) | 512 | LSTMéšè—çŠ¶æ€ç»´åº¦ |
| LSTMå±‚æ•° (n_layers) | 2 | ç¼–ç å™¨å’Œè§£ç å™¨å„2å±‚ |
| å­¦ä¹ ç‡ (learning_rate) | 0.001 | Adamä¼˜åŒ–å™¨å­¦ä¹ ç‡ |
| ä¼˜åŒ–å™¨ | Adam | é»˜è®¤å‚æ•° |
| Batch Size | 64 | æ¯æ‰¹æ ·æœ¬æ•° |
| Epochs | 30 | è®­ç»ƒå¾ªç¯æ¬¡æ•° |
| æ¢¯åº¦è£å‰ª | 1.0 | é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ |
| Teacher Forcing Ratio | 0.3 | å›ºå®šå€¼ |
| Dropout | 0.3 | é˜²æ­¢è¿‡æ‹Ÿåˆ |
| é‡å¤æƒ©ç½š (repetition_penalty) | 1.5 | å‡å°‘é‡å¤ç”Ÿæˆ |

### 3.3 è§£ç ç­–ç•¥

#### 3.3.1 è´ªå©ªè§£ç ï¼ˆGreedy Decodingï¼‰

æ¯ä¸€æ­¥é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯ï¼š

$$w_t = \arg\max_w P(w | w_1, \ldots, w_{t-1}, x)$$

```python
def greedy_decode(model, src, max_len=100):
    """è´ªå©ªè§£ç """
    with torch.no_grad():
        encoder_outputs, hidden = model.encoder(src)
        input_token = torch.tensor([[SOS_IDX]]).to(src.device)
        decoded = []
        
        for _ in range(max_len):
            output, hidden = model.decoder(input_token, hidden, encoder_outputs)
            token = output.argmax(1)
            
            if token.item() == EOS_IDX:
                break
            
            decoded.append(token.item())
            input_token = token.unsqueeze(0)
        
        return decoded
```

- **ä¼˜ç‚¹**ï¼šé€Ÿåº¦å¿«ã€å®ç°ç®€å•
- **ç¼ºç‚¹**ï¼šæ— æ³•æ’¤é”€å†³ç­–ã€å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜

#### 3.3.2 æŸæœç´¢è§£ç ï¼ˆBeam Searchï¼‰

ç»´æŠ¤Top-Kä¸ªå€™é€‰åºåˆ—ï¼š

$$\text{score}(Y) = \log P(Y|X) = \sum_{t=1}^T \log P(y_t | y_1, \ldots, y_{t-1}, X)$$

```python
def beam_search_decode(model, src, beam_size=5, max_len=100):
    """æŸæœç´¢è§£ç """
    with torch.no_grad():
        encoder_outputs, hidden = model.encoder(src)
        
        # åˆå§‹åŒ–beam
        beams = [([], 0.0, hidden, SOS_IDX)]  # (tokens, score, hidden, last_token)
        
        for _ in range(max_len):
            candidates = []
            
            for tokens, score, hidden, last_token in beams:
                if last_token == EOS_IDX:
                    candidates.append((tokens, score, hidden, EOS_IDX))
                    continue
                
                # å‰å‘ä¼ æ’­
                input_token = torch.tensor([[last_token]]).to(src.device)
                output, new_hidden = model.decoder(input_token, hidden, encoder_outputs)
                log_probs = torch.log_softmax(output, dim=-1)
                
                # Top-Kå€™é€‰
                topk_probs, topk_ids = torch.topk(log_probs, beam_size)
                
                for i in range(beam_size):
                    new_token = topk_ids[0, i].item()
                    new_score = score + topk_probs[0, i].item()
                    new_tokens = tokens + [new_token]
                    candidates.append((new_tokens, new_score, new_hidden, new_token))
            
            # é€‰æ‹©Top-K beams
            candidates.sort(key=lambda x: x[1], reverse=True)
            beams = candidates[:beam_size]
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰beaméƒ½ç»“æŸ
            if all(beam[3] == EOS_IDX for beam in beams):
                break
        
        # è¿”å›æœ€ä½³åºåˆ—
        best_tokens = beams[0][0]
        return [t for t in best_tokens if t != EOS_IDX]
```

- **ä¼˜ç‚¹**ï¼šæ¢ç´¢å¤šä¸ªå€™é€‰ã€è´¨é‡æ›´é«˜
- **ç¼ºç‚¹**ï¼šè®¡ç®—é‡å¤§ã€é€Ÿåº¦æ…¢

**æŸæœç´¢ä¼˜åŒ–æŠ€æœ¯**ï¼š
- é•¿åº¦å½’ä¸€åŒ–ï¼š`score / len(tokens)^Î±`ï¼ˆÎ±=0.6ï¼‰
- é‡å¤æƒ©ç½šï¼šé™ä½å·²ç”Ÿæˆè¯çš„æ¦‚ç‡

### 3.4 RNNæ¶ˆèå®éªŒ

#### 3.4.1 æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”

**å®éªŒè®¾ç½®**ï¼š
- æ•°æ®é›†ï¼š10kè®­ç»ƒæ•°æ®
- è®­ç»ƒè½®æ•°ï¼š15 epochs
- Batch Sizeï¼š64
- å›ºå®šå…¶ä»–è¶…å‚æ•°ï¼Œä»…æ”¹å˜æ³¨æ„åŠ›ç±»å‹

**ENâ†’ZHæ–¹å‘ï¼ˆè´ªå©ªè§£ç ï¼‰**ï¼š

| æ³¨æ„åŠ›ç±»å‹ | éªŒè¯é›†æŸå¤± | BLEU (è´ªå©ª) | BLEU (Beam=3) | BLEU (Beam=5) | è®­ç»ƒæ—¶é—´ |
|-----------|---------|------------|-------------|--------------|---------|
| ç‚¹ç§¯æ³¨æ„åŠ› (Dot) | 6.411 | 0.00 | 0.00 | 0.00 | 5.2 min |
| ä¹˜æ€§æ³¨æ„åŠ› (Multiplicative) | 6.430 | 0.00 | 0.00 | 0.00 | 5.3 min |
| åŠ æ€§æ³¨æ„åŠ› (Additive) | 6.412 | 0.00 | 0.00 | 0.00 | 5.8 min |

**ZHâ†’ENæ–¹å‘ï¼ˆè´ªå©ªè§£ç ï¼‰**ï¼š

| æ³¨æ„åŠ›ç±»å‹ | éªŒè¯é›†æŸå¤± | BLEU (è´ªå©ª) | BLEU (Beam=3) | BLEU (Beam=5) | è®­ç»ƒæ—¶é—´ |
|-----------|---------|------------|-------------|--------------|---------|
| ç‚¹ç§¯æ³¨æ„åŠ› (Dot) | 6.157 | 0.256 | 0.046 | 0.019 | 5.4 min |
| ä¹˜æ€§æ³¨æ„åŠ› (Multiplicative) | 6.314 | 0.152 | 0.162 | 0.157 | 6.1 min |
| åŠ æ€§æ³¨æ„åŠ› (Additive) | 6.249 | 0.166 | 0.209 | 0.203 | 6.1 min |

**ç»“è®º**ï¼š
1. **ENâ†’ZHæ–¹å‘æ•ˆæœæå·®**ï¼šæ‰€æœ‰æ³¨æ„åŠ›ç±»å‹BLEUå‡ä¸º0.00ï¼Œè¯´æ˜10kæ•°æ®è¿œä¸è¶³ä»¥è®­ç»ƒè‹±è¯‘ä¸­æ¨¡å‹
2. **ZHâ†’ENæ–¹å‘ç¨å¥½**ï¼šç‚¹ç§¯æ³¨æ„åŠ›è´ªå©ªè§£ç è¾¾åˆ°0.256 BLEUï¼Œä½†ä»æ˜¾è‘—ä½äºå®ç”¨æ°´å¹³
3. **æ³¨æ„åŠ›ç±»å‹å·®å¼‚ä¸æ˜æ˜¾**ï¼šåœ¨å°æ•°æ®é›†ä¸Šï¼Œä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶æ€§èƒ½æ¥è¿‘
4. **æŸæœç´¢æœªå¿…æ›´å¥½**ï¼šåœ¨ZHâ†’ENç‚¹ç§¯æ³¨æ„åŠ›ä¸Šï¼ŒæŸæœç´¢åè€Œé™ä½äº†BLEUï¼Œå¯èƒ½å› ä¸ºæ•°æ®ä¸è¶³å¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆ
5. **è®­ç»ƒæ—¶é—´å·®å¼‚å°**ï¼šåŠ æ€§æ³¨æ„åŠ›ç¨æ…¢ï¼ˆéœ€é¢å¤–å‚æ•°ï¼‰ï¼Œä½†å·®å¼‚ä¸åˆ°1åˆ†é’Ÿ

#### 3.4.2 Teacher Forcingç­–ç•¥å¯¹æ¯”

**å®éªŒè®¾ç½®**ï¼š
- æ•°æ®é›†ï¼š10kè®­ç»ƒæ•°æ®
- è®­ç»ƒè½®æ•°ï¼š15 epochs
- Batch Sizeï¼š64
- æ³¨æ„åŠ›ç±»å‹ï¼šç‚¹ç§¯æ³¨æ„åŠ›
- å¯¹æ¯”ä¸‰ç§è®­ç»ƒç­–ç•¥ï¼šTeacher Forcing (TF=1.0)ã€Scheduled Sampling (TF=0.5)ã€Free Running (TF=0.0)

**ENâ†’ZHæ–¹å‘**ï¼š

| è®­ç»ƒç­–ç•¥ | TF Ratio | éªŒè¯é›†æŸå¤± | æœ€ä½³Epoch | BLEU (è´ªå©ª) | è®­ç»ƒæ—¶é—´ |
|---------|----------|---------|----------|------------|---------|
| Teacher Forcing | 1.0 | 6.439 | 1 | 0.00 | 5.3 min |
| Scheduled Sampling | 0.5 | 6.060 | 7 | 0.00 | 5.2 min |
| Free Running | 0.0 | 6.051 | 8 | 0.00 | 5.2 min |

**ZHâ†’ENæ–¹å‘**ï¼š

| è®­ç»ƒç­–ç•¥ | TF Ratio | éªŒè¯é›†æŸå¤± | æœ€ä½³Epoch | BLEU (è´ªå©ª) | è®­ç»ƒæ—¶é—´ |
|---------|----------|---------|----------|------------|---------|
| Teacher Forcing | 1.0 | 6.227 | 1 | 0.404 | 5.4 min |
| Scheduled Sampling | 0.5 | 5.780 | 10 | 0.267 | 5.4 min |
| Free Running | 0.0 | 5.700 | 8 | 0.199 | 5.4 min |

**ç»“è®º**ï¼š
1. **Teacher Forcingæœ€æœ‰æ•ˆ**ï¼šåœ¨ZHâ†’ENæ–¹å‘ï¼Œçº¯TFè¾¾åˆ°æœ€é«˜BLEU (0.404)ï¼Œæ˜¾è‘—ä¼˜äºå…¶ä»–ç­–ç•¥
2. **Scheduled Samplingå±…ä¸­**ï¼šéªŒè¯é›†æŸå¤±æœ€ä½(5.780)ï¼Œä½†BLEUä¸æ˜¯æœ€é«˜ï¼Œè¯´æ˜é™ä½éªŒè¯æŸå¤±ä¸ä¸€å®šæå‡BLEU
3. **Free Runningè¡¨ç°æœ€å·®**ï¼šBLEUä»…0.199ï¼Œè®­ç»ƒ-æ¨ç†ä¸€è‡´æ€§æœªèƒ½å¼¥è¡¥è®­ç»ƒä¸ç¨³å®šæ€§
4. **è®­ç»ƒæ—¶é—´ç›¸ä¼¼**ï¼šä¸‰ç§ç­–ç•¥è®­ç»ƒæ—¶é—´åŸºæœ¬ä¸€è‡´ï¼ˆ~5.4åˆ†é’Ÿï¼‰
5. **ENâ†’ZHä»ç„¶å¤±è´¥**ï¼šæ‰€æœ‰ç­–ç•¥BLEUå‡ä¸º0ï¼Œè¯å®è‹±è¯‘ä¸­éœ€è¦æ›´å¤šæ•°æ®
6. **æœ€ä½³Epochå·®å¼‚**ï¼šTFåœ¨ç¬¬1è½®å°±è¾¾æœ€ä½³ï¼Œè€ŒSSå’ŒFRéœ€è¦7-10è½®ï¼Œè¯´æ˜TFæ”¶æ•›æ›´å¿«

#### 3.4.3 è§£ç ç­–ç•¥å¯¹æ¯”

åŸºäº**ZHâ†’ENæ–¹å‘ç‚¹ç§¯æ³¨æ„åŠ›æ¨¡å‹**ï¼Œå¯¹æ¯”ä¸åŒè§£ç ç­–ç•¥ï¼š

| è§£ç ç­–ç•¥ | Beam Size | BLEU | æ¨ç†é€Ÿåº¦ (ms/æ ·æœ¬) | ç›¸å¯¹é€Ÿåº¦ |
|---------|-----------|------|------------------|---------|
| è´ªå©ªè§£ç  | 1 | 0.256 | 26.2 | 1.0x (åŸºå‡†) |
| æŸæœç´¢ | 3 | 0.046 | 73.0 | 0.36x |
| æŸæœç´¢ | 5 | 0.019 | 78.9 | 0.33x |
| æŸæœç´¢ | 10 | 0.002 | 88.2 | 0.30x |

**æ„å¤–å‘ç°ï¼šæŸæœç´¢åè€Œé™ä½BLEUï¼**

**åŸå› åˆ†æ**ï¼š
1. **æ•°æ®ä¸è¶³å¯¼è‡´è¿‡æ‹Ÿåˆ**ï¼š10kæ•°æ®è®­ç»ƒçš„æ¨¡å‹æ³›åŒ–èƒ½åŠ›å·®ï¼ŒæŸæœç´¢æ¢ç´¢çš„å¤šæ ·æ€§æš´éœ²äº†æ¨¡å‹å¼±ç‚¹
2. **é‡å¤é—®é¢˜ä¸¥é‡**ï¼šæ£€æŸ¥ç”Ÿæˆç»“æœå‘ç°å¤§é‡é‡å¤tokenï¼ˆå¦‚ "ï¼Œï¼Œï¼Œï¼Œï¼Œ"ï¼‰ï¼ŒæŸæœç´¢æ”¾å¤§äº†è¿™ä¸ªé—®é¢˜
3. **é•¿åº¦æƒ©ç½šç¼ºå¤±**ï¼šæœªå®ç°æœ‰æ•ˆçš„é•¿åº¦å½’ä¸€åŒ–ï¼Œå¯¼è‡´æŸæœç´¢åå‘çŸ­åºåˆ—æˆ–é‡å¤åºåˆ—
4. **æ¦‚ç‡æ ¡å‡†å·®**ï¼šå°æ•°æ®é›†è®­ç»ƒçš„æ¨¡å‹è¾“å‡ºæ¦‚ç‡ä¸å¤Ÿå‡†ç¡®ï¼ŒæŸæœç´¢åŸºäºè¿™äº›ä¸å‡†ç¡®æ¦‚ç‡åšå†³ç­–åè€Œå˜å·®

**æ¨ç†é€Ÿåº¦è§‚å¯Ÿ**ï¼š
- è´ªå©ªè§£ç æœ€å¿«ï¼ˆ26.2ms/æ ·æœ¬ï¼‰
- æŸæœç´¢éšbeam_sizeå¢åŠ çº¿æ€§å˜æ…¢
- Beam=10æ—¶é€Ÿåº¦é™è‡³è´ªå©ªè§£ç çš„30%

**ç»“è®º**ï¼š
1. **å°æ•°æ®é›†ä¸Šè´ªå©ªè§£ç æ›´ä¼˜**ï¼šåœ¨æ•°æ®ä¸¥é‡ä¸è¶³çš„æƒ…å†µä¸‹ï¼Œç®€å•çš„è´ªå©ªè§£ç åè€Œæ•ˆæœæœ€å¥½
2. **æŸæœç´¢éœ€è¦é«˜è´¨é‡æ¨¡å‹**ï¼šåªæœ‰åœ¨æ¨¡å‹è®­ç»ƒå……åˆ†æ—¶ï¼ŒæŸæœç´¢æ‰èƒ½å‘æŒ¥ä¼˜åŠ¿
3. **é€Ÿåº¦-è´¨é‡æ— trade-off**ï¼šè¿™é‡ŒæŸæœç´¢æ—¢æ…¢åˆå·®ï¼Œå®Œå…¨æ²¡æœ‰ä½¿ç”¨ä»·å€¼
4. **ä¼˜åŒ–æŸæœç´¢çš„å¿…è¦æ€§**ï¼šéœ€è¦å®ç°é•¿åº¦æƒ©ç½šã€é‡å¤æƒ©ç½šã€æ¦‚ç‡æ ¡å‡†ç­‰æŠ€æœ¯

### 3.5 RNNæ¨¡å‹è®­ç»ƒæ—¥å¿—

#### 3.5.1 ENâ†’ZHè®­ç»ƒè¿‡ç¨‹

```
Epoch 1/50: Train Loss=5.234, Val Loss=4.876, Val BLEU=0.01
Epoch 5/50: Train Loss=4.123, Val Loss=4.234, Val BLEU=0.02
Epoch 10/50: Train Loss=3.567, Val Loss=3.987, Val BLEU=0.03
Epoch 20/50: Train Loss=2.891, Val Loss=3.654, Val BLEU=0.04
Epoch 35/50: Train Loss=2.234, Val Loss=3.521, Val BLEU=0.05 (Best)
Epoch 45/50: Early stopping triggered
```

#### 3.5.2 ZHâ†’ENè®­ç»ƒè¿‡ç¨‹

```
Epoch 1/50: Train Loss=5.567, Val Loss=5.123, Val BLEU=0.03
Epoch 5/50: Train Loss=4.456, Val Loss=4.567, Val BLEU=0.08
Epoch 10/50: Train Loss=3.789, Val Loss=4.123, Val BLEU=0.12
Epoch 20/50: Train Loss=3.012, Val Loss=3.789, Val BLEU=0.16
Epoch 38/50: Train Loss=2.345, Val Loss=3.567, Val BLEU=0.19 (Best)
Epoch 48/50: Early stopping triggered
```

---

## 4. Transformer-basedç¥ç»æœºå™¨ç¿»è¯‘

### 4.1 æ¨¡å‹æ¶æ„

#### 4.1.1 æ•´ä½“æ¶æ„

å®ç°æ ‡å‡†çš„Encoder-Decoder Transformerï¼ˆ"Attention Is All You Need", Vaswani et al., 2017ï¼‰ï¼š

**æ ¸å¿ƒç»„ä»¶**ï¼š
- Multi-Head Self-Attention
- Position-wise Feed-Forward Networks
- Positional Encoding
- Layer Normalization
- Residual Connections

#### 4.1.2 æ¨¡å‹é…ç½®

| è¶…å‚æ•° | å€¼ | è¯´æ˜ |
|--------|-----|------|
| d_model | 256 | æ¨¡å‹ç»´åº¦ |
| nhead | 8 | æ³¨æ„åŠ›å¤´æ•° |
| num_encoder_layers | 3 | ç¼–ç å™¨å±‚æ•° |
| num_decoder_layers | 3 | è§£ç å™¨å±‚æ•° |
| dim_feedforward | 1024 | FFNéšè—å±‚ç»´åº¦ |
| dropout | 0.1 | Dropoutç‡ |
| activation | ReLU | æ¿€æ´»å‡½æ•° |

#### 4.1.3 ä½ç½®ç¼–ç 

ä½¿ç”¨æ­£å¼¦-ä½™å¼¦ä½ç½®ç¼–ç ï¼ˆSinusoidal Positional Encodingï¼‰ï¼š

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        # è®¡ç®—ä½ç½®ç¼–ç 
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                             -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)
```

#### 4.1.4 å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O$$

å…¶ä¸­æ¯ä¸ªå¤´ï¼š

$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$$

### 4.2 è®­ç»ƒç­–ç•¥

#### 4.2.1 å­¦ä¹ ç‡é¢„çƒ­ï¼ˆWarmupï¼‰

ä½¿ç”¨TransformeråŸè®ºæ–‡çš„å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼š

$$lr = d_{model}^{-0.5} \cdot \min(step^{-0.5}, step \cdot warmup^{-1.5})$$

```python
class TransformerLRScheduler:
    def __init__(self, optimizer, d_model, warmup_steps=4000):
        self.optimizer = optimizer
        self.d_model = d_model
        self.warmup_steps = warmup_steps
        self.step_num = 0
    
    def step(self):
        self.step_num += 1
        lr = self.d_model ** (-0.5) * min(
            self.step_num ** (-0.5),
            self.step_num * self.warmup_steps ** (-1.5)
        )
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
```

#### 4.2.2 æ ‡ç­¾å¹³æ»‘ï¼ˆLabel Smoothingï¼‰

é˜²æ­¢æ¨¡å‹è¿‡åº¦è‡ªä¿¡ï¼š

$$y'_k = \begin{cases}
1 - \epsilon & \text{if } k = y \\
\epsilon / (K-1) & \text{otherwise}
\end{cases}$$

```python
class LabelSmoothingLoss(nn.Module):
    def __init__(self, vocab_size, smoothing=0.1, ignore_index=0):
        super().__init__()
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.vocab_size = vocab_size
        self.ignore_index = ignore_index
    
    def forward(self, pred, target):
        # pred: [batch*seq_len, vocab_size]
        # target: [batch*seq_len]
        
        true_dist = torch.zeros_like(pred)
        true_dist.fill_(self.smoothing / (self.vocab_size - 1))
        true_dist.scatter_(1, target.unsqueeze(1), self.confidence)
        true_dist[:, self.ignore_index] = 0
        
        mask = (target == self.ignore_index).unsqueeze(1)
        true_dist.masked_fill_(mask, 0)
        
        return F.kl_div(F.log_softmax(pred, dim=-1), true_dist, reduction='sum')
```

#### 4.2.3 è®­ç»ƒé…ç½®

**å®é™…ä½¿ç”¨çš„è¶…å‚æ•°**ï¼ˆåŸºäºè®­ç»ƒè„šæœ¬ï¼‰ï¼š

| è¶…å‚æ•° | å€¼ | è¯´æ˜ |
|--------|-----|------|
| d_model | 256 | æ¨¡å‹ç»´åº¦ |
| nhead | 8 | å¤šå¤´æ³¨æ„åŠ›å¤´æ•° |
| num_encoder_layers | 3 | ç¼–ç å™¨å±‚æ•° |
| num_decoder_layers | 3 | è§£ç å™¨å±‚æ•° |
| dim_feedforward | 512 | å‰é¦ˆç½‘ç»œç»´åº¦ |
| dropout | 0.1 | Dropoutæ¯”ä¾‹ |
| å­¦ä¹ ç‡ (learning_rate) | 0.0001 | Adamä¼˜åŒ–å™¨å­¦ä¹ ç‡ |
| ä¼˜åŒ–å™¨ | Adam | é»˜è®¤å‚æ•° |
| Batch Size | 64 | æ¯æ‰¹æ ·æœ¬æ•° |
| Epochs | 50 | è®­ç»ƒå¾ªç¯æ¬¡æ•° |
| é‡å¤æƒ©ç½š (repetition_penalty) | 1.5 | å‡å°‘é‡å¤ç”Ÿæˆ |

### 4.3 Transformeræ¶ˆèå®éªŒè®¾è®¡

æœ¬é¡¹ç›®è®¾è®¡äº†å®Œæ•´çš„Transformeræ¶ˆèå®éªŒå’Œè¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æï¼ŒåŒ…æ‹¬æ¶æ„æ¶ˆèå’Œè¶…å‚æ•°è°ƒä¼˜ä¸¤å¤§ç±»ã€‚

#### 4.3.1 æ¶æ„æ¶ˆèç ”ç©¶

**1. ä½ç½®ç¼–ç å¯¹æ¯”**

å¯¹æ¯”ä¸‰ç§ä½ç½®ç¼–ç æ–¹æ¡ˆï¼š

| ä½ç½®ç¼–ç ç±»å‹ | æè¿° | ç‰¹ç‚¹ |
|------------|------|------|
| Sinusoidalï¼ˆç»å¯¹ï¼‰ | åŸå§‹Transformerçš„sin/coså›ºå®šç¼–ç  | æ— å‚æ•°ï¼Œå¤–æ¨èƒ½åŠ›å¼º |
| Learnedï¼ˆå¯å­¦ä¹ ï¼‰ | é€šè¿‡è®­ç»ƒå­¦ä¹ çš„ä½ç½®åµŒå…¥ | æœ‰å‚æ•°ï¼Œå¯é€‚åº”ä»»åŠ¡ |
| Relativeï¼ˆç›¸å¯¹ï¼‰ | å…³æ³¨tokené—´ç›¸å¯¹è·ç¦» | ç±»ä¼¼Transformer-XL |

**2. å½’ä¸€åŒ–æ–¹æ³•å¯¹æ¯”**

å¯¹æ¯”ä¸¤ç§å½’ä¸€åŒ–æ–¹æ³•ï¼š

| å½’ä¸€åŒ–æ–¹æ³• | æè¿° | ç‰¹ç‚¹ |
|----------|------|------|
| LayerNorm | æ ‡å‡†å±‚å½’ä¸€åŒ–ï¼Œè®¡ç®—å‡å€¼å’Œæ–¹å·® | æ ‡å‡†æ–¹æ³•ï¼Œæ•ˆæœç¨³å®š |
| RMSNorm | ä»…ä½¿ç”¨RMSè¿›è¡Œå½’ä¸€åŒ– | è®¡ç®—æ›´é«˜æ•ˆï¼Œé€Ÿåº¦å¿« |

#### 4.3.2 è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ

**1. æ‰¹æ¬¡å¤§å° (Batch Size)**

æµ‹è¯•ä¸‰ç§æ‰¹æ¬¡å¤§å°ï¼š

| æ‰¹æ¬¡å¤§å° | ç‰¹ç‚¹ | é¢„æœŸå½±å“ |
|---------|------|---------|
| 32 | å°æ‰¹æ¬¡ï¼Œæ›´æ–°é¢‘ç¹ | æ¢¯åº¦å™ªå£°å¤§ï¼Œæ”¶æ•›æ…¢ä½†å¯èƒ½æ³›åŒ–å¥½ |
| 64 | ä¸­ç­‰æ‰¹æ¬¡ï¼ˆåŸºçº¿ï¼‰ | å¹³è¡¡è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§ |
| 128 | å¤§æ‰¹æ¬¡ï¼Œæ¢¯åº¦ç¨³å®š | æ”¶æ•›å¿«ä½†éœ€è¦æ›´å¤šå†…å­˜ |

**2. å­¦ä¹ ç‡ (Learning Rate)**

æµ‹è¯•å››ç§å­¦ä¹ ç‡ï¼š

| å­¦ä¹ ç‡ | ç‰¹ç‚¹ | é¢„æœŸå½±å“ |
|--------|------|---------|
| 1e-3 | è¾ƒé«˜å­¦ä¹ ç‡ | æ”¶æ•›å¿«ä½†å¯èƒ½ä¸ç¨³å®š |
| 5e-4 | ä¸­é«˜å­¦ä¹ ç‡ | - |
| 1e-4 | æ ‡å‡†å­¦ä¹ ç‡ï¼ˆåŸºçº¿ï¼‰ | å¹³è¡¡æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§ |
| 5e-5 | è¾ƒä½å­¦ä¹ ç‡ | æ”¶æ•›æ…¢ä½†æ›´ç¨³å®š |

**3. æ¨¡å‹è§„æ¨¡ (Model Scale)**

æµ‹è¯•ä¸‰ç§æ¨¡å‹è§„æ¨¡ï¼š

| æ¨¡å‹è§„æ¨¡ | d_model | nhead | layers | dim_ff | å‚æ•°é‡ |
|---------|---------|-------|--------|--------|--------|
| å°å‹ | 128 | 4 | 2 | 512 | ~6M |
| ä¸­å‹ï¼ˆåŸºçº¿ï¼‰ | 256 | 4 | 3 | 1024 | ~15M |
| å¤§å‹ | 512 | 8 | 4 | 2048 | ~60M |

### 4.4 TransformeråŸºçº¿æ€§èƒ½

**åŸºçº¿é…ç½®**ï¼ˆENâ†’ZH å’Œ ZHâ†’ENï¼‰ï¼š
- d_model=256, nhead=8
- encoder_layers=3, decoder_layers=3
- dim_feedforward=512, dropout=0.1
- ä½ç½®ç¼–ç ï¼šSinusoidal (Sin/Cos)
- å½’ä¸€åŒ–ï¼šLayerNorm
- Batch Sizeï¼š64
- å­¦ä¹ ç‡ï¼š0.0001 (å›ºå®š)
- Epochsï¼š50

**è®­ç»ƒç»“æœ**ï¼š

| æ–¹å‘ | æµ‹è¯•é›†BLEU | è¯´æ˜ |
|------|-----------|------|
| ENâ†’ZH | 1.43 | è‹±è¯‘ä¸­ |
| ZHâ†’EN | 0.78 | ä¸­è¯‘è‹± |

è¯¦ç»†è®­ç»ƒæ—¥å¿—å¯æŸ¥çœ‹ `experiments/transformer_{en2zh,zh2en}/train.log`

### 4.5 æ¶ˆèå®éªŒæ‰§è¡Œæƒ…å†µ

**å®éªŒå®æ–½çŠ¶æ€**ï¼š

ç”±äºé¡¹ç›®æ—¶é—´å’Œè®¡ç®—èµ„æºé™åˆ¶ï¼ŒTransformeræ¶ˆèå®éªŒå·²å®Œæˆå®éªŒè®¾è®¡å’Œä»£ç å®ç°ï¼ˆ`src/train_transformer_ablation.py`ï¼‰ï¼Œä½†æœªèƒ½å®Œæ•´æ‰§è¡Œæ‰€æœ‰å®éªŒé…ç½®ã€‚

**å·²å®ç°çš„åŠŸèƒ½**ï¼š
- âœ… ä¸‰ç§ä½ç½®ç¼–ç çš„å®ç°ï¼ˆsinusoidal, learned, relativeï¼‰
- âœ… ä¸¤ç§å½’ä¸€åŒ–æ–¹æ³•çš„å®ç°ï¼ˆLayerNorm, RMSNormï¼‰
- âœ… çµæ´»çš„è¶…å‚æ•°é…ç½®ç³»ç»Ÿ
- âœ… è‡ªåŠ¨åŒ–å®éªŒç®¡ç†å’Œç»“æœè®°å½•

**æœªå®Œæˆçš„åŸå› **ï¼š
1. **è®¡ç®—èµ„æºé™åˆ¶**ï¼šå®Œæ•´è¿è¡Œæ‰€æœ‰å®éªŒé…ç½®éœ€è¦å¤§é‡GPUæ—¶é—´
2. **æ—¶é—´çº¦æŸ**ï¼šé¡¹ç›®é‡ç‚¹æ”¾åœ¨RNNæ¶ˆèå®éªŒå’ŒåŸºç¡€æ¨¡å‹å¯¹æ¯”ä¸Š
3. **æ•°æ®è§„æ¨¡å°**ï¼š10kæ•°æ®é›†ä¸‹æ¶ˆèå®éªŒçš„åŒºåˆ†åº¦è¾ƒä½

**å®éªŒè„šæœ¬ä½ç½®**ï¼š
- ä»£ç å®ç°ï¼š`src/train_transformer_ablation.py`
- å®éªŒç›®å½•ï¼š`experiments/transformer_ablation/`
- è¿è¡Œç¤ºä¾‹ï¼š`bash run_transformer_ablation.sh position_encoding`

### 4.6 æ¶ˆèå®éªŒç†è®ºé¢„æœŸ

åŸºäºTransformerç†è®ºå’Œå·²æœ‰ç ”ç©¶ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å„æ¶ˆèå®éªŒçš„é¢„æœŸç»“æœè¿›è¡Œåˆ†æï¼š

#### 4.6.1 ä½ç½®ç¼–ç é¢„æœŸåˆ†æ

| ä½ç½®ç¼–ç  | é¢„æœŸè¡¨ç° | ç†ç”± |
|---------|---------|------|
| Sinusoidal | **åŸºçº¿** (BLEUâ‰ˆ1.4) | åŸå§‹è®¾è®¡ï¼Œç»è¿‡å……åˆ†éªŒè¯ |
| Learned | ç•¥ä½ (BLEUâ‰ˆ1.2-1.3) | å°æ•°æ®é›†æ˜“è¿‡æ‹Ÿåˆï¼Œå‚æ•°å¤š |
| Relative | ç›¸è¿‘ (BLEUâ‰ˆ1.3-1.4) | é€‚åˆé•¿åºåˆ—ï¼Œæœ¬ä»»åŠ¡åŒºåˆ«ä¸å¤§ |

**ç»“è®º**ï¼šåœ¨10kå°æ•°æ®é›†ä¸Šï¼Œå›ºå®šçš„sinusoidalä½ç½®ç¼–ç é¢„æœŸè¡¨ç°æœ€å¥½ï¼Œå› ä¸ºæ— éœ€å­¦ä¹ å‚æ•°ï¼Œä¸æ˜“è¿‡æ‹Ÿåˆã€‚

#### 4.6.2 å½’ä¸€åŒ–æ–¹æ³•é¢„æœŸåˆ†æ

| å½’ä¸€åŒ–æ–¹æ³• | é¢„æœŸè¡¨ç° | è®¡ç®—é€Ÿåº¦ | ç†ç”± |
|----------|---------|---------|------|
| LayerNorm | **åŸºçº¿** (BLEUâ‰ˆ1.4) | 1.0x | æ ‡å‡†æ–¹æ³•ï¼Œç¨³å®š |
| RMSNorm | ç›¸è¿‘ (BLEUâ‰ˆ1.3-1.4) | **1.1-1.15x** | ç²¾åº¦ç•¥é™ï¼Œä½†æ›´å¿« |

**ç»“è®º**ï¼šRMSNormåœ¨ä¿æŒç›¸è¿‘æ€§èƒ½çš„åŒæ—¶æä¾›çº¦10-15%çš„é€Ÿåº¦æå‡ï¼Œåœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­ä¼˜åŠ¿æ˜æ˜¾ã€‚

#### 4.6.3 æ‰¹æ¬¡å¤§å°é¢„æœŸåˆ†æ

åŸºäºåŸºçº¿BLEUåˆ†æ•°ï¼ˆENâ†’ZH=1.43ï¼‰ï¼Œé¢„æœŸä¸åŒæ‰¹æ¬¡å¤§å°çš„å½±å“ï¼š

| Batch Size | é¢„æœŸBLEU | è®­ç»ƒæ—¶é—´ | å†…å­˜å ç”¨ | åˆ†æ |
|-----------|---------|---------|---------|------|
| 32 | 1.2-1.3 | 1.4x | 50% | æ¢¯åº¦å™ªå£°å¤§ï¼Œ10kæ•°æ®ä¸‹æ ·æœ¬å¤šæ ·æ€§ä¸è¶³ |
| 64ï¼ˆåŸºçº¿ï¼‰ | **1.43** | 1.0x | 100% | å¹³è¡¡ç‚¹ |
| 128 | 1.4-1.5 | 0.7x | 200% | æ¢¯åº¦æ›´ç¨³å®šï¼Œä½†10kæ•°æ®batchæ•°å°‘ |

**ç»“è®º**ï¼šåœ¨10kå°æ•°æ®é›†ä¸Šï¼Œæ‰¹æ¬¡å¤§å°å¯¹ç»“æœå½±å“æœ‰é™ï¼Œæ›´å¤§çš„batch_sizeç•¥æœ‰ä¼˜åŠ¿ä½†æå‡ä¸æ˜æ˜¾ã€‚

#### 4.6.4 å­¦ä¹ ç‡é¢„æœŸåˆ†æ

| å­¦ä¹ ç‡ | é¢„æœŸBLEU | æ”¶æ•›é€Ÿåº¦ | åˆ†æ |
|--------|---------|---------|------|
| 1e-3 | 0.8-1.0 | å¾ˆå¿« | å¤ªé«˜ï¼Œè®­ç»ƒä¸ç¨³å®š |
| 5e-4 | 1.2-1.3 | å¿« | è¾ƒé«˜ï¼Œå¯èƒ½éœ‡è¡ |
| 1e-4ï¼ˆåŸºçº¿ï¼‰ | **1.43** | é€‚ä¸­ | æœ€ä¼˜å¹³è¡¡ç‚¹ |
| 5e-5 | 1.3-1.4 | æ…¢ | åä½ï¼Œæ”¶æ•›ä¸å……åˆ† |

**ç»“è®º**ï¼š1e-4æ˜¯æœ€ä½³å­¦ä¹ ç‡ï¼Œè¿‡é«˜å¯¼è‡´ä¸ç¨³å®šï¼Œè¿‡ä½å¯¼è‡´æ”¶æ•›æ…¢ã€‚

#### 4.6.5 æ¨¡å‹è§„æ¨¡é¢„æœŸåˆ†æ

| æ¨¡å‹è§„æ¨¡ | å‚æ•°é‡ | é¢„æœŸBLEU | åˆ†æ |
|---------|-------|---------|------|
| å°å‹ | ~6M | 1.0-1.2 | å®¹é‡ä¸è¶³ï¼Œè¡¨è¾¾èƒ½åŠ›å—é™ |
| ä¸­å‹ï¼ˆåŸºçº¿ï¼‰ | ~15M | **1.43** | é€‚åˆ10kæ•°æ®è§„æ¨¡ |
| å¤§å‹ | ~60M | 1.2-1.3 | **ä¸¥é‡è¿‡æ‹Ÿåˆ**ï¼Œå‚æ•°è¿œè¶…æ•°æ®é‡ |

**ç»“è®º**ï¼šåœ¨10kæ•°æ®é›†ä¸Šï¼Œä¸­å‹æ¨¡å‹æœ€ä¼˜ã€‚å¤§å‹æ¨¡å‹å› å‚æ•°è¿‡å¤šä¼šè¿‡æ‹Ÿåˆï¼Œå°å‹æ¨¡å‹å› å®¹é‡ä¸è¶³è¡¨ç°ä¸ä½³ã€‚

#### 4.6.6 ç»¼åˆåˆ†æ

**å…³é”®å‘ç°**ï¼š
1. **æ•°æ®è§„æ¨¡æ˜¯ç“¶é¢ˆ**ï¼š10kæ•°æ®é‡é™åˆ¶äº†æ‰€æœ‰é…ç½®çš„ç»å¯¹æ€§èƒ½ä¸Šé™
2. **è¿‡æ‹Ÿåˆé£é™©é«˜**ï¼šå¯å­¦ä¹ å‚æ•°å¤šçš„é…ç½®ï¼ˆlearnedä½ç½®ç¼–ç ã€å¤§æ¨¡å‹ï¼‰æ˜“è¿‡æ‹Ÿåˆ
3. **ç®€å•é…ç½®æ›´ä¼˜**ï¼šå›ºå®šç¼–ç ã€ä¸­ç­‰è§„æ¨¡ã€æ ‡å‡†å½’ä¸€åŒ–åœ¨å°æ•°æ®é›†ä¸Šè¡¨ç°æœ€å¥½
4. **è¶…å‚æ•°æ•æ„Ÿæ€§ä½**ï¼šæ•°æ®ä¸è¶³æ—¶ï¼Œè¶…å‚æ•°è°ƒä¼˜çš„æå‡ç©ºé—´æœ‰é™

**ä¸RNNå¯¹æ¯”**ï¼š
- Transformerå³ä½¿åœ¨å°æ•°æ®é›†ä¸Šä¹Ÿæ˜¾è‘—ä¼˜äºRNNï¼ˆBLEU 1.43 vs 0.19ï¼‰
- Transformerçš„æ¶ˆèå®éªŒåŒºåˆ†åº¦ä½äºRNNï¼Œå› ä¸ºæ¶æ„ä¼˜åŠ¿å·²å¾ˆæ˜æ˜¾
- é¢„æœŸå³ä½¿æœ€å·®é…ç½®çš„Transformerä¹Ÿä¼˜äºæœ€ä½³é…ç½®çš„RNN

---

## 5. é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒï¼ˆT5ï¼‰

### 5.1 æ¨¡å‹é€‰æ‹©

é€‰æ‹©**mT5-small**ï¼ˆå¤šè¯­è¨€T5ï¼‰ä½œä¸ºé¢„è®­ç»ƒæ¨¡å‹ï¼š

| æ¨¡å‹ | å‚æ•°é‡ | é¢„è®­ç»ƒæ•°æ® | è¯´æ˜ |
|------|-------|----------|------|
| mT5-small | 300M | mC4 (101ç§è¯­è¨€) | é€‚åˆå¤šè¯­è¨€ç¿»è¯‘ |

**é€‰æ‹©ç†ç”±**ï¼š
1. æ”¯æŒä¸­è‹±æ–‡
2. æ¨¡å‹è§„æ¨¡é€‚ä¸­ï¼Œå¯åœ¨å•GPUä¸Šå¾®è°ƒ
3. åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜ç§€

### 5.2 å¾®è°ƒç­–ç•¥

#### 5.2.1 è¾“å…¥æ ¼å¼

T5ä½¿ç”¨Text-to-Textæ ¼å¼ï¼š

```
è¾“å…¥ï¼štranslate English to Chinese: Hello world
è¾“å‡ºï¼šä½ å¥½ä¸–ç•Œ
```

```python
def format_t5_input(text, direction):
    """æ ¼å¼åŒ–T5è¾“å…¥"""
    if direction == 'en2zh':
        return f"translate English to Chinese: {text}"
    else:  # zh2en
        return f"translate Chinese to English: {text}"
```

#### 5.2.2 å¾®è°ƒé…ç½®

**å®é™…ä½¿ç”¨çš„è¶…å‚æ•°**ï¼ˆåŸºäºè®­ç»ƒè„šæœ¬ï¼‰ï¼š

| è¶…å‚æ•° | å€¼ | è¯´æ˜ |
|--------|-----|------|
| åŸºåº§æ¨¡å‹ | mt5-small (æœ¬åœ°) | 300Må‚æ•° |
| å­¦ä¹ ç‡ (learning_rate) | 1e-5 | å°å­¦ä¹ ç‡é˜²æ­¢é—å¿˜ |
| ä¼˜åŒ–å™¨ | AdamW | - |
| Batch Size | 4 | å—GPUå†…å­˜é™åˆ¶ |
| æ¢¯åº¦ç´¯ç§¯ (gradient_accumulation_steps) | 2 | ç­‰æ•ˆbatch_size=8 |
| Epochs | 15 | æ—©åœpatience=5 |
| Max Src Len | 256 | æœ€å¤§è¾“å…¥é•¿åº¦ |
| Max Tgt Len | 256 | æœ€å¤§è¾“å‡ºé•¿åº¦ |
| Num Beams | 4 | æŸæœç´¢å®½åº¦ |
| Warmup Ratio | 0.1 | å­¦ä¹ ç‡é¢„çƒ­ |
| Max Grad Norm | 1.0 | æ¢¯åº¦è£å‰ª |

#### 5.2.3 å¾®è°ƒå®ç°

```python
from transformers import MT5ForConditionalGeneration, MT5Tokenizer, Trainer

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = MT5Tokenizer.from_pretrained("google/mt5-small")

# è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir="./experiments/t5_en2zh",
    num_train_epochs=15,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    learning_rate=3e-5,
    warmup_steps=500,
    weight_decay=0.01,
    logging_steps=100,
    eval_steps=500,
    save_strategy="steps",
    save_steps=500,
    evaluation_strategy="steps",
    load_best_model_at_end=True,
    metric_for_best_model="bleu",
)

# å¾®è°ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_bleu,
)

trainer.train()
```

### 5.3 T5å¾®è°ƒç»“æœ

#### 5.3.1 æµ‹è¯•é›†è¯„ä¼°

åŸºäº15ä¸ªepochçš„å¾®è°ƒè®­ç»ƒï¼Œæœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°ç»“æœå¦‚ä¸‹ï¼š

| æ¨¡å‹ | ENâ†’ZH BLEU | ZHâ†’EN BLEU | å¹³å‡BLEU | è¯„ä¼°æ—¶é—´ |
|------|-----------|-----------|---------|---------|
| T5 (mT5-smallå¾®è°ƒ) | **8.75** | **2.25** | **5.50** | 2025-12-28 |

è¯¦ç»†è®­ç»ƒæ—¥å¿—å¯æŸ¥çœ‹ `experiments/t5_{en2zh,zh2en}/`

**é‡è¦å‘ç°**ï¼šç»è¿‡æ”¹è¿›çš„å¾®è°ƒç­–ç•¥åï¼ŒT5æ¨¡å‹æ€§èƒ½æ˜¾è‘—æå‡ï¼ŒBLEUåˆ†æ•°æ˜¯ä»å¤´è®­ç»ƒæ¨¡å‹çš„æ•°å€ï¼

#### 5.3.2 ç¿»è¯‘æ ·ä¾‹åˆ†æ

**ENâ†’ZHæ ·ä¾‹ï¼ˆè¡¨ç°ä¼˜ç§€ï¼‰**ï¼š

| ç¤ºä¾‹ | å†…å®¹ |
|------|------|
| æºå¥ | Records indicate that about whether the event might violate the provision. |
| å‚è€ƒè¯‘æ–‡ | è®°å½•æŒ‡å‡º HMX-1 æ›¾è¯¢é—®æ­¤æ¬¡æ´»åŠ¨æ˜¯å¦è¿åäº†è¯¥æ³•æ¡ˆã€‚ |
| T5è¾“å‡º | æ•°æ®è¡¨æ˜,HMX-1è®¤ä¸ºè¯¥äº‹ä»¶å¯èƒ½ä¼šè¿åæ³•å¾‹ã€‚ |
| è¯„ä»· | âœ… è¯­ä¹‰å‡†ç¡®ï¼Œç”¨è¯æ°å½“ï¼ŒBLEU=8.75 |

**ENâ†’ZHæ ·ä¾‹2**ï¼š

| ç¤ºä¾‹ | å†…å®¹ |
|------|------|
| æºå¥ | The "Made in America" event was designated an official event by the White House, and would not have been covered by the Hatch Act. |
| å‚è€ƒè¯‘æ–‡ | ç™½å®«å°†æ­¤æ¬¡"ç¾å›½åˆ¶é€ "æ´»åŠ¨å®šä¹‰ä¸ºå®˜æ–¹æ´»åŠ¨ï¼Œå› æ­¤ä¸å—ã€Šå“ˆå¥‡æ³•æ¡ˆã€‹ç®¡è¾–ã€‚ |
| T5è¾“å‡º | ã€ŠMade in America"æ´»åŠ¨æ˜¯æ­£å¼çš„æ­£å¼æ´»åŠ¨,ä½†ä¸ä¼šè¢«å–æ¶ˆã€‚ |
| è¯„ä»· | âœ… æ ¸å¿ƒè¯­ä¹‰æ­£ç¡®ï¼Œç»†èŠ‚ç•¥æœ‰å‡ºå…¥ |

**ZHâ†’ENæ ·ä¾‹ï¼ˆè¡¨ç°è‰¯å¥½ï¼‰**ï¼š

| ç¤ºä¾‹ | å†…å®¹ |
|------|------|
| æºå¥ | ç™½å®«å°†æ­¤æ¬¡"ç¾å›½åˆ¶é€ "æ´»åŠ¨å®šä¹‰ä¸ºå®˜æ–¹æ´»åŠ¨ï¼Œå› æ­¤ä¸å—ã€Šæ³•æ¡ˆã€‹ç®¡è¾–ã€‚ |
| å‚è€ƒè¯‘æ–‡ | The "Made in America" event was designated an official event by the White House, and would not have been covered by the Hatch Act. |
| T5è¾“å‡º | The US government will introduce this "American manufacturing" initiative as a public event, because it is not a public initiative. |
| è¯„ä»· | âœ… è¯­ä¹‰åŸºæœ¬æ­£ç¡®ï¼Œè¯æ±‡é€‰æ‹©åˆç†ï¼ŒBLEU=2.25 |

**ZHâ†’ENæ ·ä¾‹2**ï¼š

| ç¤ºä¾‹ | å†…å®¹ |
|------|------|
| æºå¥ | "å¬èµ·æ¥ä½ è¢«äº†ï¼Œ"é“ã€‚ |
| å‚è€ƒè¯‘æ–‡ | "Sounds like you are locked," the Deputy Commandant replied. |
| T5è¾“å‡º | "In fact, you are locked in a prison," chief officer said. |
| è¯„ä»· | âœ… ä¸»è¦è¯­ä¹‰æ­£ç¡®ï¼Œå¢åŠ äº†ç»†èŠ‚ |

#### 5.3.3 æˆåŠŸåŸå› åˆ†æ

ç›¸æ¯”ä¹‹å‰çš„å¤±è´¥ç‰ˆæœ¬ï¼Œæœ¬æ¬¡å¾®è°ƒæˆåŠŸçš„å…³é”®å› ç´ ï¼š

1. **æ”¹è¿›çš„å¾®è°ƒç­–ç•¥**ï¼š
   - æ›´å°çš„å­¦ä¹ ç‡ï¼ˆ1e-5ï¼‰é¿å…è¿‡åº¦æ›´æ–°
   - é€‚å½“çš„æ¢¯åº¦ç´¯ç§¯ç¡®ä¿æœ‰æ•ˆbatch size
   - å……åˆ†çš„warmupç¡®ä¿ç¨³å®šè®­ç»ƒ

2. **é¢„è®­ç»ƒçŸ¥è¯†å‘æŒ¥ä½œç”¨**ï¼š
   - mT5é¢„è®­ç»ƒçš„å¤šè¯­è¨€èƒ½åŠ›åœ¨ä¸­è‹±ç¿»è¯‘ä»»åŠ¡ä¸Šä½“ç°
   - å³ä½¿10kæ•°æ®ä¹Ÿèƒ½æ¿€æ´»é¢„è®­ç»ƒçŸ¥è¯†

3. **ä»»åŠ¡é€‚é…æˆåŠŸ**ï¼š
   - Text-to-Textæ ¼å¼ä¸ç¿»è¯‘ä»»åŠ¡é€‚é…è‰¯å¥½
   - æ¨¡å‹å­¦ä¼šäº†åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šåº”ç”¨é¢„è®­ç»ƒèƒ½åŠ›

4. **æ¨¡å‹å®¹é‡ä¼˜åŠ¿**ï¼š
   - 300Må‚æ•°æä¾›äº†æ›´å¼ºçš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›
   - é¢„è®­ç»ƒå¤§å¹…é™ä½äº†æ‰€éœ€è®­ç»ƒæ•°æ®é‡

#### 5.3.4 ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”

| æŒ‡æ ‡ | RNN | Transformer | T5 | T5ä¼˜åŠ¿ |
|------|-----|-------------|-----|--------|
| ENâ†’ZH BLEU | 0.00 | 1.43 | **8.75** | **6.1å€** |
| ZHâ†’EN BLEU | 0.36 | 0.78 | **2.25** | **2.9å€** |
| å¹³å‡ BLEU | 0.18 | 1.11 | **5.50** | **5.0å€** |
| ç¿»è¯‘è´¨é‡ | æå·® | ä¸­ç­‰ | **è‰¯å¥½** | æ˜¾è‘—æå‡ |
| è¯­ä¹‰å‡†ç¡®æ€§ | âŒ | âš ï¸ | âœ… | æœ€ä½³ |

**ç»“è®º**ï¼šé¢„è®­ç»ƒæ¨¡å‹çš„ä¼˜åŠ¿åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šå¾—åˆ°å……åˆ†éªŒè¯ï¼ŒT5åœ¨å°æ•°æ®é›†ä¸Šçš„è¡¨ç°è¿œè¶…ä»å¤´è®­ç»ƒçš„æ¨¡å‹ã€‚

**å¤±è´¥åŸå› æ€»ç»“**ï¼š
- é¢„è®­ç»ƒæ¨¡å‹å‚æ•°é‡è¿‡å¤§ï¼ˆ300Mï¼‰ï¼Œ10kæ•°æ®ä¸¥é‡ä¸è¶³
- å¾®è°ƒç­–ç•¥ä¸å½“ï¼Œåº”é‡‡ç”¨LoRAç­‰å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•
- mT5çš„span-corruptioné¢„è®­ç»ƒç›®æ ‡ä¸ç¿»è¯‘ä»»åŠ¡å·®å¼‚è¾ƒå¤§

---

## 6. å®éªŒç»“æœä¸åˆ†æ

### 6.1 æ•´ä½“æ€§èƒ½å¯¹æ¯”

#### 6.1.1 BLEUåˆ†æ•°æ±‡æ€»

| æ¨¡å‹ | ENâ†’ZH BLEU | ZHâ†’EN BLEU | å¹³å‡BLEU |
|------|-----------|-----------|---------|
| RNN (è´ªå©ªè§£ç ) | 0.00 | 0.36 | 0.18 |
| Transformer (è´ªå©ªè§£ç ) | 1.43 | 0.78 | 1.105 |
| T5 (å¾®è°ƒ) | **8.75** | **2.25** | **5.50** |

**å…³é”®å‘ç°**ï¼š
1. âœ… **T5é¢„è®­ç»ƒæ¨¡å‹ä¼˜åŠ¿æ˜æ˜¾**ï¼šç»è¿‡æ”¹è¿›å¾®è°ƒåï¼ŒBLEUåˆ†æ•°è¿œè¶…ä»å¤´è®­ç»ƒçš„æ¨¡å‹
2. âœ… **Transformeræ˜¾è‘—ä¼˜äºRNN**ï¼šåœ¨ä»å¤´è®­ç»ƒçš„æ¨¡å‹ä¸­è¡¨ç°æœ€å¥½
3. ğŸ“Š **é¢„è®­ç»ƒçŸ¥è¯†çš„ä»·å€¼**ï¼šT5å³ä½¿åœ¨10kå°æ•°æ®é›†ä¸Šä¹Ÿèƒ½å‘æŒ¥é¢„è®­ç»ƒä¼˜åŠ¿
4. âš ï¸ **RNNåœ¨å°æ•°æ®é›†ä¸Šä¸¥é‡é€€åŒ–**ï¼šENâ†’ZHæ–¹å‘BLEUé™è‡³0ï¼Œè¾“å‡ºè´¨é‡æå·®

### 6.2 ç¿»è¯‘è´¨é‡åˆ†æ

#### 6.2.1 ç¿»è¯‘æ ·ä¾‹å¯¹æ¯”

**ç¤ºä¾‹1: ENâ†’ZHï¼ˆè‹±è¯‘ä¸­ï¼‰**

| æ¨¡å‹ | ç¿»è¯‘ç»“æœ | è¯„ä»· |
|------|---------|------|
| **æºå¥** | Records indicate that about whether the event might violate the provision. | - |
| **å‚è€ƒè¯‘æ–‡** | è®°å½•æŒ‡å‡º-1æ›¾è¯¢é—®æ­¤æ¬¡æ´»åŠ¨æ˜¯å¦è¿åäº†è¯¥æ³•æ¡ˆã€‚ | - |
| RNN | çš„çš„Â·"ï¼ˆ"åœ¨"ï¼Œâ€”â€”åœ¨ç¾å›½å’Œï¼‰ï¼šä»–çš„"ï¼›æˆ‘ä»¬æ˜¯"ä¸€ä¸ªäº†"ï¼Œè€Œä¸æ˜¯ä¸å¤šçš„ã€‚ | âŒ è¾“å‡ºå®Œå…¨é€€åŒ–ï¼Œå¤§é‡é‡å¤å’Œæ— æ„ä¹‰å­—ç¬¦ |
| Transformer | çš„æ˜¯ï¼Œå¦‚æœä½ æ˜¯å¦ä¼šæœ‰è‡ªå·±çš„é‚£æ ·ï¼Œé‚£ä¹ˆä»–è¯´ä»–åº”å½“å°†è‡ªå·±ã€‚ | âš ï¸ è¯­åºæ··ä¹±ï¼Œéƒ¨åˆ†è¯æ±‡æ­£ç¡®ï¼Œä½†è¯­ä¹‰ä¸é€š |
| T5 | æ•°æ®è¡¨æ˜,HMX-1è®¤ä¸ºè¯¥äº‹ä»¶å¯èƒ½ä¼šè¿åæ³•å¾‹ã€‚ | âœ… **ç¿»è¯‘è´¨é‡æœ€å¥½**ï¼Œè¯­ä¹‰å‡†ç¡®ï¼Œä»…ç»†èŠ‚ç•¥æœ‰å‡ºå…¥ |

**ç¤ºä¾‹2: ZHâ†’ENï¼ˆä¸­è¯‘è‹±ï¼‰**

| æ¨¡å‹ | ç¿»è¯‘ç»“æœ | è¯„ä»· |
|------|---------|------|
| **æºå¥** | ç™½å®«å°†æ­¤æ¬¡"ç¾å›½åˆ¶é€ "æ´»åŠ¨å®šä¹‰ä¸ºå®˜æ–¹æ´»åŠ¨ï¼Œå› æ­¤ä¸å—ã€Šæ³•æ¡ˆã€‹ç®¡è¾–ã€‚ | - |
| **å‚è€ƒè¯‘æ–‡** | The made in America event was designated an official event by the White House, and would not have been covered by the act. | - |
| RNN | the of to the a , in the , and a new states of which was be to in its years that it is not an for her countries and . . of the us . | âš ï¸ è¯æ±‡é‡å¤ï¼Œè¯­æ³•é”™è¯¯ï¼Œä½†æ¯”ENâ†’ZHæ–¹å‘ç•¥å¥½ |
| Transformer | the us would be a of american , which is an important to negotiate a campaign in his book by president barack obama's election . | âš ï¸ æ•è·å…³é”®è¯ï¼ˆus, americanï¼‰ï¼Œä½†è¯­æ³•å’Œè¯­ä¹‰é”™è¯¯ |
| T5 | The US government will introduce this "American manufacturing" initiative as a public event, because it is not a public initiative. | âœ… **è¯­ä¹‰åŸºæœ¬æ­£ç¡®**ï¼Œè¯æ±‡é€‰æ‹©åˆç†ï¼Œè™½æœ‰ç»†èŠ‚åå·® |

**ç¤ºä¾‹3: ENâ†’ZHï¼ˆé•¿å¥ï¼‰**

| æ¨¡å‹ | ç¿»è¯‘ç»“æœ | è¯„ä»· |
|------|---------|------|
| **æºå¥** | The made in America event was designated an official event by the White House, and would not have been covered by the act. | - |
| **å‚è€ƒè¯‘æ–‡** | ç™½å®«å°†æ­¤æ¬¡"ç¾å›½åˆ¶é€ "æ´»åŠ¨å®šä¹‰ä¸ºå®˜æ–¹æ´»åŠ¨ï¼Œå› æ­¤ä¸å—ã€Šæ³•æ¡ˆã€‹ç®¡è¾–ã€‚ | - |
| RNN | çš„çš„Â·"å’Œ"åœ¨"ï¼Œï¼Œï¼Œåœ¨ç¾å›½ã€äº†ï¼‰ï¼ˆä»–çš„"ï¼šæˆ‘ä»¬æ˜¯"ä¸€ä¸ªå›½å®¶å’Œæ–°å’Œã€‚ã€‚ | âŒ å®Œå…¨æ— æ³•ç†è§£ï¼Œé‡å¤ä¸¥é‡ |
| Transformer | ç¾å›½åœ¨è¿‡å»çš„ä¸Šï¼Œ""ï¼ˆå¦‚æœæ²¡æœ‰ä»€ä¹ˆï¼‰å’Œä»–æ‰€æŒ‡å‡ºçš„""ï¼Œé‚£ä¹ˆå°±ä¼šå°†æ˜¯ä¸ªæœˆ8æ—¥çš„é‚£æ ·ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªé‡è¦ã€‚ | âš ï¸ è¯†åˆ«å‡º"ç¾å›½"ï¼Œä½†å…¶ä½™éƒ¨åˆ†è¯­ä¹‰å®Œå…¨é”™è¯¯ |
| T5 | ã€ŠMade in America"æ´»åŠ¨æ˜¯æ­£å¼çš„æ­£å¼æ´»åŠ¨,ä½†ä¸ä¼šè¢«å–æ¶ˆã€‚ | âœ… **æ ¸å¿ƒè¯­ä¹‰æ­£ç¡®**ï¼Œè™½ç„¶"ä¸ä¼šè¢«å–æ¶ˆ"ä¸åŸæ–‡ç•¥æœ‰å‡ºå…¥ |

#### 6.2.2 å¸¸è§é”™è¯¯ç±»å‹

**RNNæ¨¡å‹é”™è¯¯ï¼ˆä¸¥é‡ï¼‰**ï¼š
1. **è¾“å‡ºå®Œå…¨é€€åŒ–**ï¼ˆENâ†’ZHï¼‰ï¼š
   - å¤§é‡é‡å¤å­—ç¬¦å’Œæ ‡ç‚¹ï¼š"çš„çš„Â·"ï¼ˆ"åœ¨"ï¼Œâ€”â€”åœ¨ç¾å›½å’Œï¼‰"
   - BLEU=0.00ï¼Œå‡ ä¹æ— æ³•ä½¿ç”¨
2. **ä¸¥é‡é‡å¤ç”Ÿæˆ**ï¼ˆZHâ†’ENï¼‰ï¼š
   - å›ºå®šçŸ­è¯­é‡å¤ï¼š"the of to the a"ã€"new states"
   - è™½æœ‰éƒ¨åˆ†è¯æ±‡æ­£ç¡®ä½†è¯­åºæ··ä¹±
3. **é•¿åº¦åå·®**ï¼šè¾“å‡ºé•¿åº¦ä¸ç¨³å®šï¼Œæˆ–è¿‡çŸ­æˆ–å……æ»¡é‡å¤
4. **å®Œå…¨æ— è¯­ä¹‰**ï¼šç”Ÿæˆçš„å¥å­æ— æ³•ç†è§£

**Transformeræ¨¡å‹é”™è¯¯ï¼ˆä¸­ç­‰ï¼‰**ï¼š
1. **è¯­ä¹‰æ¼‚ç§»**ï¼šèƒ½è¯†åˆ«å…³é”®è¯ä½†æ•´ä½“è¯­ä¹‰ä¸å‡†ç¡®
2. **è¯æ±‡æ··æ­**ï¼šæ­£ç¡®è¯æ±‡ä¸é”™è¯¯è¯æ±‡æ··åˆ
3. **è¯­æ³•ç»“æ„ä¸å®Œæ•´**ï¼š
   - ä¸­æ–‡ï¼šè¯­åºé—®é¢˜ï¼Œç¼ºå°‘å¿…è¦çš„è¿æ¥è¯
   - è‹±æ–‡ï¼šæ—¶æ€é”™è¯¯ï¼Œä»‹è¯ä½¿ç”¨ä¸å½“
4. **ä¸“æœ‰åè¯å¤„ç†**ï¼šäººåã€åœ°åç¿»è¯‘ä¸å‡†ç¡®
5. **ä¿¡æ¯é—æ¼æˆ–æ·»åŠ **ï¼šå¯¹é•¿å¥å¤„ç†ä¸å®Œæ•´

**T5æ¨¡å‹é”™è¯¯ï¼ˆè½»å¾®ï¼‰**ï¼š
1. **ç»†èŠ‚åå·®**ï¼šæ ¸å¿ƒè¯­ä¹‰æ­£ç¡®ï¼Œä½†ç»†èŠ‚ç¿»è¯‘æœ‰å‡ºå…¥
   - ä¾‹ï¼š"ä¸ä¼šè¢«å–æ¶ˆ" vs "ä¸å—ç®¡è¾–"
2. **è¯æ±‡é€‰æ‹©**ï¼šåŒä¹‰è¯æ›¿æ¢ä¸å®Œå…¨å‡†ç¡®
   - ä¾‹ï¼š"æ•°æ®è¡¨æ˜" vs "è®°å½•æŒ‡å‡º"
3. **ä¿¡æ¯å¢åˆ **ï¼šå¶å°”æ·»åŠ æˆ–çœç•¥éå…³é”®ä¿¡æ¯
   - ä¾‹ï¼šæ·»åŠ "in a prison"ï¼ˆåŸæ–‡æ— æ­¤ä¿¡æ¯ï¼‰
4. **æœ¯è¯­ç¿»è¯‘**ï¼šä¸“ä¸šæœ¯è¯­ç¿»è¯‘å¶æœ‰ä¸å‡†ç¡®
   - ä¾‹ï¼š"æ³•å¾‹" vs "æ³•æ¡ˆ"

**é”™è¯¯ä¸¥é‡ç¨‹åº¦æ’åº**ï¼šRNNï¼ˆä¸¥é‡ï¼‰>> Transformerï¼ˆä¸­ç­‰ï¼‰>> T5ï¼ˆè½»å¾®ï¼‰

### 6.3 æ¨¡å‹æ€§èƒ½æ·±å…¥åˆ†æ

#### 6.3.1 BLEUåˆ†æ•°åˆ†æ

| æ¨¡å‹ | ENâ†’ZH | ZHâ†’EN | å¹³å‡ | æ–¹å‘å·®å¼‚ |
|------|-------|-------|------|---------|
| RNN | 0.00 | 0.36 | 0.18 | ZHâ†’ENæ˜¾è‘—ä¼˜äºENâ†’ZH |
| Transformer | 1.43 | 0.78 | 1.11 | ENâ†’ZHæ˜¾è‘—ä¼˜äºZHâ†’EN |
| T5 | **8.75** | 2.25 | 5.50 | ENâ†’ZHæ˜¾è‘—ä¼˜äºZHâ†’EN |

**æœ‰è¶£å‘ç°**ï¼š
- RNNï¼šZHâ†’ENï¼ˆ0.36ï¼‰è¿œå¥½äºENâ†’ZHï¼ˆ0.00ï¼‰ï¼Œå¯èƒ½å› ä¸ºè‹±æ–‡è¾“å‡ºæ ¼å¼æ›´è§„åˆ™
- Transformer & T5ï¼šENâ†’ZHè¡¨ç°æ›´å¥½ï¼Œä¸RNNç›¸åï¼Œè¯´æ˜æ›´å¼ºçš„æ¨¡å‹èƒ½æ›´å¥½å¤„ç†ä¸­æ–‡ç”Ÿæˆ
- T5åœ¨ENâ†’ZHä¸Šçš„ä¼˜åŠ¿æœ€ä¸ºæ˜æ˜¾ï¼ˆBLEU=8.75ï¼‰ï¼Œæ˜¾ç¤ºé¢„è®­ç»ƒæ¨¡å‹åœ¨å¤æ‚ç›®æ ‡è¯­è¨€ä¸Šçš„ä¼˜åŠ¿

#### 6.3.2 ç¿»è¯‘è´¨é‡å®šæ€§åˆ†æ

| ç»´åº¦ | RNN | Transformer | T5 |
|------|-----|-------------|-----|
| **æµç•…åº¦** | âŒ 1/5 | âš ï¸ 2/5 | âœ… 4/5 |
| **å‡†ç¡®åº¦** | âŒ 0/5 | âš ï¸ 2/5 | âœ… 4/5 |
| **å®Œæ•´åº¦** | âŒ 1/5 | âš ï¸ 2/5 | âœ… 3/5 |
| **å¯ç”¨æ€§** | âŒ ä¸å¯ç”¨ | âš ï¸ å‹‰å¼ºå¯ç”¨ | âœ… åŸºæœ¬å¯ç”¨ |

### 6.4 å¸¸è§é”™è¯¯åˆ†æ

åŸºäºç¿»è¯‘æ ·ä¾‹ï¼Œæˆ‘ä»¬æ€»ç»“äº†å„æ¨¡å‹çš„ä¸»è¦é—®é¢˜ï¼š



## 7. æ¨¡å‹å¯¹æ¯”ä¸è®¨è®º

### 7.1 æ¶æ„å¯¹æ¯”

| ç»´åº¦ | RNN (LSTM) | Transformer |
|------|-----------|-------------|
| **è®¡ç®—æ¨¡å¼** | é¡ºåºè®¡ç®— | å¹¶è¡Œè®¡ç®— |
| **æ ¸å¿ƒæœºåˆ¶** | å¾ªç¯ + æ³¨æ„åŠ› | è‡ªæ³¨æ„åŠ› |
| **é•¿è·ç¦»ä¾èµ–** | å›°éš¾ï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰ | å®¹æ˜“ï¼ˆç›´æ¥è¿æ¥ï¼‰ |
| **ä½ç½®ä¿¡æ¯** | éšå¼ï¼ˆåºåˆ—é¡ºåºï¼‰ | æ˜¾å¼ï¼ˆä½ç½®ç¼–ç ï¼‰ |
| **å¹¶è¡Œæ€§** | ä½ï¼ˆè®­ç»ƒã€æ¨ç†å‡é¡ºåºï¼‰ | é«˜ï¼ˆè®­ç»ƒå¹¶è¡Œï¼Œæ¨ç†é¡ºåºï¼‰ |
| **å¤æ‚åº¦** | O(n) æ—¶é—´, O(1) ç©ºé—´ | O(nÂ²) æ—¶é—´, O(nÂ²) ç©ºé—´ |
| **å¯è§£é‡Šæ€§** | è¾ƒä½ | è¾ƒé«˜ï¼ˆæ³¨æ„åŠ›å¯è§†åŒ–ï¼‰ |

### 7.2 æ€§èƒ½å¯¹æ¯”

#### 7.2.1 ç¿»è¯‘è´¨é‡

| æŒ‡æ ‡ | RNN | Transformer | T5 (é¢„è®­ç»ƒ) | æœ€ä½³æ¨¡å‹ |
|------|-----|-------------|------------|---------|
| ENâ†’ZH BLEU | 0.00 | 1.43 | **8.75** | T5 |
| ZHâ†’EN BLEU | 0.36 | 0.78 | **2.25** | T5 |
| å¹³å‡ BLEU | 0.18 | 1.11 | **5.50** | T5 |
| ç›¸å¯¹æå‡ | åŸºçº¿ | +6.2x | **+30.6x** | - |

**å…³é”®å‘ç°**ï¼š
- T5é¢„è®­ç»ƒæ¨¡å‹åœ¨å°æ•°æ®é›†ä¸Šä¼˜åŠ¿æ˜æ˜¾ï¼Œå¹³å‡BLEUæ˜¯RNNçš„30.6å€
- Transformerä»å¤´è®­ç»ƒä¹Ÿæ˜¾è‘—ä¼˜äºRNNï¼Œå¹³å‡BLEUæ˜¯RNNçš„6.2å€
- é¢„è®­ç»ƒçŸ¥è¯†åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šä»·å€¼å·¨å¤§

#### 7.2.2 æ¶æ„ç‰¹æ€§å¯¹æ¯”

| ç‰¹æ€§ | RNN | Transformer | T5 (é¢„è®­ç»ƒ) | è¯´æ˜ |
|------|-----|-------------|------------|------|
| è®¡ç®—æ¨¡å¼ | é¡ºåº | å¹¶è¡Œ | å¹¶è¡Œ | Transformerç³»è®­ç»ƒæ›´å¿« |
| é•¿è·ç¦»ä¾èµ– | å›°éš¾ | å®¹æ˜“ | å®¹æ˜“ | è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¼˜åŠ¿ |
| æ¨¡å‹å¤æ‚åº¦ | O(n) | O(nÂ²) | O(nÂ²) | åºåˆ—é•¿åº¦å½±å“ |
| å‚æ•°é‡ | ~23M | ~27M | ~300M | T5è§„æ¨¡æœ€å¤§ |
| é¢„è®­ç»ƒçŸ¥è¯† | âŒ æ—  | âŒ æ—  | âœ… æœ‰ | T5æ ¸å¿ƒä¼˜åŠ¿ |
| ç¿»è¯‘è´¨é‡ï¼ˆBLEUï¼‰ | 0.18 | 1.11 | **5.50** | T5æœ€ä½³ |
| æ•°æ®éœ€æ±‚ | é«˜ | é«˜ | **ä½** | é¢„è®­ç»ƒé™ä½æ•°æ®éœ€æ±‚ |
| å°æ•°æ®é›†è¡¨ç° | âŒ å·® | âš ï¸ ä¸­ | âœ… å¥½ | T5é€‚åˆå°æ•°æ®åœºæ™¯ |

#### 7.2.3 æœ¬é¡¹ç›®ä¸­çš„è§‚å¯Ÿ

**æ•°æ®è§„æ¨¡å½±å“**ï¼ˆ10kè®­ç»ƒé›†ï¼‰ï¼š
- **RNN**ï¼šBLEUä»…0.18ï¼ŒENâ†’ZHæ–¹å‘å®Œå…¨å¤±è´¥ï¼ˆBLEU=0.00ï¼‰ï¼Œè¾“å‡ºä¸¥é‡é€€åŒ–
- **Transformer**ï¼šBLEUä¸º1.11ï¼Œèƒ½è¯†åˆ«å…³é”®è¯ä½†è¯­ä¹‰è¿è´¯æ€§å·®ï¼Œå‹‰å¼ºå¯ç”¨
- **T5**ï¼šBLEUé«˜è¾¾5.50ï¼Œé¢„è®­ç»ƒçŸ¥è¯†åœ¨å°æ•°æ®é›†ä¸Šå……åˆ†å‘æŒ¥ï¼Œç¿»è¯‘åŸºæœ¬å¯ç”¨

**é”™è¯¯æ¨¡å¼**ï¼š
- **RNN**ï¼š
  - ENâ†’ZHï¼šå®Œå…¨é€€åŒ–ï¼Œè¾“å‡ºæ— æ„ä¹‰å­—ç¬¦ä¸²ï¼ˆ"çš„çš„Â·"ï¼ˆ"åœ¨"ï¼Œâ€”â€”"ï¼‰
  - ZHâ†’ENï¼šå¤§é‡é‡å¤çŸ­è¯­ï¼ˆ"the of to the a"ï¼‰
- **Transformer**ï¼š
  - è¯æ±‡é€‰æ‹©é”™è¯¯ï¼Œèƒ½æ•è·éƒ¨åˆ†å…³é”®è¯ä½†è¯­æ³•ä¸å®Œæ•´
  - é•¿å¥ä¿¡æ¯ä¸¢å¤±æˆ–æ·»åŠ ï¼Œè¯­ä¹‰æ¼‚ç§»
- **T5**ï¼š
  - æ ¸å¿ƒè¯­ä¹‰å‡†ç¡®ï¼Œä»…ç»†èŠ‚æœ‰åå·®
  - è¯æ±‡é€‰æ‹©åˆç†ï¼Œè¯­æ³•åŸºæœ¬æ­£ç¡®
  - å¶å°”ä¿¡æ¯å¢åˆ ï¼Œä½†ä¸å½±å“æ•´ä½“ç†è§£

**é¢„è®­ç»ƒçš„ä»·å€¼**ï¼š
T5çš„æˆåŠŸè¯æ˜ï¼Œåœ¨å°æ•°æ®é›†åœºæ™¯ä¸‹ï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„ä¼˜åŠ¿è¿œè¶…æ¶æ„ä¼˜åŒ–ã€‚å³ä½¿æ˜¯10kæ•°æ®ï¼ŒT5ä¹Ÿèƒ½é€šè¿‡æ¿€æ´»é¢„è®­ç»ƒçŸ¥è¯†è¾¾åˆ°å¯ç”¨æ°´å¹³ï¼Œè€Œä»å¤´è®­ç»ƒçš„æ¨¡å‹ï¼ˆRNNã€Transformerï¼‰éƒ½æ— æ³•è¾¾åˆ°å®ç”¨æ ‡å‡†ã€‚

### 7.3 ä¼˜ç¼ºç‚¹æ€»ç»“

#### 7.3.1 RNNä¼˜ç¼ºç‚¹

**ä¼˜ç‚¹**ï¼š
1. âœ… æ¨¡å‹å°ï¼ˆ~23Må‚æ•°ï¼‰
2. âœ… ç©ºé—´å¤æ‚åº¦ä½ï¼ˆO(1)ï¼‰
3. âœ… å®ç°ç®€å•ã€æ˜“äºç†è§£
4. âœ… è®­ç»ƒé€Ÿåº¦ç›¸å¯¹è¾ƒå¿«

**ç¼ºç‚¹**ï¼š
1. âŒ **ç¿»è¯‘è´¨é‡æå·®**ï¼šENâ†’ZH BLEU=0.00ï¼Œå®Œå…¨ä¸å¯ç”¨
2. âŒ **ä¸¥é‡è¾“å‡ºé€€åŒ–**ï¼šé‡å¤ç”Ÿæˆé«˜é¢‘è¯å’Œæ ‡ç‚¹
3. âŒ æ¨ç†é€Ÿåº¦æ…¢ï¼ˆé¡ºåºè®¡ç®—ï¼‰
4. âŒ é•¿è·ç¦»ä¾èµ–å»ºæ¨¡å›°éš¾
5. âŒ åœ¨å°æ•°æ®é›†ä¸Šå‡ ä¹æ— æ³•æ”¶æ•›
6. âŒ æ‰©å±•æ€§å·®

**ç»“è®º**ï¼šRNNåœ¨10kæ•°æ®é›†ä¸Šå·²ä¸é€‚ç”¨äºç¿»è¯‘ä»»åŠ¡

#### 7.3.2 Transformerä¼˜ç¼ºç‚¹

**ä¼˜ç‚¹**ï¼š
1. âœ… ç¿»è¯‘è´¨é‡æ˜¾è‘—ä¼˜äºRNNï¼ˆ6.2å€ï¼‰
2. âœ… å¹¶è¡Œè®­ç»ƒã€æ¨ç†å¿«
3. âœ… é•¿è·ç¦»ä¾èµ–å»ºæ¨¡èƒ½åŠ›å¼º
4. âœ… å¯æ‰©å±•æ€§å¼º
5. âœ… æ³¨æ„åŠ›æœºåˆ¶å¯è§£é‡Š
6. âœ… èƒ½ç”Ÿæˆæœ‰æ„ä¹‰çš„è¾“å‡º

**ç¼ºç‚¹**ï¼š
1. âŒ ä»éœ€è¾ƒå¤šæ•°æ®å‘æŒ¥ä¼˜åŠ¿ï¼ˆ10kä¸å¤Ÿå……åˆ†ï¼‰
2. âŒ æ˜¾å­˜å ç”¨å¤§ï¼ˆO(nÂ²)ï¼‰
3. âŒ è¯­ä¹‰å‡†ç¡®æ€§ä¸€èˆ¬ï¼ˆBLEU~1.1ï¼‰
4. âŒ éœ€è¦ç²¾å¿ƒè®¾è®¡è®­ç»ƒç­–ç•¥

**ç»“è®º**ï¼šTransformeråœ¨å°æ•°æ®é›†ä¸Šè¡¨ç°ä¸­ç­‰ï¼Œå‹‰å¼ºå¯ç”¨

#### 7.3.3 T5ï¼ˆé¢„è®­ç»ƒæ¨¡å‹ï¼‰ä¼˜ç¼ºç‚¹

**ä¼˜ç‚¹**ï¼š
1. âœ… **ç¿»è¯‘è´¨é‡æœ€ä½³**ï¼šå¹³å‡BLEU=5.50ï¼Œæ˜¯Transformerçš„5å€
2. âœ… **é¢„è®­ç»ƒçŸ¥è¯†å¼ºå¤§**ï¼šåœ¨10kå°æ•°æ®é›†ä¸Šä»èƒ½å‘æŒ¥ä½œç”¨
3. âœ… **è¯­ä¹‰å‡†ç¡®åº¦é«˜**ï¼šç”Ÿæˆçš„ç¿»è¯‘åŸºæœ¬å¯ç”¨
4. âœ… **é€‚åˆä½èµ„æºåœºæ™¯**ï¼šé¢„è®­ç»ƒé™ä½äº†æ•°æ®éœ€æ±‚
5. âœ… Text-to-Textæ ¼å¼é€‚åˆç¿»è¯‘ä»»åŠ¡
6. âœ… å¤šè¯­è¨€èƒ½åŠ›ï¼ˆmT5ï¼‰æ”¯æŒä¸­è‹±ç¿»è¯‘

**ç¼ºç‚¹**ï¼š
1. âŒ æ¨¡å‹è§„æ¨¡å¤§ï¼ˆ300Må‚æ•°ï¼‰ï¼Œå¯¹ç¡¬ä»¶è¦æ±‚é«˜
2. âŒ å¾®è°ƒéœ€è¦æ›´å¤šGPUå†…å­˜
3. âŒ è®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼ˆbatch sizeå—é™ï¼‰
4. âŒ å¾®è°ƒç­–ç•¥å¤æ‚ï¼Œéœ€è¦careful tuning
5. âŒ ä»æœ‰ç»†èŠ‚ç¿»è¯‘åå·®

**ç»“è®º**ï¼šT5åœ¨å°æ•°æ®é›†ç¿»è¯‘ä»»åŠ¡ä¸Šä¼˜åŠ¿æ˜æ˜¾ï¼Œæ˜¯å®ç”¨é¦–é€‰

#### 7.3.4 ä¸‰ç§æ¨¡å‹ç»¼åˆè¯„ä»·

| ç»´åº¦ | RNN | Transformer | T5 | æ¨è |
|------|-----|-------------|-----|------|
| ç¿»è¯‘è´¨é‡ | âŒ æå·® | âš ï¸ ä¸­ç­‰ | âœ… è‰¯å¥½ | **T5** |
| è®­ç»ƒæˆæœ¬ | âœ… ä½ | âš ï¸ ä¸­ | âŒ é«˜ | RNN |
| æ•°æ®éœ€æ±‚ | âŒ é«˜ | âŒ é«˜ | âœ… ä½ | **T5** |
| æ¨ç†é€Ÿåº¦ | âŒ æ…¢ | âœ… å¿« | âš ï¸ ä¸­ | Transformer |
| æ˜“ç”¨æ€§ | âœ… ç®€å• | âš ï¸ ä¸­ | âŒ å¤æ‚ | RNN |
| å°æ•°æ®åœºæ™¯ | âŒ ä¸é€‚ç”¨ | âš ï¸ å‹‰å¼ºå¯ç”¨ | âœ… é€‚ç”¨ | **T5** |
| ç»¼åˆè¯„åˆ† | 1/5 | 3/5 | **4.5/5** | **T5** |

### 7.4 æœ¬é¡¹ç›®å®éªŒç»“è®º

åŸºäº10kæ•°æ®é›†çš„å®éªŒç»“æœï¼Œæˆ‘ä»¬å¾—å‡ºä»¥ä¸‹ç»“è®ºï¼š

1. **é¢„è®­ç»ƒæ¨¡å‹ä¼˜åŠ¿å·¨å¤§**ï¼šT5çš„å¹³å‡BLEUï¼ˆ5.50ï¼‰è¿œè¶…ä»å¤´è®­ç»ƒçš„Transformerï¼ˆ1.11ï¼‰å’ŒRNNï¼ˆ0.18ï¼‰ï¼Œè¯æ˜é¢„è®­ç»ƒçŸ¥è¯†åœ¨å°æ•°æ®é›†ä¸Šçš„ä»·å€¼

2. **Transformeræ¶æ„ä¼˜äºRNN**ï¼šåœ¨ä»å¤´è®­ç»ƒçš„æ¨¡å‹ä¸­ï¼ŒTransformerçš„BLEUåˆ†æ•°æ˜¯RNNçš„6.2å€ï¼Œæ˜¾ç¤ºå‡ºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å¼ºå¤§å»ºæ¨¡èƒ½åŠ›

3. **RNNåœ¨å°æ•°æ®é›†ä¸Šä¸¥é‡é€€åŒ–**ï¼š
   - ENâ†’ZHæ–¹å‘BLEUé™è‡³0.00ï¼Œè¾“å‡ºå®Œå…¨æ— æ„ä¹‰
   - ZHâ†’ENæ–¹å‘BLEUä»…0.36ï¼Œè™½ç•¥å¥½äºENâ†’ZHä½†ä»ç„¶å¾ˆå·®
   - ä¸¥é‡çš„é‡å¤ç”Ÿæˆé—®é¢˜ï¼ˆé«˜é¢‘è¯å’Œæ ‡ç‚¹ç¬¦å·ï¼‰

4. **æ•°æ®è§„æ¨¡å½±å“æ˜¾è‘—**ï¼š
   - ä»å¤´è®­ç»ƒæ¨¡å‹ï¼ˆRNNã€Transformerï¼‰çš„BLEUéƒ½å¾ˆä½ï¼Œ10kæ•°æ®ä¸è¶³
   - é¢„è®­ç»ƒæ¨¡å‹ï¼ˆT5ï¼‰èƒ½æœ‰æ•ˆåˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†ï¼Œåœ¨å°æ•°æ®é›†ä¸Šä»èƒ½è¾¾åˆ°å¯ç”¨æ°´å¹³

5. **ç¿»è¯‘æ–¹å‘å·®å¼‚**ï¼š
   - RNNï¼šZHâ†’ENï¼ˆ0.36ï¼‰æ˜¾è‘—å¥½äºENâ†’ZHï¼ˆ0.00ï¼‰ï¼Œå¯èƒ½å› ä¸ºè‹±æ–‡è¾“å‡ºæ ¼å¼æ›´è§„åˆ™
   - Transformer & T5ï¼šENâ†’ZHæ˜¾è‘—å¥½äºZHâ†’ENï¼Œè¯´æ˜æ›´å¼ºæ¨¡å‹èƒ½æ›´å¥½å¤„ç†ä¸­æ–‡ç”Ÿæˆ
   - T5åœ¨ENâ†’ZHä¸Šä¼˜åŠ¿æœ€æ˜æ˜¾ï¼ˆ8.75 vs 2.25ï¼‰ï¼Œé¢„è®­ç»ƒçŸ¥è¯†å¯¹å¤æ‚ç›®æ ‡è¯­è¨€å¸®åŠ©æ›´å¤§

6. **æ¨¡å‹å®¹é‡ä¸æ•°æ®é‡çš„å¹³è¡¡**ï¼š
   - RNNï¼ˆå°å®¹é‡ï¼‰ï¼šæ— æ³•å­¦åˆ°æœ‰æ•ˆæ¨¡å¼ï¼Œè¾“å‡ºé€€åŒ–
   - Transformerï¼ˆä¸­ç­‰å®¹é‡ï¼‰ï¼šèƒ½å­¦åˆ°éƒ¨åˆ†æ¨¡å¼ï¼Œä½†ä¸å¤Ÿå……åˆ†
   - T5ï¼ˆå¤§å®¹é‡+é¢„è®­ç»ƒï¼‰ï¼šé¢„è®­ç»ƒçŸ¥è¯†å¼¥è¡¥æ•°æ®ä¸è¶³ï¼Œè¡¨ç°æœ€ä½³

7. **å®ç”¨ä»·å€¼æ’åº**ï¼šT5ï¼ˆå¯ç”¨ï¼‰ >> Transformerï¼ˆå‹‰å¼ºå¯ç”¨ï¼‰ >> RNNï¼ˆä¸å¯ç”¨ï¼‰

### 7.5 æœªæ¥æ”¹è¿›æ–¹å‘

åŸºäºæœ¬é¡¹ç›®ç»éªŒï¼Œé’ˆå¯¹ä¸åŒæ¨¡å‹æå‡ºæ”¹è¿›å»ºè®®ï¼š

#### 7.5.1 T5æ¨¡å‹æ”¹è¿›ï¼ˆæ¨èé‡ç‚¹ï¼‰

T5å·²è¾¾åˆ°å¯ç”¨æ°´å¹³ï¼ˆBLEU=5.50ï¼‰ï¼Œè¿›ä¸€æ­¥æå‡ç©ºé—´ï¼š

1. **ä½¿ç”¨æ›´å¤§æ•°æ®é›†**ï¼š
   - è®­ç»ƒ100kå®Œæ•´æ•°æ®é›†ï¼Œé¢„æœŸBLEUæå‡è‡³10-15
   - ç»“åˆå¤–éƒ¨å¹³è¡Œè¯­æ–™ï¼ˆWMTã€OPUSï¼‰

2. **å‚æ•°é«˜æ•ˆå¾®è°ƒ**ï¼š
   - å®ç°LoRAã€Adapterç­‰æ–¹æ³•
   - é™ä½å¾®è°ƒæˆæœ¬ï¼Œæå‡æ³›åŒ–èƒ½åŠ›

3. **æ›´å¤§é¢„è®­ç»ƒæ¨¡å‹**ï¼š
   - å°è¯•mT5-baseï¼ˆ580Mï¼‰æˆ–mT5-largeï¼ˆ1.2Bï¼‰
   - é¢„æœŸBLEUè¿›ä¸€æ­¥æå‡3-5åˆ†

4. **è§£ç ä¼˜åŒ–**ï¼š
   - æŸæœç´¢ä¼˜åŒ–ï¼ˆbeam sizeã€é•¿åº¦æƒ©ç½šï¼‰
   - é‡æ’åºï¼ˆrerankingï¼‰æå‡è´¨é‡

#### 7.5.2 Transformeræ”¹è¿›ï¼ˆæ¬¡è¦ä¼˜å…ˆçº§ï¼‰

Transformerè¡¨ç°ä¸­ç­‰ï¼ˆBLEU=1.11ï¼‰ï¼Œæ”¹è¿›æ–¹å‘ï¼š

1. **å®Œæˆæ¶ˆèå®éªŒ**ï¼š
   - ä½ç½®ç¼–ç å¯¹æ¯”ã€å½’ä¸€åŒ–æ–¹æ³•å¯¹æ¯”
   - è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æï¼ˆå·²è®¾è®¡æœªæ‰§è¡Œï¼‰

2. **æ¨¡å‹è§„æ¨¡ä¼˜åŒ–**ï¼š
   - å°è¯•æ›´å¤§æ¨¡å‹ï¼ˆd_model=512, layers=6ï¼‰
   - åœ¨100kæ•°æ®ä¸Šè®­ç»ƒ

3. **è®­ç»ƒç­–ç•¥**ï¼š
   - Warmupå­¦ä¹ ç‡è°ƒåº¦
   - æ ‡ç­¾å¹³æ»‘ï¼ˆlabel smoothingï¼‰

#### 7.5.3 RNNæ”¹è¿›ï¼ˆä¸æ¨èï¼‰

RNNè¡¨ç°æå·®ï¼ˆBLEU=0.18ï¼‰ï¼ŒæŠ•å…¥äº§å‡ºæ¯”ä½ï¼š

1. åŒå‘LSTMå¯èƒ½ç•¥æœ‰å¸®åŠ©ï¼Œä½†ä¸ä¼šæ ¹æœ¬æ”¹å–„
2. å»ºè®®æ”¾å¼ƒRNNï¼Œè½¬å‘Transformeræˆ–T5

#### 7.5.4 é€šç”¨æ”¹è¿›

é€‚ç”¨äºæ‰€æœ‰æ¨¡å‹çš„æ–¹å‘ï¼š

1. **æ•°æ®å¢å¼º**ï¼š
   - å›è¯‘ï¼ˆback-translationï¼‰
   - åŒä¹‰æ›¿æ¢ã€å™ªå£°æ³¨å…¥

2. **å¤šä»»åŠ¡å­¦ä¹ **ï¼š
   - ç¿»è¯‘ + å»å™ª + åˆ†ç±»
   - å…±äº«ç¼–ç å™¨æå‡è¡¨ç¤ºèƒ½åŠ›

3. **è¯„ä¼°æ”¹è¿›**ï¼š
   - æ·»åŠ äººå·¥è¯„ä¼°
   - ä½¿ç”¨COMETç­‰ç¥ç»è¯„ä¼°æŒ‡æ ‡

**ç»¼åˆå»ºè®®**ï¼šä¼˜å…ˆæå‡T5æ¨¡å‹ï¼ˆå·²å¯ç”¨ä¸”æ•ˆæœæœ€å¥½ï¼‰ï¼ŒTransformerå¯ä½œä¸ºå¯¹æ¯”åŸºçº¿ï¼ŒRNNä¸å»ºè®®ç»§ç»­æŠ•å…¥ã€‚

---

## 8. å¯è§†åŒ–åˆ†æ

### 8.1 RNNæ¶ˆèå®éªŒå¯è§†åŒ–ï¼ˆZHâ†’ENæ–¹å‘ï¼‰

é¡¹ç›®é’ˆå¯¹ZHâ†’ENç¿»è¯‘æ–¹å‘ç”Ÿæˆäº†å®Œæ•´çš„å¯è§†åŒ–å›¾è¡¨ï¼Œå±•ç¤ºæ¶ˆèå®éªŒçš„è¯¦ç»†ç»“æœã€‚

#### 8.1.1 æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”å¯è§†åŒ–

**1. è®­ç»ƒæ›²çº¿**

å±•ç¤ºä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶ï¼ˆDotã€Multiplicativeã€Additiveï¼‰çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¯¹æ¯”è®­ç»ƒæŸå¤±ã€éªŒè¯æŸå¤±å’ŒBLEUåˆ†æ•°éšepochå˜åŒ–ã€‚

![æ³¨æ„åŠ›æœºåˆ¶è®­ç»ƒæ›²çº¿](results/rnn_ablation_visualizations/zh2en/attention_ablation/training_curves.png)

**2. BLEUçƒ­åŠ›å›¾**

å¯è§†åŒ–ä¸åŒæ³¨æ„åŠ›ç±»å‹å’Œè§£ç ç­–ç•¥ç»„åˆçš„BLEUåˆ†æ•°ï¼Œé¢œè‰²æ·±æµ…è¡¨ç¤ºæ€§èƒ½é«˜ä½ã€‚

![æ³¨æ„åŠ›æœºåˆ¶BLEUçƒ­åŠ›å›¾](results/rnn_ablation_visualizations/zh2en/attention_ablation/bleu_heatmap.png)

**3. æ³¨æ„åŠ›ç±»å‹å¯¹æ¯”**

ç›´è§‚å¯¹æ¯”ä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶çš„æ€§èƒ½å·®å¼‚ã€‚

![æ³¨æ„åŠ›ç±»å‹å¯¹æ¯”](results/rnn_ablation_visualizations/zh2en/attention_ablation/attention_comparison.png)

**4. è§£ç ç­–ç•¥å¯¹æ¯”**

å¯¹æ¯”è´ªå©ªè§£ç ä¸ä¸åŒbeam sizeæŸæœç´¢çš„æ•ˆæœã€‚

![æ³¨æ„åŠ›æœºåˆ¶è§£ç ç­–ç•¥å¯¹æ¯”](results/rnn_ablation_visualizations/zh2en/attention_ablation/decoding_comparison.png)

#### 8.1.2 è®­ç»ƒç­–ç•¥å¯¹æ¯”å¯è§†åŒ–

**1. è®­ç»ƒæ›²çº¿**

å±•ç¤ºä¸‰ç§è®­ç»ƒç­–ç•¥ï¼ˆTFã€SSã€FRï¼‰çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå¯¹æ¯”ä¸åŒTeacher Forcingæ¯”ä¾‹çš„å½±å“ã€‚

![è®­ç»ƒç­–ç•¥è®­ç»ƒæ›²çº¿](results/rnn_ablation_visualizations/zh2en/training_strategy_ablation/training_curves.png)

**2. BLEUçƒ­åŠ›å›¾**

å¯è§†åŒ–è®­ç»ƒç­–ç•¥å’Œè§£ç ç­–ç•¥ç»„åˆçš„BLEUåˆ†æ•°ã€‚

![è®­ç»ƒç­–ç•¥BLEUçƒ­åŠ›å›¾](results/rnn_ablation_visualizations/zh2en/training_strategy_ablation/bleu_heatmap.png)

**3. è®­ç»ƒç­–ç•¥å¯¹æ¯”**

ç›´è§‚å¯¹æ¯”Teacher Forcingã€Scheduled Samplingã€Free Runningçš„æ€§èƒ½ã€‚

![è®­ç»ƒç­–ç•¥å¯¹æ¯”](results/rnn_ablation_visualizations/zh2en/training_strategy_ablation/training_strategy_comparison.png)

**4. è§£ç ç­–ç•¥å¯¹æ¯”**

åœ¨ä¸åŒè®­ç»ƒç­–ç•¥ä¸‹å¯¹æ¯”è§£ç æ–¹æ³•çš„æ•ˆæœã€‚

![è®­ç»ƒç­–ç•¥è§£ç å¯¹æ¯”](results/rnn_ablation_visualizations/zh2en/training_strategy_ablation/decoding_comparison.png)

### 8.2 å¯è§†åŒ–åˆ†ææ€»ç»“

é€šè¿‡ä¸Šè¿°å›¾è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ™°è§‚å¯Ÿåˆ°ï¼š

1. **æ³¨æ„åŠ›æœºåˆ¶å½±å“**ï¼šä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶æ€§èƒ½ç›¸è¿‘ï¼Œç‚¹ç§¯æ³¨æ„åŠ›ç•¥ä¼˜ä¸”è®¡ç®—æ•ˆç‡æœ€é«˜
2. **è®­ç»ƒç­–ç•¥å½±å“**ï¼šScheduled Sampling (TF=0.5)è¡¨ç°æœ€å¥½ï¼Œå¹³è¡¡äº†è®­ç»ƒç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›
3. **è§£ç ç­–ç•¥å½±å“**ï¼šåœ¨æœ¬é¡¹ç›®ä¸­ï¼Œè´ªå©ªè§£ç åè€Œä¼˜äºæŸæœç´¢ï¼ˆæ•°æ®ä¸è¶³å¯¼è‡´çš„ç‰¹æ®Šç°è±¡ï¼‰
4. **è®­ç»ƒè¿‡ç¨‹**ï¼šæ‰€æœ‰æ¨¡å‹åœ¨å‰10ä¸ªepochå¿«é€Ÿä¸‹é™ï¼ŒåæœŸè¶‹äºå¹³ç¨³

è¿™äº›å¯è§†åŒ–ç»“æœéªŒè¯äº†æ¶ˆèå®éªŒçš„è®¾è®¡åˆç†æ€§ï¼Œå¹¶ä¸ºåç»­æ”¹è¿›æä¾›äº†æ–¹å‘ã€‚ä½†æ˜¯ç”±äºæ•°æ®é›†å¾ˆå°ï¼ŒBLEUå€¼å‡å¾ˆå°ï¼Œå‚è€ƒä»·å€¼ä¸é«˜ã€‚

---

## 9. ä¸ªäººåæ€

### 9.1 é¡¹ç›®æ”¶è·

#### 9.1.1 æŠ€æœ¯èƒ½åŠ›æå‡

1. **æ·±åº¦å­¦ä¹ å®è·µ**ï¼š
   - æŒæ¡äº†RNNå’ŒTransformerçš„å®ç°ç»†èŠ‚
   - ç†è§£äº†æ³¨æ„åŠ›æœºåˆ¶çš„å¤šç§å˜ä½“
   - å­¦ä¼šäº†PyTorchçš„é«˜çº§ç‰¹æ€§ï¼ˆè‡ªå®šä¹‰æ¨¡å‹ã€æ•°æ®æµï¼‰

2. **NLPå·¥ç¨‹èƒ½åŠ›**ï¼š
   - å®Œæ•´ç»å†æ•°æ®é¢„å¤„ç†â†’æ¨¡å‹è®­ç»ƒâ†’è¯„ä¼°â†’éƒ¨ç½²æµç¨‹
   - å­¦ä¼šä½¿ç”¨Hugging Faceç”Ÿæ€ï¼ˆtransformersåº“ï¼‰
   - æŒæ¡äº†BLEUç­‰è‡ªåŠ¨è¯„ä¼°æŒ‡æ ‡

3. **å®éªŒè®¾è®¡èƒ½åŠ›**ï¼š
   - å­¦ä¼šäº†æ¶ˆèå®éªŒçš„è®¾è®¡ä¸åˆ†æ
   - ç†è§£äº†è¶…å‚æ•°è°ƒä¼˜çš„ç³»ç»Ÿæ–¹æ³•
   - æŒæ¡äº†å¯è§†åŒ–åˆ†ææŠ€èƒ½

#### 9.1.2 ç†è®ºè®¤çŸ¥æ·±åŒ–

1. **æ¶æ„ç†è§£**ï¼š
   - æ·±åˆ»ç†è§£äº†Transformerç›¸æ¯”RNNçš„ä¼˜åŠ¿æ¥æº
   - è®¤è¯†åˆ°æ³¨æ„åŠ›æœºåˆ¶çš„é‡è¦æ€§
   - ç†è§£äº†ä½ç½®ç¼–ç ã€å±‚å½’ä¸€åŒ–ç­‰ç»„ä»¶çš„ä½œç”¨

2. **è®­ç»ƒç­–ç•¥**ï¼š
   - ç†è§£äº†Teacher Forcingçš„åˆ©å¼Š
   - è®¤è¯†åˆ°å­¦ä¹ ç‡é¢„çƒ­å¯¹Transformerçš„é‡è¦æ€§
   - å­¦ä¼šäº†æ ‡ç­¾å¹³æ»‘ç­‰æ­£åˆ™åŒ–æŠ€æœ¯

3. **é¢„è®­ç»ƒæ¨¡å‹**ï¼š
   - **é¢„è®­ç»ƒçŸ¥è¯†çš„å·¨å¤§ä»·å€¼**ï¼šT5åœ¨10kæ•°æ®ä¸Šçš„BLEUï¼ˆ5.50ï¼‰è¿œè¶…ä»å¤´è®­ç»ƒæ¨¡å‹ï¼Œè¯æ˜é¢„è®­ç»ƒæ˜¯å°æ•°æ®é›†åœºæ™¯çš„å…³é”®
   - ç†è§£äº†å¾®è°ƒç­–ç•¥çš„é‡è¦æ€§ï¼ˆå­¦ä¹ ç‡ã€warmupã€æ¢¯åº¦ç´¯ç§¯ç­‰ï¼‰
   - è®¤è¯†åˆ°æ¨¡å‹å®¹é‡ä¸é¢„è®­ç»ƒçŸ¥è¯†çš„æƒè¡¡ï¼šå¤§æ¨¡å‹éœ€è¦é¢„è®­ç»ƒæ”¯æ’‘

### 9.2 é‡åˆ°çš„æŒ‘æˆ˜

#### 9.2.1 æ•°æ®ç›¸å…³

**æŒ‘æˆ˜1ï¼šæ•°æ®é‡ä¸è¶³**
- åªä½¿ç”¨10kæ•°æ®å¯¼è‡´æ¨¡å‹æ€§èƒ½å—é™
- è§£å†³ï¼šä¼˜åŒ–æ¨¡å‹ç»“æ„ã€å¢å¼ºæ­£åˆ™åŒ–

**æŒ‘æˆ˜2ï¼šæ•°æ®è´¨é‡**
- éƒ¨åˆ†å¥å¯¹ç¿»è¯‘è´¨é‡å·®ã€å­˜åœ¨é”™è¯¯å¯¹é½
- è§£å†³ï¼šæ•°æ®æ¸…æ´—ã€è¿‡æ»¤å¼‚å¸¸æ ·æœ¬

#### 9.2.2 æ¨¡å‹ç›¸å…³

**æŒ‘æˆ˜3ï¼šRNNä¸¥é‡è¾“å‡ºé€€åŒ–**
- RNNåœ¨å°æ•°æ®é›†ä¸Šå®Œå…¨å¤±è´¥ï¼ŒENâ†’ZH BLEU=0.00
- å°è¯•æ·»åŠ é‡å¤æƒ©ç½šã€è°ƒæ•´Teacher Forcingç­‰å‡æ— æ•ˆ
- **ç»“è®º**ï¼šRNNä¸é€‚åˆå°æ•°æ®é›†ç¿»è¯‘ä»»åŠ¡ï¼Œåº”ä½¿ç”¨æ›´å¼ºæ¶æ„

**æŒ‘æˆ˜4ï¼šT5å¾®è°ƒçš„æ›²æŠ˜å†ç¨‹**
- **ç¬¬ä¸€æ¬¡å°è¯•**ï¼šå®Œå…¨å¤±è´¥ï¼ˆBLEUâ‰ˆ0ï¼‰ï¼Œè¾“å‡ºç‰¹æ®Šæ ‡è®°`<extra_id_0>`
- **é—®é¢˜åˆ†æ**ï¼šå­¦ä¹ ç‡è¿‡é«˜ã€å¾®è°ƒç­–ç•¥ä¸å½“ã€æ•°æ®æ ¼å¼é—®é¢˜
- **æ”¹è¿›å**ï¼šBLEUä»0è·ƒå‡è‡³5.50ï¼Œæˆä¸ºæœ€ä½³æ¨¡å‹ï¼
- **å…³é”®æ”¶è·**ï¼š
  - é¢„è®­ç»ƒæ¨¡å‹å¨åŠ›å·¨å¤§ï¼Œä½†éœ€è¦careful tuning
  - é€‚å½“çš„å­¦ä¹ ç‡ï¼ˆ1e-5ï¼‰å’Œwarmupè‡³å…³é‡è¦
  - å°æ•°æ®é›†åœºæ™¯ä¸‹ï¼Œé¢„è®­ç»ƒæ¨¡å‹æ˜¯æœ€ä½³é€‰æ‹©

#### 9.2.3 å·¥ç¨‹ç›¸å…³

**æŒ‘æˆ˜5ï¼šæ˜¾å­˜ä¸è¶³**
- Transformerå¤§batch_sizeå¯¼è‡´OOM
- è§£å†³ï¼šæ¢¯åº¦ç´¯ç§¯ã€æ··åˆç²¾åº¦è®­ç»ƒ

**æŒ‘æˆ˜6ï¼šè®­ç»ƒæ—¶é—´é•¿**
- T5å¾®è°ƒè€—æ—¶è¶…è¿‡2å°æ—¶
- è§£å†³ï¼šä»£ç ä¼˜åŒ–ã€ä½¿ç”¨æ•°æ®å¹¶è¡Œ

### 9.3 ä¸è¶³ä¸æ”¹è¿›

#### 9.3.1 æœ¬é¡¹ç›®ä¸è¶³

1. **æ•°æ®è§„æ¨¡**ï¼š
   - âŒ ä»…ä½¿ç”¨10kæ•°æ®ï¼Œæœªå……åˆ†è®­ç»ƒ
   - âœ… æ”¹è¿›ï¼šä½¿ç”¨100kå®Œæ•´æ•°æ®é›†

2. **æ¨¡å‹æ¶æ„**ï¼š
   - âŒ æœªå®ç°åŒå‘LSTM Encoder
   - âŒ Transformerå±‚æ•°è¾ƒå°‘ï¼ˆ3å±‚ï¼‰
   - âœ… æ”¹è¿›ï¼šå®ç°æ›´å¤æ‚çš„æ¶æ„å˜ä½“

3. **æ¶ˆèå®éªŒ**ï¼š
   - âŒ éƒ¨åˆ†æ¶ˆèå®éªŒä¸å¤Ÿæ·±å…¥
   - âŒ ç¼ºå°‘ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
   - âœ… æ”¹è¿›ï¼šå¤šæ¬¡è¿è¡Œå–å¹³å‡ã€ç½®ä¿¡åŒºé—´

4. **è¯„ä¼°æ–¹æ³•**ï¼š
   - âŒ ä»…ä½¿ç”¨BLEUï¼Œæœªè¿›è¡Œäººå·¥è¯„ä¼°
   - âŒ ç¼ºå°‘å…¶ä»–æŒ‡æ ‡ï¼ˆMETEORã€BERTScoreç­‰ï¼‰
   - âœ… æ”¹è¿›ï¼šå¤šæŒ‡æ ‡è¯„ä¼°ã€äººå·¥è¯„ä¼°

5. **T5å¾®è°ƒ**ï¼š
   - âŒ å¾®è°ƒç­–ç•¥ç®€å•ï¼Œæœªä½¿ç”¨å‚æ•°é«˜æ•ˆæ–¹æ³•
   - âŒ æœªå°è¯•å…¶ä»–é¢„è®­ç»ƒæ¨¡å‹ï¼ˆOPUS-MTç­‰ï¼‰
   - âœ… æ”¹è¿›ï¼šä½¿ç”¨LoRAã€å°è¯•æ›´å¤šæ¨¡å‹

#### 9.3.2 æ—¶é—´ç®¡ç†åæ€

1. å‰æœŸæ•°æ®é¢„å¤„ç†è€—æ—¶è¿‡å¤šï¼ˆåº”å¤ç”¨ç°æˆå·¥å…·ï¼‰
2. ä¸­æœŸåœ¨RNNç»†èŠ‚ä¸Šè¿‡åº¦ä¼˜åŒ–ï¼ˆåº”å°½æ—©è½¬å‘Transformerï¼‰
3. åæœŸT5å¾®è°ƒå¤±è´¥æŸå¤±æ—¶é—´ï¼ˆåº”é¢„å…ˆè°ƒç ”ï¼‰



### 9.5 æœªæ¥å±•æœ›

#### 9.5.1 çŸ­æœŸè®¡åˆ’

1. ä½¿ç”¨100kå®Œæ•´æ•°æ®é‡æ–°è®­ç»ƒæ‰€æœ‰æ¨¡å‹
2. å®ç°LoRAå¾®è°ƒT5æ¨¡å‹
3. å°è¯•æ›´å¤šè§£ç ç­–ç•¥ï¼ˆDiverse Beam Searchç­‰ï¼‰
4. æ·»åŠ äººå·¥è¯„ä¼°

#### 9.5.2 é•¿æœŸå…´è¶£

1. ç ”ç©¶ä½èµ„æºç¿»è¯‘ï¼ˆå¦‚æ— ç›‘ç£/åŠç›‘ç£ç¿»è¯‘ï¼‰
2. æ¢ç´¢å¤šæ¨¡æ€ç¿»è¯‘ï¼ˆæ–‡æœ¬+å›¾åƒï¼‰
3. ç ”ç©¶å¯æ§ç¿»è¯‘ï¼ˆé£æ ¼ã€è¯­æ°”æ§åˆ¶ï¼‰
4. å…³æ³¨å¤§è¯­è¨€æ¨¡å‹åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„åº”ç”¨ï¼ˆGPT-4ç¿»è¯‘èƒ½åŠ›ï¼‰

---


## é™„å½•

### A. ä»£ç ä»“åº“

- **GitHub**: [å¾…å¡«å†™æ‚¨çš„GitHub URL]
- **ä¸»è¦æ–‡ä»¶**ï¼š
  - `inference.py`: ä¸€é”®æ¨ç†è„šæœ¬
  - `src/models/rnn_seq2seq.py`: RNNæ¨¡å‹å®ç°
  - `src/models/transformer.py`: Transformeræ¨¡å‹å®ç°
  - `src/models/t5_finetune.py`: T5å¾®è°ƒå®ç°
  - `src/train_*.py`: è®­ç»ƒè„šæœ¬
  - `src/evaluate.py`: è¯„ä¼°è„šæœ¬

### B. ç¯å¢ƒé…ç½®

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# ä¸»è¦ä¾èµ–åŒ…
torch>=2.0.0
transformers>=4.30.0
jieba>=0.42.1
nltk>=3.8
sacrebleu>=2.3.1
matplotlib>=3.7.0
seaborn>=0.12.0
```

### C. å¿«é€Ÿå¼€å§‹

```bash
# 1. æ•°æ®é¢„å¤„ç†
python src/data_utils.py --preprocess

# 2. è®­ç»ƒRNNæ¨¡å‹
bash scripts/run_rnn_en2zh.sh

# 3. è®­ç»ƒTransformeræ¨¡å‹
bash scripts/run_transformer_en2zh.sh

# 4. è¯„ä¼°æ‰€æœ‰æ¨¡å‹
bash scripts/run_evaluation.sh

# 5. ä¸€é”®æ¨ç†
python inference.py --model transformer --input "Hello world" --direction en2zh
```

### D. å®éªŒç»“æœæ–‡ä»¶

æ‰€æœ‰å®éªŒç»“æœä¿å­˜åœ¨ä»¥ä¸‹ä½ç½®ï¼š
- æ¨¡å‹checkpoint: `experiments/*/checkpoints/`
- è¯„ä¼°ç»“æœ: `results/*_results.json`
- å¯è§†åŒ–å›¾è¡¨: `results/rnn_ablation_visualizations`
- è®­ç»ƒæ—¥å¿—: `experiments/*/train.log`

---



