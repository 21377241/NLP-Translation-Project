该课程作业为《自然语言处理与大型语言模型》课程的期中与期末项目，主题为“中英机器翻译”，要求学生分别基于RNN和Transformer架构实现机器翻译系统，并进行全面比较。以下是作业要求的详细总结：

---

### 一、总体目标
- 实现基于RNN和Transformer的中英双向机器翻译系统。
- 对比两种模型在架构、训练效率、翻译性能、扩展性等方面的差异。

---

### 二、具体任务要求

#### 1. RNN-based NMT（基于循环神经网络的神经机器翻译）
- **模型结构**：
  - 使用GRU或LSTM，编码器和解码器各包含两个单向层。
- **注意力机制**：
  - 实现注意力机制，比较不同对齐函数的效果：点积、乘性、加性注意力。
- **训练策略**：
  - 比较Teacher Forcing与Free Running训练策略的效果。
- **解码策略**：
  - 比较贪婪解码与束搜索解码的效果。

#### 2. Transformer-based NMT（基于Transformer的神经机器翻译）
- **从零开始训练**：
  - 实现完整的Encoder-Decoder Transformer架构，并进行训练。
- **架构消融研究**：
  - 比较不同位置编码方案（如绝对位置编码 vs 相对位置编码）和归一化方法（如LayerNorm vs RMSNorm）。
- **超参数敏感性分析**：
  - 调整批次大小、学习率、模型规模，分析其对性能的影响。
- **基于预训练模型的微调**：
  - 使用预训练语言模型（如T5）进行微调，并比较其与从头训练模型的性能。

#### 3. 分析与比较
对两种模型在以下方面进行全面对比：
- 模型架构（如顺序计算 vs 并行计算、循环 vs 自注意力）
- 训练效率（如训练时间、收敛速度、硬件需求）
- 翻译性能（如BLEU分数、流畅度、忠实度）
- 扩展性与泛化能力（如长句处理、低资源场景）
- 实际权衡（如模型大小、推理延迟、实现难度）

---

### 三、数据集说明
- 包含四个JSONL文件：
  - 小型训练集（100k句对）
  - 大型训练集（10k句对）
  - 验证集（500句对）
  - 测试集（200句对）
- 数据访问链接已提供，建议优先使用大型训练集，计算资源有限时可仅使用小型训练集。

---

### 四、数据预处理
- 数据清洗：去除非法字符、过滤低频词、截断过长句子。
- 分词：
  - 英文：可使用NLTK、BPE或WordPiece。
  - 中文：推荐使用Jieba或HanLP。
- 词表构建：基于分词结果构建词表，过滤低频词。
- 词嵌入初始化：建议使用预训练词向量并进行微调。

---

### 五、评估指标
- 主要使用**BLEU-4**作为自动评估指标，计算公式已给出。

---

### 六、提交要求
1. **源代码**：
   - 提交至GitHub仓库，包含一键推理脚本`inference.py`。
2. **项目报告**：
   - PDF格式，命名为“ID_姓名.pdf”。
   - 内容需包括：模型架构说明、代码实现与完成过程、实验结果分析（强调评分不依赖最终BLEU分数）、可视化分析、个人反思。
   - 报告首页需注明代码仓库URL。
3. **最终提交**：
   - 仅需提交项目报告至Piazza平台。
4. **截止日期**：2025年12月28日。

---

### 七、展示要求
- 每组展示10分钟，问答5分钟。
- 班级分为18组，每组约7人，可自行组队或随机分配。
- 每组需进行联合展示，但每位学生需独立提交个人报告。
- 展示截止日期同报告提交截止日期。

---

### 八、评分标准
- RNN-based NMT实现与实验：15%
- Transformer-based NMT实现与实验：25%
- 比较分析与讨论：5%
- 项目报告：5%
- 小组展示：50%

---

### 九、参考资料
- 教程：PyTorch Seq2Seq翻译教程
- 分词工具：Jieba（中文）、SentencePiece（多语言子词分词）
- 关键论文：
  - *Attention Is All You Need* (2017)
  - *Neural Machine Translation by Jointly Learning to Align and Translate* (2015)
  - *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer* (2020)
- 预训练模型权重：Hugging Face T5-base

---

