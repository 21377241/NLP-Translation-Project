# ä¸­è‹±æœºå™¨ç¿»è¯‘é¡¹ç›® - 48å°æ—¶ç´§æ€¥æ‰§è¡Œæ–¹æ¡ˆï¼ˆä¿®è®¢ç‰ˆï¼‰

**åˆ¶å®šæ—¶é—´**ï¼š2025å¹´12æœˆ26æ—¥ 20:00  
**æœ€åæ›´æ–°**ï¼š2025å¹´12æœˆ26æ—¥ 21:00  
**æˆªæ­¢æ—¶é—´**ï¼š2025å¹´12æœˆ28æ—¥ 23:59  
**å‰©ä½™æ—¶é—´**ï¼šçº¦48å°æ—¶  
**ç­–ç•¥å®šä½**ï¼šä¿è¯æ ¸å¿ƒåŠŸèƒ½å®Œæˆï¼Œç¡®ä¿å¯æäº¤

---

## âš ï¸ é‡è¦ä¿®æ­£ï¼ˆv2.0æ›´æ–°ï¼‰

### ä¿®æ­£1ï¼šRNNç¼–ç å™¨æ–¹å‘é—®é¢˜
**åŸé—®é¢˜**ï¼šåˆç‰ˆè®¡åˆ’ä½¿ç”¨äº†åŒå‘LSTMï¼Œä½†ä½œä¸šè¦æ±‚æ˜¯å•å‘å±‚  
**è§£å†³æ–¹æ¡ˆ**ï¼š
- âœ… é»˜è®¤ä½¿ç”¨**å•å‘LSTM/GRU**ï¼ˆä¸¥æ ¼ç¬¦åˆä½œä¸šè¦æ±‚ï¼‰
- âœ… å¯é€‰ï¼šå®ç°åŒå‘ç‰ˆæœ¬ä½œä¸ºå¯¹æ¯”å®éªŒï¼Œå¹¶åœ¨æŠ¥å‘Šä¸­è¯´æ˜
- âœ… æ›´æ–°äº†ä»£ç ç¤ºä¾‹å’Œé…ç½®æ–‡ä»¶

### ä¿®æ­£2ï¼šTransformerå®ç°å¤æ‚åº¦
**åŸé—®é¢˜**ï¼šä»é›¶å®ç°å®Œæ•´Transformeråœ¨3å°æ—¶å†…éš¾ä»¥ä¿è¯æ— bug  
**è§£å†³æ–¹æ¡ˆ**ï¼š
- âœ… **æ¨èä½¿ç”¨PyTorchå†…ç½®`nn.Transformer`**ï¼ˆç¨³å®šå¯é ï¼‰
- âœ… æä¾›äº†å®Œæ•´çš„å°è£…ä»£ç ï¼ˆåŒ…å«ä½ç½®ç¼–ç ï¼‰
- âœ… å¯é€‰ï¼šæœ‰æ—¶é—´å†å®ç°è‡ªå®šä¹‰ç‰ˆæœ¬è¿›è¡Œå¯¹æ¯”
- â±ï¸ é¢„è®¡èŠ‚çœ2-3å°æ—¶å¼€å‘å’Œè°ƒè¯•æ—¶é—´

### æ–°å¢3ï¼šå®éªŒç®¡ç†ç³»ç»Ÿ
**æ–°å¢å†…å®¹**ï¼šç‹¬ç«‹çš„å®éªŒç®¡ç†æ¡†æ¶  
**æ ¸å¿ƒåŠŸèƒ½**ï¼š
- âœ… æ¯ä¸ªå®éªŒæœ‰ç‹¬ç«‹çš„ç›®å½•ã€é…ç½®ã€ç»“æœæ–‡ä»¶
- âœ… ä½¿ç”¨YAMLé…ç½®æ–‡ä»¶ç®¡ç†è¶…å‚æ•°
- âœ… ç»Ÿä¸€çš„å®éªŒæ‰§è¡Œæ¥å£
- âœ… è‡ªåŠ¨åŒ–çš„å®éªŒå¯¹æ¯”å·¥å…·
- ğŸ¯ **ç¡®ä¿å®éªŒäº’ä¸å¹²æ‰°ï¼Œç»“æœå¯å¤ç°**

### å…³é”®ä¼˜åŠ¿
1. **æ›´ç¬¦åˆè¦æ±‚**ï¼šä¸¥æ ¼éµå¾ªä½œä¸šè§„å®šï¼ˆå•å‘å±‚ï¼‰
2. **æ›´å¿«å®ç°**ï¼šä½¿ç”¨æˆç†Ÿç»„ä»¶ï¼Œå‡å°‘è°ƒè¯•æ—¶é—´
3. **æ›´å¥½ç®¡ç†**ï¼šå®éªŒç³»ç»ŸåŒ–ï¼Œä¾¿äºæŠ¥å‘Šæ’°å†™
4. **æ›´æ˜“å¤ç°**ï¼šé…ç½®æ–‡ä»¶è®°å½•æ‰€æœ‰ç»†èŠ‚

---

## ğŸ“Š æ•°æ®é›†åˆ†ææŠ¥å‘Š

### æ•°æ®é›†æ¦‚å†µ
| æ–‡ä»¶å | æ•°é‡ | ç”¨é€” | ä¼˜å…ˆçº§ |
|--------|------|------|--------|
| `train_10k.jsonl` | 10,000æ¡ | å¿«é€Ÿè®­ç»ƒè¿­ä»£ | **P0 å¿…ç”¨** |
| `train_100k.jsonl` | 100,000æ¡ | å®Œæ•´è®­ç»ƒï¼ˆæ—¶é—´å…è®¸ï¼‰ | P1 å¯é€‰ |
| `valid.jsonl` | 500æ¡ | éªŒè¯é›† | P0 å¿…ç”¨ |
| `test.jsonl` | 200æ¡ | æµ‹è¯•é›†ï¼ˆæœ€ç»ˆè¯„ä¼°ï¼‰ | P0 å¿…ç”¨ |

### æ•°æ®æ ¼å¼
```json
{
  "en": "1929 or 1989?",
  "zh": "1929å¹´è¿˜æ˜¯1989å¹´?",
  "index": 0
}
```

### æ•°æ®ç‰¹ç‚¹åˆ†æ
- âœ… **æ ¼å¼ç»Ÿä¸€**ï¼šæ ‡å‡†JSON Linesæ ¼å¼ï¼Œæ˜“äºè§£æ
- âœ… **åŒè¯­å¯¹é½**ï¼šè‹±ä¸­å¥å¯¹ä¸€ä¸€å¯¹åº”
- âœ… **è´¨é‡è¾ƒé«˜**ï¼šæ¥è‡ªæ–°é—»è¯­æ–™ï¼Œå¥å­ç»“æ„å®Œæ•´
- âš ï¸ **é¢†åŸŸç‰¹å®š**ï¼šä¸»è¦æ˜¯æ–°é—»/æ”¿æ²»ç»æµç±»æ–‡æœ¬
- âš ï¸ **å¯èƒ½é—®é¢˜**ï¼šé•¿å¥è¾ƒå¤šã€ä¸“ä¸šæœ¯è¯­ã€æ ‡ç‚¹å·®å¼‚

### æ¨èç­–ç•¥
**ç¬¬ä¸€é˜¶æ®µ**ï¼šä»…ä½¿ç”¨ `train_10k.jsonl`ï¼ˆä¿è¯å¿«é€Ÿè¿­ä»£ï¼‰  
**ç¬¬äºŒé˜¶æ®µ**ï¼šå¦‚æœæ—¶é—´å……è£•ï¼Œä½¿ç”¨ `train_100k.jsonl` æå‡æ€§èƒ½

---

## â° 48å°æ—¶æ—¶é—´è½´ï¼ˆç²¾ç¡®åˆ°å°æ—¶ï¼‰

### ğŸ”¥ Day 1 - 12æœˆ26æ—¥ï¼ˆä»Šå¤©å‰©ä½™æ—¶é—´ + æ™šä¸Šï¼‰
**ç›®æ ‡ï¼šå®Œæˆç¯å¢ƒæ­å»º + æ•°æ®å¤„ç† + RNNåŸºç¡€å®ç°**

| æ—¶é—´ | ä»»åŠ¡ | å…·ä½“å†…å®¹ | æ£€æŸ¥ç‚¹ |
|------|------|----------|--------|
| 20:00-21:00 | ç¯å¢ƒæ­å»º | å®‰è£…ä¾èµ–ã€åˆ›å»ºé¡¹ç›®ç»“æ„ | `requirements.txt`ã€ç›®å½•åˆ›å»ºå®Œæˆ |
| 21:00-22:30 | æ•°æ®é¢„å¤„ç† | åˆ†è¯ã€æ„å»ºè¯è¡¨ã€æ•°æ®é›†ç±» | å¯ä»¥åŠ è½½batchæ•°æ® |
| 22:30-01:00 | RNNæ¨¡å‹å®ç° | Encoder+Decoder+Attention | æ¨¡å‹å¯ä»¥forward |
| 01:00-02:00 | è®­ç»ƒè„šæœ¬ | è®­ç»ƒå¾ªç¯ã€ä¿å­˜checkpoint | RNNå¼€å§‹è®­ç»ƒï¼Œlossä¸‹é™ |

### âš¡ Day 2 - 12æœˆ27æ—¥ï¼ˆå…¨å¤©ï¼‰
**ç›®æ ‡ï¼šå®ŒæˆTransformer + å®éªŒå¯¹æ¯” + æŠ¥å‘Šåˆç¨¿**

| æ—¶é—´ | ä»»åŠ¡ | å…·ä½“å†…å®¹ | æ£€æŸ¥ç‚¹ |
|------|------|----------|--------|
| 08:00-09:00 | ç»§ç»­RNNè®­ç»ƒ | ç›‘æ§è®­ç»ƒã€è°ƒæ•´è¶…å‚æ•° | RNNæ¨¡å‹è®­ç»ƒå®Œæˆ |
| 09:00-10:00 | è¯„ä¼°ç³»ç»Ÿ | BLEUè®¡ç®—ã€inference.py | èƒ½å¤Ÿè¯„ä¼°RNNæ¨¡å‹ |
| 10:00-13:00 | Transformerå®ç° | å®Œæ•´Transformeræ¶æ„ | Transformerå¯ä»¥è®­ç»ƒ |
| 13:00-14:00 | åˆé¤ä¼‘æ¯ | æ£€æŸ¥GPU/è®­ç»ƒè¿›åº¦ | - |
| 14:00-16:00 | Transformerè®­ç»ƒ | è®­ç»ƒTransformeræ¨¡å‹ | Transformerè®­ç»ƒå®Œæˆ |
| 16:00-17:00 | æ¨¡å‹å¯¹æ¯”å®éªŒ | ä¸¤æ¨¡å‹BLEUå¯¹æ¯”ã€ç”Ÿæˆæ ·ä¾‹ | å¯¹æ¯”æ•°æ®æ”¶é›†å®Œæˆ |
| 17:00-19:00 | æŠ¥å‘Šæ’°å†™ï¼ˆç¬¬ä¸€éƒ¨åˆ†ï¼‰ | æ¨¡å‹æ¶æ„ã€å®ç°è¿‡ç¨‹ | æŠ¥å‘Š50%å®Œæˆ |
| 19:00-20:00 | æ™šé¤ä¼‘æ¯ | ä»£ç æäº¤Git | - |
| 20:00-22:00 | æŠ¥å‘Šæ’°å†™ï¼ˆç¬¬äºŒéƒ¨åˆ†ï¼‰ | å®éªŒç»“æœã€åˆ†æè®¨è®º | æŠ¥å‘Š90%å®Œæˆ |
| 22:00-23:00 | å¯¹æ¯”å®éªŒï¼ˆå¦‚æœ‰æ—¶é—´ï¼‰ | æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”ç­‰ | é¢å¤–å®éªŒæ•°æ® |
| 23:00-00:00 | ä»£ç æ•´ç† | READMEã€æ³¨é‡Šå®Œå–„ | GitHubä»“åº“å®Œå–„ |

### ğŸ¯ Day 3 - 12æœˆ28æ—¥ï¼ˆæˆªæ­¢æ—¥ï¼‰
**ç›®æ ‡ï¼šæœ€åè°ƒæ•´ + æŠ¥å‘Šå®Œå–„ + æäº¤**

| æ—¶é—´ | ä»»åŠ¡ | å…·ä½“å†…å®¹ | æ£€æŸ¥ç‚¹ |
|------|------|----------|--------|
| 08:00-10:00 | æŠ¥å‘Šå®Œå–„ | æ·»åŠ å›¾è¡¨ã€ä¿®æ”¹æ ¼å¼ | æŠ¥å‘Š100%å®Œæˆ |
| 10:00-11:00 | inference.pyæµ‹è¯• | ç¡®ä¿æ¨ç†è„šæœ¬å®Œç¾è¿è¡Œ | å¯ä¸€é”®æ¨ç† |
| 11:00-12:00 | æœ€åæ£€æŸ¥ | æ‰€æœ‰æ–‡ä»¶ã€Gitã€PDF | - |
| 12:00-14:00 | ç¼“å†²æ—¶é—´ | å¤„ç†çªå‘é—®é¢˜ | - |
| 14:00-20:00 | **å®‰å…¨è¾¹ç•Œ** | å‡†å¤‡å±•ç¤ºPPTï¼ˆå¦‚éœ€è¦ï¼‰ | - |
| **23:59** | **æˆªæ­¢** | æäº¤æŠ¥å‘Šåˆ°Piazza | âœ… å®Œæˆ |

---

## ğŸ—ï¸ é¡¹ç›®ç»“æ„ï¼ˆå®é™…æ‰§è¡Œç‰ˆï¼‰

```
NLP/
â”œâ”€â”€ AP0004_Midterm&Final_translation_dataset_zh_en/  # æ•°æ®é›†ï¼ˆå·²æœ‰ï¼‰
â”‚   â”œâ”€â”€ train_10k.jsonl         # 10kè®­ç»ƒé›†
â”‚   â”œâ”€â”€ train_100k.jsonl        # 100kè®­ç»ƒé›†ï¼ˆå¤‡ç”¨ï¼‰
â”‚   â”œâ”€â”€ valid.jsonl             # éªŒè¯é›†
â”‚   â””â”€â”€ test.jsonl              # æµ‹è¯•é›†
â”‚
â”œâ”€â”€ data/                        # å¤„ç†åçš„æ•°æ®
â”‚   â”œâ”€â”€ vocab/                   # è¯è¡¨
â”‚   â”‚   â”œâ”€â”€ vocab_en.json       # è‹±æ–‡è¯è¡¨
â”‚   â”‚   â””â”€â”€ vocab_zh.json       # ä¸­æ–‡è¯è¡¨
â”‚   â””â”€â”€ processed/               # é¢„å¤„ç†åçš„æ•°æ®ï¼ˆå¯é€‰ï¼‰
â”‚
â”œâ”€â”€ experiments/                 # ğŸ”¥ å®éªŒç®¡ç†ç›®å½•ï¼ˆæ–°å¢ï¼‰
â”‚   â”œâ”€â”€ exp_001_rnn_baseline/   # å®éªŒ1ï¼šRNNåŸºçº¿
â”‚   â”‚   â”œâ”€â”€ config.yaml         # å®éªŒé…ç½®
â”‚   â”‚   â”œâ”€â”€ checkpoints/        # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â”‚   â”œâ”€â”€ logs/               # è®­ç»ƒæ—¥å¿—
â”‚   â”‚   â””â”€â”€ results.json        # å®éªŒç»“æœ
â”‚   â”œâ”€â”€ exp_002_transformer_baseline/  # å®éªŒ2ï¼šTransformeråŸºçº¿
â”‚   â”œâ”€â”€ exp_003_rnn_attention_comparison/  # å®éªŒ3ï¼šæ³¨æ„åŠ›å¯¹æ¯”
â”‚   â”œâ”€â”€ exp_004_transformer_ablation/      # å®éªŒ4ï¼šTransformeræ¶ˆè
â”‚   â””â”€â”€ summary.md              # æ‰€æœ‰å®éªŒæ€»ç»“
â”‚
â”œâ”€â”€ src/                         # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data_utils.py           # æ•°æ®å¤„ç†å·¥å…·
â”‚   â”‚   # - load_data()         # åŠ è½½JSONL
â”‚   â”‚   # - Tokenizerç±»         # ä¸­è‹±æ–‡åˆ†è¯
â”‚   â”‚   # - build_vocab()       # æ„å»ºè¯è¡¨
â”‚   â”‚   # - TranslationDataset  # PyTorch Dataset
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                 # æ¨¡å‹å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rnn_seq2seq.py     # RNNç¿»è¯‘æ¨¡å‹
â”‚   â”‚   â”‚   # - Encoderï¼ˆBiLSTMï¼Œ2å±‚ï¼‰
â”‚   â”‚   â”‚   # - Attentionï¼ˆDot-product/Additiveï¼‰
â”‚   â”‚   â”‚   # - Decoderï¼ˆLSTMï¼Œ2å±‚ï¼‰
â”‚   â”‚   â”‚   # - Seq2Seqï¼ˆå®Œæ•´æ¨¡å‹ï¼‰
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ transformer.py      # Transformeræ¨¡å‹
â”‚   â”‚       # - PositionalEncoding
â”‚   â”‚       # - MultiHeadAttention
â”‚   â”‚       # - TransformerEncoder
â”‚   â”‚       # - TransformerDecoder
â”‚   â”‚       # - Transformerï¼ˆå®Œæ•´æ¨¡å‹ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ train_rnn.py            # RNNè®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_transformer.py    # Transformerè®­ç»ƒè„šæœ¬
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluate.py             # è¯„ä¼°å·¥å…·
â”‚   â”‚   # - calculate_bleu()
â”‚   â”‚   # - translate_batch()
â”‚   â”‚
â”‚   â””â”€â”€ config.py               # é…ç½®æ–‡ä»¶ï¼ˆè¶…å‚æ•°ï¼‰
â”‚
â”œâ”€â”€ checkpoints/                # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ rnn_best.pt
â”‚   â””â”€â”€ transformer_best.pt
â”‚
â”œâ”€â”€ results/                    # å®éªŒç»“æœ
â”‚   â”œâ”€â”€ rnn_results.json       # RNNå®éªŒç»“æœ
â”‚   â”œâ”€â”€ transformer_results.json
â”‚   â”œâ”€â”€ translations/          # ç¿»è¯‘æ ·ä¾‹
â”‚   â””â”€â”€ figures/               # å¯è§†åŒ–å›¾è¡¨
â”‚
â”œâ”€â”€ docs/                       # æ–‡æ¡£
â”‚   â”œâ”€â”€ project_require.md     # ä½œä¸šè¦æ±‚
â”‚   â”œâ”€â”€ implementation_plan.md # åŸå§‹è®¡åˆ’
â”‚   â””â”€â”€ execution_plan_48h.md  # æœ¬æ–‡æ¡£ï¼ˆæ‰§è¡Œæ–¹æ¡ˆï¼‰
â”‚
â”œâ”€â”€ inference.py                # ğŸ”¥ ä¸€é”®æ¨ç†è„šæœ¬ï¼ˆå¿…éœ€ï¼ï¼‰
â”œâ”€â”€ requirements.txt            # ä¾èµ–åŒ…
â”œâ”€â”€ README.md                   # é¡¹ç›®è¯´æ˜
â””â”€â”€ .gitignore                  # Gitå¿½ç•¥æ–‡ä»¶
```

---

## ğŸ’» æŠ€æœ¯å®ç°ç»†èŠ‚

### 1ï¸âƒ£ æ•°æ®å¤„ç†æ¨¡å—ï¼ˆdata_utils.pyï¼‰

#### å…³é”®åŠŸèƒ½
```python
# 1. åŠ è½½æ•°æ®
def load_data(file_path):
    """åŠ è½½JSONLæ ¼å¼æ•°æ®"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    return data

# 2. ä¸­æ–‡åˆ†è¯ï¼ˆä½¿ç”¨jiebaï¼‰
def tokenize_zh(text):
    return list(jieba.cut(text))

# 3. è‹±æ–‡åˆ†è¯ï¼ˆç®€å•splitï¼‰
def tokenize_en(text):
    return text.lower().split()

# 4. æ„å»ºè¯è¡¨
def build_vocab(tokens_list, min_freq=2, max_size=30000):
    """
    æ„å»ºè¯è¡¨ï¼š<PAD>=0, <UNK>=1, <SOS>=2, <EOS>=3
    """
    counter = Counter([token for tokens in tokens_list for token in tokens])
    vocab = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}
    for word, freq in counter.most_common(max_size):
        if freq >= min_freq:
            vocab[word] = len(vocab)
    return vocab

# 5. Datasetç±»
class TranslationDataset(Dataset):
    def __init__(self, data, src_vocab, tgt_vocab, max_len=100):
        self.data = data
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
        self.max_len = max_len
    
    def __getitem__(self, idx):
        # è¿”å›ï¼šsrc_ids, tgt_ids, src_len, tgt_len
        pass
```

#### é¢„å¤„ç†æµç¨‹
```
åŸå§‹æ•°æ® â†’ åˆ†è¯ â†’ æ„å»ºè¯è¡¨ â†’ æ•°å­—åŒ– â†’ DataLoader
```

---

### 2ï¸âƒ£ RNNæ¨¡å‹å®ç°ï¼ˆrnn_seq2seq.pyï¼‰

#### âš ï¸ é‡è¦ä¿®æ­£ï¼šç¼–ç å™¨æ–¹å‘é—®é¢˜

**ä½œä¸šè¦æ±‚**ï¼šç¼–ç å™¨å’Œè§£ç å™¨å„åŒ…å«ä¸¤ä¸ª**å•å‘å±‚**ï¼ˆunidirectional layersï¼‰

**æ–¹æ¡ˆè°ƒæ•´**ï¼š
1. **æ ‡å‡†å®ç°**ï¼ˆæ¨èï¼‰ï¼šä½¿ç”¨å•å‘LSTM/GRUï¼Œç¬¦åˆä½œä¸šè¦æ±‚
2. **å¢å¼ºå®ç°**ï¼ˆå¯é€‰ï¼‰ï¼šä½¿ç”¨åŒå‘ç¼–ç å™¨ï¼Œä½†éœ€åœ¨æŠ¥å‘Šä¸­è¯´æ˜å¹¶å¯¹æ¯”å•å‘/åŒå‘æ•ˆæœ

#### æ¨¡å‹æ¶æ„ï¼ˆå•å‘ç‰ˆæœ¬ - ç¬¦åˆè¦æ±‚ï¼‰
```python
class Encoder(nn.Module):
    """
    å•å‘ç¼–ç å™¨ï¼ˆç¬¦åˆä½œä¸šè¦æ±‚ï¼‰
    è¾“å…¥ï¼šæºè¯­è¨€åºåˆ— [batch, src_len]
    è¾“å‡ºï¼šéšè—çŠ¶æ€ [batch, src_len, hidden], æœ€ç»ˆçŠ¶æ€
    """
    def __init__(self, vocab_size, embed_dim, hidden_dim, n_layers=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, 
                           bidirectional=False,  # å•å‘ï¼
                           batch_first=True, 
                           dropout=0.3 if n_layers > 1 else 0)

class Attention(nn.Module):
    """
    æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¯åˆ‡æ¢ä¸åŒç±»å‹ï¼‰
    æ”¯æŒï¼šç‚¹ç§¯ï¼ˆdot-productï¼‰ã€åŠ æ€§ï¼ˆadditiveï¼‰ã€ä¹˜æ€§ï¼ˆmultiplicativeï¼‰
    """
    def __init__(self, hidden_dim, attn_type='dot'):
        super().__init__()
        self.attn_type = attn_type
        if attn_type == 'additive':
            # Bahdanauæ³¨æ„åŠ›
            self.W1 = nn.Linear(hidden_dim, hidden_dim)
            self.W2 = nn.Linear(hidden_dim, hidden_dim)
            self.v = nn.Linear(hidden_dim, 1)
        elif attn_type == 'multiplicative':
            self.W = nn.Linear(hidden_dim, hidden_dim)
    
    def forward(self, query, keys, values, mask=None):
        # query: [batch, hidden] (decoder hidden state)
        # keys: [batch, src_len, hidden] (encoder outputs)
        # values: [batch, src_len, hidden] (same as keys)
        
        if self.attn_type == 'dot':
            # ç‚¹ç§¯æ³¨æ„åŠ›
            scores = torch.matmul(query.unsqueeze(1), keys.transpose(1, 2))
            # scores: [batch, 1, src_len]
        elif self.attn_type == 'additive':
            # åŠ æ€§æ³¨æ„åŠ›ï¼ˆBahdanauï¼‰
            q = self.W1(query).unsqueeze(1)  # [batch, 1, hidden]
            k = self.W2(keys)  # [batch, src_len, hidden]
            scores = self.v(torch.tanh(q + k))  # [batch, src_len, 1]
            scores = scores.transpose(1, 2)  # [batch, 1, src_len]
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e10)
        
        attn_weights = F.softmax(scores, dim=-1)  # [batch, 1, src_len]
        context = torch.matmul(attn_weights, values)  # [batch, 1, hidden]
        
        return context.squeeze(1), attn_weights.squeeze(1)
        
class Decoder(nn.Module):
    """
    å¸¦æ³¨æ„åŠ›çš„å•å‘è§£ç å™¨ï¼ˆç¬¦åˆä½œä¸šè¦æ±‚ï¼‰
    """
    def __init__(self, vocab_size, embed_dim, hidden_dim, n_layers=2, attn_type='dot'):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.attention = Attention(hidden_dim, attn_type)
        # è¾“å…¥æ˜¯embedding + context vector
        self.lstm = nn.LSTM(embed_dim + hidden_dim, hidden_dim, n_layers,
                           batch_first=True,
                           dropout=0.3 if n_layers > 1 else 0)
        self.fc = nn.Linear(hidden_dim, vocab_size)
```

#### è®­ç»ƒç­–ç•¥
- **Teacher Forcing**ï¼šè§£ç å™¨è¾“å…¥ä½¿ç”¨çœŸå®ç›®æ ‡åºåˆ—ï¼ˆè®­ç»ƒåˆæœŸï¼‰
- **Free Running**ï¼šè§£ç å™¨è¾“å…¥ä½¿ç”¨è‡ªå·±çš„é¢„æµ‹ï¼ˆå¯é€‰å¯¹æ¯”å®éªŒï¼‰

#### è¶…å‚æ•°è®¾ç½®ï¼ˆå¿«é€Ÿç‰ˆ - ç¬¦åˆä½œä¸šè¦æ±‚ï¼‰
```python
CONFIG_RNN = {
    # æ¨¡å‹æ¶æ„ï¼ˆä¸¥æ ¼ç¬¦åˆä½œä¸šè¦æ±‚ï¼‰
    'embed_dim': 256,
    'hidden_dim': 256,
    'n_layers': 2,              # ç¼–ç å™¨å’Œè§£ç å™¨å„2å±‚
    'bidirectional': False,     # âš ï¸ å•å‘ï¼ˆunidirectionalï¼‰
    'dropout': 0.3,
    'attention_type': 'dot',    # dot, additive, multiplicative
    
    # è®­ç»ƒå‚æ•°
    'batch_size': 64,
    'learning_rate': 0.001,
    'epochs': 15,
    'max_len': 100,
    'grad_clip': 1.0,
    'teacher_forcing_ratio': 1.0,  # Teacher Forcing vs Free Running
    
    # è¯è¡¨
    'vocab_size': 30000,
    'min_freq': 2,
}

# å¯é€‰ï¼šåŒå‘ç¼–ç å™¨ç‰ˆæœ¬ï¼ˆç”¨äºå¯¹æ¯”å®éªŒï¼‰
CONFIG_RNN_BIDIRECTIONAL = {
    **CONFIG_RNN,
    'bidirectional': True,  # ä½¿ç”¨åŒå‘ç¼–ç å™¨
    # æ³¨æ„ï¼šä½¿ç”¨åŒå‘æ—¶ï¼Œéœ€è¦åœ¨æŠ¥å‘Šä¸­è¯´æ˜å¹¶å¯¹æ¯”å•å‘/åŒå‘çš„å·®å¼‚
}
```

---

### 3ï¸âƒ£ Transformeræ¨¡å‹å®ç°ï¼ˆtransformer.pyï¼‰

#### âš ï¸ é‡è¦ç®€åŒ–ï¼šä½¿ç”¨PyTorchå†…ç½®Transformer

**é—®é¢˜**ï¼šä»é›¶å®ç°å®Œæ•´Transformeråœ¨3å°æ—¶å†…å¾ˆéš¾ä¿è¯æ— bugè¿è¡Œ

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨PyTorchå†…ç½®çš„`nn.Transformer`ï¼Œå¤§å¹…ç®€åŒ–å®ç°

#### æ¨èå®ç°ï¼ˆä½¿ç”¨å†…ç½®æ¨¡å—ï¼‰

```python
import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç ï¼ˆå¿…é¡»è‡ªå·±å®ç°ï¼Œå› ä¸ºå†…ç½®Transformerä¸åŒ…å«ï¼‰"""
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                            (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        # x: [batch, seq_len, d_model]
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)

class TransformerNMT(nn.Module):
    """
    ä½¿ç”¨PyTorchå†…ç½®Transformerçš„ç¿»è¯‘æ¨¡å‹
    ä¼˜åŠ¿ï¼šç¨³å®šå¯é ã€å®ç°ç®€å•ã€3å°æ—¶å†…å¯å®Œæˆ
    """
    def __init__(self, src_vocab_size, tgt_vocab_size, 
                 d_model=256, nhead=4, num_encoder_layers=3, 
                 num_decoder_layers=3, dim_feedforward=1024, dropout=0.1):
        super().__init__()
        
        self.d_model = d_model
        
        # è¯åµŒå…¥å±‚
        self.src_embedding = nn.Embedding(src_vocab_size, d_model, padding_idx=0)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model, padding_idx=0)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)
        
        # ğŸ”¥ ä½¿ç”¨PyTorchå†…ç½®Transformerï¼ˆæ ¸å¿ƒç®€åŒ–ï¼‰
        self.transformer = nn.Transformer(
            d_model=d_model,
            nhead=nhead,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True  # ä½¿ç”¨batch_firstæ ¼å¼
        )
        
        # è¾“å‡ºå±‚
        self.fc_out = nn.Linear(d_model, tgt_vocab_size)
        
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        initrange = 0.1
        self.src_embedding.weight.data.uniform_(-initrange, initrange)
        self.tgt_embedding.weight.data.uniform_(-initrange, initrange)
        self.fc_out.bias.data.zero_()
        self.fc_out.weight.data.uniform_(-initrange, initrange)
    
    def generate_square_subsequent_mask(self, sz):
        """ç”Ÿæˆdecoderçš„causal maskï¼ˆä¸‹ä¸‰è§’ï¼‰"""
        mask = torch.triu(torch.ones(sz, sz), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask
    
    def forward(self, src, tgt, src_padding_mask=None, tgt_padding_mask=None):
        """
        src: [batch, src_len]
        tgt: [batch, tgt_len]
        """
        # ç”Ÿæˆtgt maskï¼ˆé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰
        tgt_len = tgt.size(1)
        tgt_mask = self.generate_square_subsequent_mask(tgt_len).to(tgt.device)
        
        # Embedding + Positional Encoding
        src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))
        tgt_emb = self.pos_encoder(self.tgt_embedding(tgt) * math.sqrt(self.d_model))
        
        # Transformer forward
        output = self.transformer(
            src_emb, tgt_emb,
            tgt_mask=tgt_mask,
            src_key_padding_mask=src_padding_mask,
            tgt_key_padding_mask=tgt_padding_mask
        )
        
        # è¾“å‡ºæŠ•å½±
        logits = self.fc_out(output)
        return logits
    
    def translate(self, src, max_len=100, sos_idx=2, eos_idx=3):
        """è´ªå©ªè§£ç ï¼ˆæ¨ç†æ—¶ä½¿ç”¨ï¼‰"""
        self.eval()
        device = src.device
        batch_size = src.size(0)
        
        # ç¼–ç æºåºåˆ—
        src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))
        memory = self.transformer.encoder(src_emb)
        
        # åˆå§‹åŒ–ç›®æ ‡åºåˆ—ï¼ˆ<SOS>ï¼‰
        tgt = torch.full((batch_size, 1), sos_idx, dtype=torch.long, device=device)
        
        for _ in range(max_len):
            tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(device)
            tgt_emb = self.pos_encoder(self.tgt_embedding(tgt) * math.sqrt(self.d_model))
            
            output = self.transformer.decoder(tgt_emb, memory, tgt_mask=tgt_mask)
            logits = self.fc_out(output[:, -1, :])  # åªå–æœ€åä¸€ä¸ªtoken
            
            next_token = logits.argmax(dim=-1, keepdim=True)
            tgt = torch.cat([tgt, next_token], dim=1)
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åºåˆ—éƒ½ç”Ÿæˆäº†<EOS>
            if (next_token == eos_idx).all():
                break
        
        return tgt[:, 1:]  # å»æ‰<SOS>
```

#### å¯é€‰ï¼šä»é›¶å®ç°ï¼ˆä»…å½“æœ‰å……è¶³æ—¶é—´ï¼‰

å¦‚æœæ—¶é—´å…è®¸ä¸”æƒ³å±•ç¤ºæ›´å¤šæŠ€æœ¯ç»†èŠ‚ï¼Œå¯ä»¥å®ç°ä»¥ä¸‹ç»„ä»¶ï¼š

```python
class MultiHeadAttention(nn.Module):
    """è‡ªå®šä¹‰å¤šå¤´æ³¨æ„åŠ›ï¼ˆç”¨äºæ¶ˆèå®éªŒï¼‰"""
    pass

class TransformerEncoderLayer(nn.Module):
    """è‡ªå®šä¹‰ç¼–ç å™¨å±‚ï¼ˆç”¨äºä¿®æ”¹å½’ä¸€åŒ–æ–¹å¼ç­‰ï¼‰"""
    pass
```

**å»ºè®®**ï¼šå…ˆç”¨å†…ç½®ç‰ˆæœ¬è·‘é€šï¼Œæœ‰æ—¶é—´å†å®ç°è‡ªå®šä¹‰ç‰ˆæœ¬è¿›è¡Œå¯¹æ¯”

#### è¶…å‚æ•°è®¾ç½®ï¼ˆå¿«é€Ÿç‰ˆï¼‰
```python
CONFIG_TRANSFORMER = {
    'd_model': 256,
    'n_heads': 4,
    'n_layers': 3,  # Encoderå’ŒDecoderå„3å±‚
    'd_ff': 1024,
    'dropout': 0.1,
    'batch_size': 64,
    'learning_rate': 0.0001,
    'epochs': 15,
    'max_len': 100,
}
```

---

### 4ï¸âƒ£ è®­ç»ƒè„šæœ¬ï¼ˆtrain_rnn.py / train_transformer.pyï¼‰

#### é€šç”¨è®­ç»ƒæµç¨‹
```python
def train(model, train_loader, valid_loader, config):
    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥PAD
    
    best_bleu = 0
    for epoch in range(config['epochs']):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        for batch in tqdm(train_loader):
            src, tgt = batch
            output = model(src, tgt[:, :-1])  # Teacher forcing
            loss = criterion(output.view(-1, vocab_size), tgt[:, 1:].view(-1))
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            train_loss += loss.item()
        
        # éªŒè¯é˜¶æ®µ
        bleu = evaluate(model, valid_loader)
        print(f"Epoch {epoch}: Loss={train_loss:.4f}, BLEU={bleu:.2f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if bleu > best_bleu:
            best_bleu = bleu
            torch.save(model.state_dict(), 'checkpoints/model_best.pt')
```

---

### 5ï¸âƒ£ è¯„ä¼°ç³»ç»Ÿï¼ˆevaluate.pyï¼‰

#### BLEU-4è®¡ç®—
```python
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu

def calculate_bleu(model, test_loader, vocab_tgt):
    """è®¡ç®—BLEUåˆ†æ•°"""
    references = []
    hypotheses = []
    
    model.eval()
    with torch.no_grad():
        for batch in test_loader:
            src, tgt = batch
            pred = model.translate(src)  # è´ªå©ªè§£ç æˆ–æŸæœç´¢
            
            # è½¬æ¢ä¸ºæ–‡æœ¬
            for i in range(len(pred)):
                ref = ids_to_tokens(tgt[i], vocab_tgt)
                hyp = ids_to_tokens(pred[i], vocab_tgt)
                references.append([ref])
                hypotheses.append(hyp)
    
    bleu = corpus_bleu(references, hypotheses)
    return bleu * 100
```

---

### 6ï¸âƒ£ æ¨ç†è„šæœ¬ï¼ˆinference.pyï¼‰ğŸ”¥

```python
#!/usr/bin/env python3
"""
ä¸€é”®æ¨ç†è„šæœ¬ - å¿…éœ€æ–‡ä»¶
ç”¨æ³•ï¼špython inference.py --model rnn --input "Hello world"
"""
import argparse
import torch
from src.models.rnn_seq2seq import Seq2Seq
from src.models.transformer import Transformer
from src.data_utils import Tokenizer, load_vocab

def translate(text, model, src_vocab, tgt_vocab, device='cpu'):
    """ç¿»è¯‘å•ä¸ªå¥å­"""
    model.eval()
    tokens = tokenize(text)
    ids = [src_vocab.get(t, 1) for t in tokens]  # 1=<UNK>
    src_tensor = torch.LongTensor([ids]).to(device)
    
    with torch.no_grad():
        output_ids = model.translate(src_tensor, max_len=100)
    
    output_tokens = [tgt_vocab[id] for id in output_ids]
    return ' '.join(output_tokens)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', choices=['rnn', 'transformer'], required=True)
    parser.add_argument('--input', type=str, required=True)
    parser.add_argument('--direction', choices=['en2zh', 'zh2en'], default='en2zh')
    args = parser.parse_args()
    
    # åŠ è½½æ¨¡å‹
    model = load_model(args.model)
    vocab_src, vocab_tgt = load_vocabs(args.direction)
    
    # ç¿»è¯‘
    result = translate(args.input, model, vocab_src, vocab_tgt)
    print(f"è¾“å…¥ï¼š{args.input}")
    print(f"ç¿»è¯‘ï¼š{result}")
```

---

## ğŸ“ å®éªŒç®¡ç†ç³»ç»Ÿï¼ˆç‹¬ç«‹äº’ä¸å¹²æ‰°ï¼‰

### ä¸ºä»€ä¹ˆéœ€è¦å®éªŒç®¡ç†ï¼Ÿ

åœ¨å¤šä¸ªå®éªŒä¹‹é—´åˆ‡æ¢æ—¶ï¼Œå®¹æ˜“å‡ºç°ï¼š
- âŒ é…ç½®æ··ä¹±ï¼ˆä¸çŸ¥é“å“ªä¸ªæ¨¡å‹ç”¨äº†ä»€ä¹ˆè¶…å‚æ•°ï¼‰
- âŒ ç»“æœè¦†ç›–ï¼ˆæ–°å®éªŒè¦†ç›–æ—§å®éªŒçš„checkpointï¼‰
- âŒ æ— æ³•å¤ç°ï¼ˆå¿˜è®°æŸä¸ªå®éªŒçš„å…·ä½“è®¾ç½®ï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼šä¸ºæ¯ä¸ªå®éªŒåˆ›å»ºç‹¬ç«‹çš„ç›®å½•å’Œé…ç½®æ–‡ä»¶

---

### å®éªŒç›®å½•ç»“æ„

```
experiments/
â”œâ”€â”€ exp_001_rnn_baseline/              # å®éªŒ1ï¼šRNNåŸºçº¿æ¨¡å‹
â”‚   â”œâ”€â”€ config.yaml                    # å®éªŒé…ç½®ï¼ˆè¶…å‚æ•°ã€æ•°æ®è·¯å¾„ç­‰ï¼‰
â”‚   â”œâ”€â”€ checkpoints/                   # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â”‚   â”œâ”€â”€ model_epoch_5.pt
â”‚   â”‚   â”œâ”€â”€ model_epoch_10.pt
â”‚   â”‚   â””â”€â”€ model_best.pt             # æœ€ä½³æ¨¡å‹
â”‚   â”œâ”€â”€ logs/                          # è®­ç»ƒæ—¥å¿—
â”‚   â”‚   â””â”€â”€ train.log
â”‚   â”œâ”€â”€ results/                       # å®éªŒç»“æœ
â”‚   â”‚   â”œâ”€â”€ metrics.json              # BLEUã€Lossç­‰æŒ‡æ ‡
â”‚   â”‚   â”œâ”€â”€ translations.txt          # ç¿»è¯‘æ ·ä¾‹
â”‚   â”‚   â””â”€â”€ figures/                  # å›¾è¡¨
â”‚   â”‚       â”œâ”€â”€ loss_curve.png
â”‚   â”‚       â””â”€â”€ bleu_curve.png
â”‚   â””â”€â”€ README.md                      # å®éªŒè¯´æ˜
â”‚
â”œâ”€â”€ exp_002_transformer_baseline/      # å®éªŒ2ï¼šTransformeråŸºçº¿
â”‚   â””â”€â”€ (åŒä¸Šç»“æ„)
â”‚
â”œâ”€â”€ exp_003_rnn_dot_vs_additive/       # å®éªŒ3ï¼šRNNæ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”
â”‚   â”œâ”€â”€ dot_attention/                 # å­å®éªŒï¼šç‚¹ç§¯æ³¨æ„åŠ›
â”‚   â”œâ”€â”€ additive_attention/            # å­å®éªŒï¼šåŠ æ€§æ³¨æ„åŠ›
â”‚   â””â”€â”€ comparison.md                  # å¯¹æ¯”ç»“æœ
â”‚
â”œâ”€â”€ exp_004_transformer_ablation/      # å®éªŒ4ï¼šTransformeræ¶ˆèå®éªŒ
â”‚   â”œâ”€â”€ with_pos_encoding/
â”‚   â”œâ”€â”€ without_pos_encoding/
â”‚   â””â”€â”€ comparison.md
â”‚
â””â”€â”€ summary.md                         # ğŸ”¥ æ‰€æœ‰å®éªŒæ€»ç»“ï¼ˆç”¨äºæŠ¥å‘Šï¼‰
```

---

### é…ç½®æ–‡ä»¶æ¨¡æ¿ï¼ˆconfig.yamlï¼‰

```yaml
# exp_001_rnn_baseline/config.yaml
experiment:
  name: "RNN Baseline"
  id: "exp_001"
  description: "RNN with dot-product attention (unidirectional)"
  date: "2025-12-26"

model:
  type: "rnn_seq2seq"
  embed_dim: 256
  hidden_dim: 256
  n_layers: 2
  dropout: 0.3
  attention_type: "dot"  # dot, additive, multiplicative
  bidirectional: false   # å•å‘ï¼ˆç¬¦åˆè¦æ±‚ï¼‰

data:
  train_file: "AP0004_Midterm&Final_translation_dataset_zh_en/train_10k.jsonl"
  valid_file: "AP0004_Midterm&Final_translation_dataset_zh_en/valid.jsonl"
  test_file: "AP0004_Midterm&Final_translation_dataset_zh_en/test.jsonl"
  max_len: 100
  min_freq: 2
  vocab_size: 30000

training:
  batch_size: 64
  learning_rate: 0.001
  epochs: 15
  optimizer: "adam"
  grad_clip: 1.0
  teacher_forcing_ratio: 1.0
  early_stopping_patience: 3

evaluation:
  beam_size: 1  # 1=greedy, >1=beam search
  metrics: ["bleu-4", "loss"]

output:
  checkpoint_dir: "experiments/exp_001_rnn_baseline/checkpoints"
  log_dir: "experiments/exp_001_rnn_baseline/logs"
  results_dir: "experiments/exp_001_rnn_baseline/results"
```

---

### å®éªŒç®¡ç†è„šæœ¬

#### 1. åˆ›å»ºæ–°å®éªŒ
```bash
# scripts/create_experiment.sh
#!/bin/bash
EXP_ID=$1
EXP_NAME=$2

mkdir -p experiments/${EXP_ID}_{$EXP_NAME}/{checkpoints,logs,results/figures}
echo "Created experiment: ${EXP_ID}"
```

#### 2. è¿è¡Œå®éªŒï¼ˆç»Ÿä¸€æ¥å£ï¼‰
```python
# scripts/run_experiment.py
import yaml
import argparse

def run_experiment(config_path):
    """æ ¹æ®é…ç½®æ–‡ä»¶è¿è¡Œå®éªŒ"""
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    exp_id = config['experiment']['id']
    model_type = config['model']['type']
    
    print(f"Running experiment: {exp_id}")
    print(f"Model type: {model_type}")
    
    if model_type == 'rnn_seq2seq':
        from src.train_rnn import train_rnn
        train_rnn(config)
    elif model_type == 'transformer':
        from src.train_transformer import train_transformer
        train_transformer(config)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', required=True, help='Path to config.yaml')
    args = parser.parse_args()
    
    run_experiment(args.config)
```

**ä½¿ç”¨æ–¹æ³•**ï¼š
```bash
python scripts/run_experiment.py --config experiments/exp_001_rnn_baseline/config.yaml
```

---

### å®éªŒç»“æœè®°å½•ï¼ˆresults/metrics.jsonï¼‰

```json
{
  "experiment_id": "exp_001",
  "experiment_name": "RNN Baseline",
  "date": "2025-12-26",
  "status": "completed",
  
  "training": {
    "total_epochs": 15,
    "best_epoch": 12,
    "training_time_hours": 1.5,
    "convergence": true
  },
  
  "metrics": {
    "train_loss_final": 2.34,
    "valid_loss_best": 2.89,
    "test_bleu_4": 15.67,
    "test_loss": 2.92
  },
  
  "model_info": {
    "total_parameters": 12500000,
    "trainable_parameters": 12500000,
    "model_size_mb": 47.6
  },
  
  "inference": {
    "avg_inference_time_ms": 45,
    "sentences_per_second": 22
  },
  
  "samples": [
    {
      "source": "Hello world",
      "reference": "ä½ å¥½ä¸–ç•Œ",
      "prediction": "ä½ å¥½ ä¸–ç•Œ",
      "bleu": 85.6
    }
  ]
}
```

---

### å®éªŒå¯¹æ¯”å·¥å…·

```python
# scripts/compare_experiments.py
import json
import pandas as pd
import matplotlib.pyplot as plt

def compare_experiments(exp_ids):
    """å¯¹æ¯”å¤šä¸ªå®éªŒçš„ç»“æœ"""
    results = []
    
    for exp_id in exp_ids:
        metrics_path = f"experiments/{exp_id}/results/metrics.json"
        with open(metrics_path, 'r') as f:
            data = json.load(f)
            results.append({
                'Experiment': data['experiment_name'],
                'BLEU-4': data['metrics']['test_bleu_4'],
                'Training Time (h)': data['training']['training_time_hours'],
                'Parameters (M)': data['model_info']['total_parameters'] / 1e6,
                'Inference (ms)': data['inference']['avg_inference_time_ms']
            })
    
    df = pd.DataFrame(results)
    print(df.to_string(index=False))
    
    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    axes[0, 0].bar(df['Experiment'], df['BLEU-4'])
    axes[0, 0].set_title('BLEU-4 Score')
    axes[0, 0].set_ylabel('BLEU')
    
    axes[0, 1].bar(df['Experiment'], df['Training Time (h)'])
    axes[0, 1].set_title('Training Time')
    axes[0, 1].set_ylabel('Hours')
    
    axes[1, 0].bar(df['Experiment'], df['Parameters (M)'])
    axes[1, 0].set_title('Model Size')
    axes[1, 0].set_ylabel('Parameters (M)')
    
    axes[1, 1].bar(df['Experiment'], df['Inference (ms)'])
    axes[1, 1].set_title('Inference Speed')
    axes[1, 1].set_ylabel('ms/sentence')
    
    plt.tight_layout()
    plt.savefig('experiments/comparison.png', dpi=300)
    print("Comparison chart saved to: experiments/comparison.png")

if __name__ == '__main__':
    # å¯¹æ¯”æ‰€æœ‰å®Œæˆçš„å®éªŒ
    exp_ids = [
        'exp_001_rnn_baseline',
        'exp_002_transformer_baseline',
        'exp_003_rnn_dot_vs_additive'
    ]
    compare_experiments(exp_ids)
```

---

### å®éªŒæ€»ç»“æ–‡æ¡£ï¼ˆexperiments/summary.mdï¼‰

```markdown
# å®éªŒæ€»ç»“æŠ¥å‘Š

## å®éªŒæ¦‚è§ˆ

| ID | å®éªŒåç§° | çŠ¶æ€ | BLEU-4 | è®­ç»ƒæ—¶é—´ |
|----|---------|------|--------|---------|
| exp_001 | RNN Baseline | âœ… å®Œæˆ | 15.67 | 1.5h |
| exp_002 | Transformer Baseline | âœ… å®Œæˆ | 18.23 | 2.1h |
| exp_003 | RNN Attention Comparison | âœ… å®Œæˆ | - | 1.8h |
| exp_004 | Transformer Ablation | â³ è¿›è¡Œä¸­ | - | - |

## æ ¸å¿ƒå‘ç°

### 1. RNN vs Transformer
- Transformerçš„BLEUåˆ†æ•°æ¯”RNNé«˜2.56åˆ†
- ä½†è®­ç»ƒæ—¶é—´å¤š40%
- RNNæ¨ç†é€Ÿåº¦æ›´å¿«ï¼ˆ45ms vs 67msï¼‰

### 2. æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”
- ç‚¹ç§¯æ³¨æ„åŠ›ï¼šBLEU 15.67
- åŠ æ€§æ³¨æ„åŠ›ï¼šBLEU 15.82ï¼ˆç•¥ä¼˜ï¼‰
- å·®å¼‚ä¸å¤§ï¼Œå¯èƒ½å› ä¸ºæ¨¡å‹è§„æ¨¡è¾ƒå°

### 3. ä½ç½®ç¼–ç æ¶ˆè
- æœ‰ä½ç½®ç¼–ç ï¼šBLEU 18.23
- æ— ä½ç½®ç¼–ç ï¼šBLEU 12.45
- ä½ç½®ç¼–ç å¯¹Transformerè‡³å…³é‡è¦

## ç¿»è¯‘æ ·ä¾‹å¯¹æ¯”

[æ’å…¥å¯¹æ¯”è¡¨æ ¼]

## ç»“è®ºä¸å»ºè®®

[æ€»ç»“å…³é”®å‘ç°]
```

---

## ğŸ”¬ å®éªŒå¯¹æ¯”è®¡åˆ’

### P0 - å¿…é¡»å®Œæˆçš„å®éªŒï¼ˆç¡®ä¿æäº¤ï¼‰

#### å®éªŒ1ï¼šRNNåŸºçº¿æ¨¡å‹
**å®éªŒID**ï¼š`exp_001_rnn_baseline`  
**ç›®æ ‡**ï¼šå®ç°å¹¶è®­ç»ƒåŸºç¡€RNNç¿»è¯‘æ¨¡å‹  
**é…ç½®**ï¼š
- æ¨¡å‹ï¼šå•å‘LSTMï¼ˆ2å±‚ï¼‰+ ç‚¹ç§¯æ³¨æ„åŠ›
- æ•°æ®ï¼štrain_10k.jsonl
- è®­ç»ƒï¼š15 epochsï¼Œbatch_size=64
- è§£ç ï¼šè´ªå©ªè§£ç 

**äº§å‡º**ï¼š
- è®­ç»ƒå¥½çš„æ¨¡å‹checkpoint
- BLEU-4åˆ†æ•°
- 10ä¸ªç¿»è¯‘æ ·ä¾‹
- è®­ç»ƒæ›²çº¿å›¾

**æ‰§è¡Œå‘½ä»¤**ï¼š
```bash
python scripts/run_experiment.py --config experiments/exp_001_rnn_baseline/config.yaml
```

---

#### å®éªŒ2ï¼šTransformeråŸºçº¿æ¨¡å‹
**å®éªŒID**ï¼š`exp_002_transformer_baseline`  
**ç›®æ ‡**ï¼šä½¿ç”¨PyTorchå†…ç½®Transformerå®ç°ç¿»è¯‘  
**é…ç½®**ï¼š
- æ¨¡å‹ï¼šTransformerï¼ˆ3å±‚encoder + 3å±‚decoderï¼‰
- ä½ç½®ç¼–ç ï¼šsin/cosç»å¯¹ä½ç½®ç¼–ç 
- æ•°æ®ï¼štrain_10k.jsonl
- è®­ç»ƒï¼š15 epochsï¼Œbatch_size=64

**äº§å‡º**ï¼š
- è®­ç»ƒå¥½çš„æ¨¡å‹checkpoint
- BLEU-4åˆ†æ•°
- 10ä¸ªç¿»è¯‘æ ·ä¾‹
- è®­ç»ƒæ›²çº¿å›¾

**æ‰§è¡Œå‘½ä»¤**ï¼š
```bash
python scripts/run_experiment.py --config experiments/exp_002_transformer_baseline/config.yaml
```

---

#### å®éªŒ3ï¼šRNN vs Transformerå¯¹æ¯”
**å®éªŒID**ï¼šåœ¨å®éªŒ1å’Œ2çš„åŸºç¡€ä¸Šè¿›è¡Œå¯¹æ¯”  
**å¯¹æ¯”ç»´åº¦**ï¼š

| æŒ‡æ ‡ | RNN | Transformer | å¯¹æ¯”åˆ†æ |
|------|-----|-------------|----------|
| è®­ç»ƒæ—¶é—´ | ? å°æ—¶ | ? å°æ—¶ | å“ªä¸ªæ›´å¿«ï¼Ÿ |
| BLEU-4 | ? | ? | ç¿»è¯‘è´¨é‡å·®å¼‚ |
| æ¨¡å‹å‚æ•°é‡ | ? M | ? M | æ¨¡å‹å¤æ‚åº¦ |
| æ¨ç†é€Ÿåº¦ | ? ms/å¥ | ? ms/å¥ | å®é™…åº”ç”¨æ•ˆç‡ |
| é•¿å¥è¡¨ç° | ? | ? | >50è¯çš„å¥å­BLEU |

**äº§å‡º**ï¼š
- å¯¹æ¯”è¡¨æ ¼ï¼ˆç”¨äºæŠ¥å‘Šï¼‰
- å¯è§†åŒ–å›¾è¡¨ï¼ˆ4ä¸ªå­å›¾ï¼‰
- é”™è¯¯æ¡ˆä¾‹åˆ†æï¼ˆå„5ä¸ªï¼‰

**æ‰§è¡Œå‘½ä»¤**ï¼š
```bash
python scripts/compare_experiments.py exp_001_rnn_baseline exp_002_transformer_baseline
```

---

### P1 - å°½é‡å®Œæˆçš„å®éªŒï¼ˆæå‡æŠ¥å‘Šè´¨é‡ï¼‰

#### å®éªŒ4ï¼šRNNæ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”
**å®éªŒID**ï¼š`exp_003_rnn_attention_comparison`  
**ç›®æ ‡**ï¼šå¯¹æ¯”ä¸åŒæ³¨æ„åŠ›æœºåˆ¶çš„æ•ˆæœ  
**å­å®éªŒ**ï¼š
- 4a. ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆdot-productï¼‰- å·²åœ¨exp_001å®Œæˆ
- 4b. åŠ æ€§æ³¨æ„åŠ›ï¼ˆadditive/Bahdanauï¼‰
- 4c. ä¹˜æ€§æ³¨æ„åŠ›ï¼ˆmultiplicativeï¼‰- å¯é€‰

**é…ç½®å·®å¼‚**ï¼šä»…ä¿®æ”¹`attention_type`å‚æ•°

**äº§å‡º**ï¼š
- 3ç§æ³¨æ„åŠ›çš„BLEUå¯¹æ¯”
- æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–ï¼ˆheatmapï¼‰
- åˆ†æï¼šå“ªç§æ³¨æ„åŠ›æ›´é€‚åˆä¸­è‹±ç¿»è¯‘

**é¢„æœŸç»“æœ**ï¼šå·®å¼‚å¯èƒ½ä¸å¤§ï¼ˆ<1 BLEUï¼‰ï¼Œä½†å±•ç¤ºäº†ç†è§£

---

#### å®éªŒ5ï¼šTeacher Forcingæ¯”ä¾‹å¯¹æ¯”
**å®éªŒID**ï¼š`exp_004_rnn_teacher_forcing`  
**ç›®æ ‡**ï¼šç ”ç©¶Teacher Forcingå¯¹è®­ç»ƒçš„å½±å“  
**å­å®éªŒ**ï¼š
- 5a. TF ratio = 1.0ï¼ˆå§‹ç»ˆä½¿ç”¨çœŸå®æ ‡ç­¾ï¼‰
- 5b. TF ratio = 0.5ï¼ˆ50%æ¦‚ç‡ä½¿ç”¨çœŸå®æ ‡ç­¾ï¼‰
- 5c. TF ratio = 0.0ï¼ˆFree Runningï¼Œå®Œå…¨ä½¿ç”¨é¢„æµ‹ï¼‰

**äº§å‡º**ï¼š
- è®­ç»ƒç¨³å®šæ€§å¯¹æ¯”ï¼ˆlossæ›²çº¿ï¼‰
- æœ€ç»ˆBLEUå¯¹æ¯”
- åˆ†æï¼šTFå¯¹æ”¶æ•›é€Ÿåº¦å’Œæ³›åŒ–çš„å½±å“

---

### P2 - æ—¶é—´å…è®¸å†åšï¼ˆé”¦ä¸Šæ·»èŠ±ï¼‰

#### å®éªŒ6ï¼šTransformerä½ç½®ç¼–ç æ¶ˆè
**å®éªŒID**ï¼š`exp_005_transformer_ablation`  
**ç›®æ ‡**ï¼šéªŒè¯ä½ç½®ç¼–ç çš„é‡è¦æ€§  
**å­å®éªŒ**ï¼š
- 6a. æ ‡å‡†sin/cosä½ç½®ç¼–ç ï¼ˆå·²åœ¨exp_002å®Œæˆï¼‰
- 6b. ä¸ä½¿ç”¨ä½ç½®ç¼–ç 

**äº§å‡º**ï¼š
- BLEUå¯¹æ¯”ï¼ˆé¢„æœŸå·®è·æ˜¾è‘—ï¼‰
- è¯æ˜ä½ç½®ç¼–ç å¯¹Transformerçš„å¿…è¦æ€§

---

#### å®éªŒ7ï¼šæŸæœç´¢ vs è´ªå©ªè§£ç 
**å®éªŒID**ï¼š`exp_006_decoding_strategy`  
**ç›®æ ‡**ï¼šå¯¹æ¯”ä¸åŒè§£ç ç­–ç•¥  
**å­å®éªŒ**ï¼š
- 7a. è´ªå©ªè§£ç ï¼ˆbeam_size=1ï¼‰
- 7b. æŸæœç´¢ï¼ˆbeam_size=3ï¼‰
- 7c. æŸæœç´¢ï¼ˆbeam_size=5ï¼‰

**äº§å‡º**ï¼š
- BLEUæå‡å¹…åº¦
- æ¨ç†æ—¶é—´å¢åŠ 
- åˆ†æï¼šæ€§èƒ½ä¸æ•ˆç‡çš„æƒè¡¡

---

#### å®éªŒ8ï¼šå¤§æ•°æ®é›†è®­ç»ƒï¼ˆå¯é€‰ï¼‰
**å®éªŒID**ï¼š`exp_007_large_data`  
**ç›®æ ‡**ï¼šå¦‚æœæ—¶é—´å…è®¸ï¼Œä½¿ç”¨100kæ•°æ®é›†é‡æ–°è®­ç»ƒ  
**é¢„æœŸ**ï¼šBLEUå¯èƒ½æå‡3-5åˆ†

---

### å®éªŒä¼˜å…ˆçº§æ€»ç»“

**å¿…åšï¼ˆç¡®ä¿æŠ¥å‘Šå®Œæ•´ï¼‰**ï¼š
1. âœ… exp_001: RNNåŸºçº¿
2. âœ… exp_002: TransformeråŸºçº¿  
3. âœ… ä¸¤æ¨¡å‹å¯¹æ¯”åˆ†æ

**æ¨èåšï¼ˆæå‡æŠ¥å‘Šè´¨é‡ï¼‰**ï¼š
4. âœ… exp_003: æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”ï¼ˆRNNè¦æ±‚ï¼‰
5. â­• exp_004: Teacher Forcingå¯¹æ¯”ï¼ˆå¦‚æœæ—¶é—´å…è®¸ï¼‰

**å¯é€‰åšï¼ˆæ—¶é—´å……è£•æ‰è€ƒè™‘ï¼‰**ï¼š
6. â­• exp_005: ä½ç½®ç¼–ç æ¶ˆè
7. â­• exp_006: è§£ç ç­–ç•¥å¯¹æ¯”
8. â­• exp_007: å¤§æ•°æ®é›†è®­ç»ƒ

---

### å®éªŒæ—¶é—´è§„åˆ’

| å®éªŒ | é¢„è®¡æ—¶é—´ | ä½•æ—¶æ‰§è¡Œ |
|-----|---------|---------|
| exp_001 | 2å°æ—¶ | Day 1æ™šä¸Š |
| exp_002 | 2.5å°æ—¶ | Day 2ä¸Šåˆ |
| ä¸¤æ¨¡å‹å¯¹æ¯” | 1å°æ—¶ | Day 2ä¸‹åˆ |
| exp_003 | 2å°æ—¶ | Day 2ä¸‹åˆï¼ˆå¦‚æœ‰æ—¶é—´ï¼‰|
| å…¶ä»–å®éªŒ | - | è§†æƒ…å†µè€Œå®š |

---

## ğŸ“ æŠ¥å‘Šç»“æ„ï¼ˆ10-15é¡µï¼‰

### æŠ¥å‘Šå¤§çº²
```
ç¬¬1é¡µï¼šå°é¢
  - è¯¾ç¨‹åç§°ã€é¡¹ç›®æ ‡é¢˜
  - å­¦å·ã€å§“å
  - GitHubä»“åº“URL âš ï¸ å¿…é¡»

ç¬¬2é¡µï¼šæ‘˜è¦ï¼ˆ0.5é¡µï¼‰
  - é¡¹ç›®ç›®æ ‡
  - ä½¿ç”¨çš„æ¨¡å‹å’Œæ–¹æ³•
  - ä¸»è¦ç»“è®ºï¼ˆ3-5æ¡ï¼‰

ç¬¬3-4é¡µï¼šæ•°æ®é›†ä¸é¢„å¤„ç†ï¼ˆ1.5é¡µï¼‰
  - æ•°æ®é›†ç»Ÿè®¡
  - é¢„å¤„ç†æµç¨‹å›¾
  - è¯è¡¨å¤§å°ã€å¹³å‡å¥é•¿ç­‰ç»Ÿè®¡

ç¬¬5-7é¡µï¼šæ¨¡å‹æ¶æ„ï¼ˆ3é¡µï¼‰
  - RNNæ¶æ„å›¾ + è¯´æ˜ï¼ˆ1é¡µï¼‰
  - Transformeræ¶æ„å›¾ + è¯´æ˜ï¼ˆ1é¡µï¼‰
  - å…³é”®ä»£ç ç‰‡æ®µï¼ˆ0.5é¡µï¼‰
  - è¶…å‚æ•°è¡¨æ ¼ï¼ˆ0.5é¡µï¼‰

ç¬¬8-9é¡µï¼šå®ç°è¿‡ç¨‹ï¼ˆ1.5é¡µï¼‰
  - æŠ€æœ¯é€‰å‹
  - é‡åˆ°çš„ä¸»è¦é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
  - è®­ç»ƒç­–ç•¥

ç¬¬10-12é¡µï¼šå®éªŒç»“æœï¼ˆ3é¡µï¼‰
  - è®­ç»ƒæ›²çº¿å¯¹æ¯”å›¾ï¼ˆLoss/BLEUéšepochå˜åŒ–ï¼‰
  - BLEUå¯¹æ¯”è¡¨æ ¼
  - ç¿»è¯‘æ ·ä¾‹å±•ç¤ºï¼ˆä¸­è¯‘è‹±ã€è‹±è¯‘ä¸­å„5ä¸ªï¼‰
  - æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”ï¼ˆå¦‚å®Œæˆï¼‰
  - é”™è¯¯æ¡ˆä¾‹åˆ†æ

ç¬¬13é¡µï¼šåˆ†æä¸è®¨è®ºï¼ˆ1é¡µï¼‰
  - RNN vs Transformerä¼˜åŠ£å¯¹æ¯”è¡¨
  - è®­ç»ƒæ•ˆç‡åˆ†æ
  - ç¿»è¯‘è´¨é‡åˆ†æ
  - é•¿å¥å¤„ç†èƒ½åŠ›

ç¬¬14é¡µï¼šä¸ªäººåæ€ï¼ˆ1é¡µï¼‰
  - å­¦åˆ°çš„çŸ¥è¯†å’ŒæŠ€èƒ½
  - é‡åˆ°çš„æœ€å¤§æŒ‘æˆ˜
  - å¯æ”¹è¿›çš„æ–¹å‘
  - å¿ƒå¾—ä½“ä¼š

ç¬¬15é¡µï¼šå‚è€ƒæ–‡çŒ®
```

### æŠ¥å‘Šåˆ¶ä½œå»ºè®®
1. **ä½¿ç”¨LaTeXæˆ–Wordæ¨¡æ¿**ï¼šç¡®ä¿æ ¼å¼ä¸“ä¸š
2. **å›¾è¡¨æ¸…æ™°**ï¼šä½¿ç”¨matplotlibç”Ÿæˆé«˜è´¨é‡å›¾è¡¨
3. **ä»£ç ç‰‡æ®µ**ï¼šé€‰æ‹©æœ€å…³é”®çš„ä»£ç ï¼Œæ·»åŠ æ³¨é‡Š
4. **è¯šå®æ±‡æŠ¥**ï¼šå³ä½¿BLEUä¸é«˜ï¼Œä¹Ÿè¦åˆ†æåŸå› 
5. **çªå‡ºæ€è€ƒ**ï¼šä½“ç°å¯¹æ¨¡å‹åŸç†çš„ç†è§£

---

## ğŸ“¦ ä¾èµ–åŒ…æ¸…å•ï¼ˆrequirements.txtï¼‰

```txt
# æ·±åº¦å­¦ä¹ æ¡†æ¶
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0

# Hugging Faceï¼ˆç”¨äºé¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯é€‰ï¼‰
transformers>=4.30.0

# åˆ†è¯å·¥å…·
jieba>=0.42.1
nltk>=3.8

# è¯„ä¼°å·¥å…·
sacrebleu>=2.3.0

# æ•°æ®å¤„ç†
numpy>=1.24.0
pandas>=2.0.0

# å¯è§†åŒ–
matplotlib>=3.7.0
seaborn>=0.12.0

# å·¥å…·åº“
tqdm>=4.65.0
tensorboard>=2.13.0

# å…¶ä»–
scikit-learn>=1.3.0
```

**å®‰è£…å‘½ä»¤**ï¼š
```bash
pip install -r requirements.txt
python -c "import nltk; nltk.download('punkt')"
```

---

## âœ… æ£€æŸ¥æ¸…å•ï¼ˆç¡®ä¿å®Œæˆï¼‰

### ä»£ç éƒ¨åˆ†
- [ ] `src/data_utils.py` - æ•°æ®å¤„ç†å·¥å…·
- [ ] `src/models/rnn_seq2seq.py` - RNNæ¨¡å‹
- [ ] `src/models/transformer.py` - Transformeræ¨¡å‹
- [ ] `src/train_rnn.py` - RNNè®­ç»ƒè„šæœ¬
- [ ] `src/train_transformer.py` - Transformerè®­ç»ƒè„šæœ¬
- [ ] `src/evaluate.py` - è¯„ä¼°è„šæœ¬
- [ ] `inference.py` - ğŸ”¥ ä¸€é”®æ¨ç†è„šæœ¬ï¼ˆå¿…éœ€ï¼‰
- [ ] `requirements.txt` - ä¾èµ–åŒ…
- [ ] `README.md` - é¡¹ç›®è¯´æ˜

### æ¨¡å‹éƒ¨åˆ†
- [ ] RNNæ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆè‡³å°‘10ä¸ªepochï¼‰
- [ ] Transformeræ¨¡å‹è®­ç»ƒå®Œæˆï¼ˆè‡³å°‘10ä¸ªepochï¼‰
- [ ] ä¿å­˜æœ€ä½³checkpointï¼ˆ`checkpoints/`ç›®å½•ï¼‰
- [ ] åœ¨test.jsonlä¸Šè¯„ä¼°BLEU

### å®éªŒéƒ¨åˆ†
- [ ] RNN vs Transformerå¯¹æ¯”æ•°æ®
- [ ] è‡³å°‘ä¸€ä¸ªå¯¹æ¯”å®éªŒï¼ˆæ³¨æ„åŠ›æœºåˆ¶æˆ–å…¶ä»–ï¼‰
- [ ] ç¿»è¯‘æ ·ä¾‹ï¼ˆ10ä¸ªä»¥ä¸Šï¼‰
- [ ] è®­ç»ƒæ›²çº¿å›¾

### æŠ¥å‘Šéƒ¨åˆ†
- [ ] æŠ¥å‘ŠPDFå®Œæˆï¼ˆ10é¡µä»¥ä¸Šï¼‰
- [ ] æ–‡ä»¶å‘½åæ­£ç¡®ï¼šå­¦å·_å§“å.pdf
- [ ] é¦–é¡µåŒ…å«GitHubä»“åº“URL
- [ ] åŒ…å«æ‰€æœ‰å¿…éœ€ç« èŠ‚

### Gitä»“åº“
- [ ] ä»£ç å·²ä¸Šä¼ GitHub
- [ ] READMEå†™æ¸…æ¥šè¿è¡Œæ–¹æ³•
- [ ] .gitignoreé…ç½®æ­£ç¡®ï¼ˆæ’é™¤æ•°æ®é›†ã€æ¨¡å‹æƒé‡ï¼‰
- [ ] ä»“åº“è®¾ç½®ä¸ºpublicï¼ˆæˆ–æä¾›è®¿é—®æƒé™ï¼‰

---

## âš ï¸ å…³é”®é£é™©ä¸åº”å¯¹

### é£é™©1ï¼šè®­ç»ƒæ—¶é—´è¿‡é•¿
**åº”å¯¹**ï¼š
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆhidden_dim=128ï¼‰
- å‡å°‘å±‚æ•°ï¼ˆn_layers=1ï¼‰
- å‡å°‘epochï¼ˆ5-10ä¸ªepochè¶³å¤Ÿï¼‰
- åªä½¿ç”¨train_10kæ•°æ®é›†

### é£é™©2ï¼šæ˜¾å­˜ä¸è¶³ï¼ˆGPU OOMï¼‰
**åº”å¯¹**ï¼š
- å‡å°batch_sizeï¼ˆä»64é™åˆ°32æˆ–16ï¼‰
- å‡å°max_lenï¼ˆä»100é™åˆ°50ï¼‰
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼ˆaccumulate gradientsï¼‰
- CPUè®­ç»ƒï¼ˆæ…¢ä½†å¯è¡Œï¼‰

### é£é™©3ï¼šBLEUåˆ†æ•°å¾ˆä½
**åº”å¯¹**ï¼š
- **ä¸è¦æ…Œï¼** ä½œä¸šè¦æ±‚æ˜ç¡®è¯´è¯„åˆ†ä¸ä¾èµ–BLEUé«˜ä½
- åœ¨æŠ¥å‘Šä¸­è¯šå®æ±‡æŠ¥ï¼Œåˆ†æå¯èƒ½åŸå› ï¼š
  - è®­ç»ƒæ•°æ®å°‘ã€è®­ç»ƒæ—¶é—´çŸ­
  - æ¨¡å‹è§„æ¨¡å°ã€è¶…å‚æ•°æœªå……åˆ†è°ƒä¼˜
  - ä¸­æ–‡åˆ†è¯è´¨é‡ã€è¯è¡¨å¤§å°é™åˆ¶
- å±•ç¤ºæ¨¡å‹ç¡®å®åœ¨å­¦ä¹ ï¼ˆlossä¸‹é™æ›²çº¿ï¼‰
- æå‡ºæ”¹è¿›æ–¹å‘

### é£é™©4ï¼šä»£ç æœ‰bugï¼Œæ— æ³•è¿è¡Œ
**åº”å¯¹**ï¼š
- æ¯å®Œæˆä¸€ä¸ªæ¨¡å—ç«‹å³æµ‹è¯•
- ä½¿ç”¨å°æ•°æ®ï¼ˆ3-5ä¸ªæ ·æœ¬ï¼‰å…ˆè¿‡æ‹Ÿåˆ
- æ‰“å°ä¸­é—´tensorçš„shape
- å‚è€ƒPyTorchå®˜æ–¹æ•™ç¨‹ä»£ç 

### é£é™©5ï¼šæ—¶é—´ä¸å¤Ÿ
**åº”å¯¹**ï¼š
- **ç«‹å³å¯åŠ¨æœ€å°å¯è¡Œæ–¹æ¡ˆï¼ˆMVPï¼‰**
- æ”¾å¼ƒæ‰€æœ‰P2å®éªŒï¼ŒåªåšP0
- æ”¾å¼ƒé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒ
- æŠ¥å‘Šç®€åŒ–ï¼Œç¡®ä¿ç»“æ„å®Œæ•´

---

## ğŸš€ ç«‹å³è¡ŒåŠ¨è®¡åˆ’ï¼ˆä»Šæ™šå¿…åšï¼‰

### ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒæ­å»ºï¼ˆ20:00-21:00ï¼‰

```bash
# 1. åˆ›å»ºé¡¹ç›®ç»“æ„
cd /mnt/afs/250010036/course/NLP
mkdir -p src/models checkpoints results/figures data/vocab docs

# 2. åˆ›å»ºrequirements.txt
cat > requirements.txt << 'EOF'
torch>=2.0.0
jieba>=0.42.1
nltk>=3.8
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
tqdm>=4.65.0
sacrebleu>=2.3.0
EOF

# 3. å®‰è£…ä¾èµ–ï¼ˆå¦‚æœç¯å¢ƒæœªé…ç½®ï¼‰
# pip install -r requirements.txt
# python -c "import nltk; nltk.download('punkt')"

# 4. åˆå§‹åŒ–Gitï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
git init
cat > .gitignore << 'EOF'
__pycache__/
*.pyc
*.pth
*.pt
checkpoints/
data/processed/
*.log
.ipynb_checkpoints/
EOF

git add .
git commit -m "Initial project structure"
```

### ç¬¬äºŒæ­¥ï¼šæ•°æ®é¢„å¤„ç†ï¼ˆ21:00-22:30ï¼‰

åˆ›å»º `src/data_utils.py`ï¼Œå®ç°ï¼š
1. åŠ è½½JSONLæ•°æ®
2. ä¸­è‹±æ–‡åˆ†è¯
3. æ„å»ºè¯è¡¨
4. TranslationDatasetç±»

**éªŒè¯**ï¼šèƒ½å¤ŸæˆåŠŸåŠ è½½ä¸€ä¸ªbatchæ•°æ®å¹¶æ‰“å°shape

### ç¬¬ä¸‰æ­¥ï¼šRNNæ¨¡å‹ï¼ˆ22:30-01:00ï¼‰

åˆ›å»º `src/models/rnn_seq2seq.py`ï¼Œå®ç°ï¼š
1. Encoder
2. Attention
3. Decoder
4. Seq2Seqå®Œæ•´æ¨¡å‹

**éªŒè¯**ï¼šéšæœºè¾“å…¥å¯ä»¥forwardï¼Œè¾“å‡ºshapeæ­£ç¡®

### ç¬¬å››æ­¥ï¼šå¼€å§‹è®­ç»ƒï¼ˆ01:00-02:00ï¼‰

åˆ›å»º `src/train_rnn.py`ï¼Œå¼€å§‹è®­ç»ƒ
- è®¾ç½®åå°è®­ç»ƒï¼ˆnohupæˆ–tmuxï¼‰
- ç¡è§‰å‰å¯åŠ¨è®­ç»ƒï¼Œæ˜æ—©æ£€æŸ¥

---

## ğŸ¯ æˆåŠŸæ ‡å‡†

### æœ€ä½æ ‡å‡†ï¼ˆåŠæ ¼çº¿ï¼‰
- âœ… RNNå’ŒTransformerä¸¤ä¸ªæ¨¡å‹éƒ½èƒ½è®­ç»ƒå¹¶äº§å‡ºç¿»è¯‘
- âœ… inference.pyèƒ½å¤Ÿè¿è¡Œ
- âœ… æœ‰BLEUè¯„ä¼°æ•°æ®ï¼ˆå³ä½¿ä¸é«˜ï¼‰
- âœ… æŠ¥å‘Šç»“æ„å®Œæ•´ï¼Œ10é¡µä»¥ä¸Š
- âœ… GitHubä»“åº“å¯è®¿é—®

### ç›®æ ‡æ ‡å‡†ï¼ˆè‰¯å¥½ï¼‰
- âœ… è‡³å°‘å®Œæˆ1-2ä¸ªå¯¹æ¯”å®éªŒ
- âœ… ç¿»è¯‘è´¨é‡å°šå¯ï¼ˆBLEU > 5ï¼‰
- âœ… æŠ¥å‘Šåˆ†ææœ‰æ·±åº¦ï¼Œæœ‰å¯è§†åŒ–å›¾è¡¨
- âœ… ä»£ç è§„èŒƒï¼Œæœ‰æ³¨é‡Š

### ç†æƒ³æ ‡å‡†ï¼ˆä¼˜ç§€ï¼‰
- âœ… å®Œæˆ3ä¸ªä»¥ä¸Šå¯¹æ¯”å®éªŒ
- âœ… ç¿»è¯‘è´¨é‡è¾ƒå¥½ï¼ˆBLEU > 10ï¼‰
- âœ… æŠ¥å‘Šæœ‰æ·±åº¦è§è§£å’Œåˆ›æ–°æ€è€ƒ
- âœ… ä»£ç è´¨é‡é«˜ï¼Œæ¨¡å—åŒ–å¥½

---

## ğŸ“ ç´§æ€¥æ±‚åŠ©èµ„æº

1. **PyTorchå®˜æ–¹æ•™ç¨‹**ï¼šhttps://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html
2. **Annotated Transformer**ï¼šhttp://nlp.seas.harvard.edu/annotated-transformer/
3. **BLEUè®¡ç®—**ï¼šnltk.translate.bleu_score
4. **ChatGPT/Claude**ï¼šé‡åˆ°bugæ—¶å¿«é€Ÿè°ƒè¯•
5. **GitHubæœç´¢**ï¼šæœç´¢"pytorch seq2seq translation"æ‰¾å‚è€ƒä»£ç 

---

## ğŸ’ª æœ€åçš„è¯

**æ—¶é—´å¾ˆç´§ï¼Œä½†å®Œå…¨å¯è¡Œï¼**

å…³é”®åŸåˆ™ï¼š
1. **ä¸è¿½æ±‚å®Œç¾**ï¼šèƒ½è·‘é€šæœ€é‡è¦
2. **ä¼˜å…ˆæ ¸å¿ƒåŠŸèƒ½**ï¼šä¿è¯P0ä»»åŠ¡å®Œæˆ
3. **åŠæ—¶æäº¤Git**ï¼šé¿å…ä»£ç ä¸¢å¤±
4. **è¯šå®æ±‡æŠ¥**ï¼šæŠ¥å‘Šé‡è¿‡ç¨‹è½»ç»“æœ
5. **ä¿æŒå†·é™**ï¼šé‡åˆ°é—®é¢˜å¿«é€Ÿå¯»æ±‚è§£å†³æ–¹æ¡ˆ

**ç«‹å³å¼€å§‹æ‰§è¡Œï¼ç¥ä½ é¡ºåˆ©å®Œæˆä½œä¸šï¼ğŸš€**

---

---

## ğŸ“‹ é™„å½•ï¼šå¿«é€Ÿå¯åŠ¨æ¸…å•

### A. ç«‹å³åˆ›å»ºå®éªŒç›®å½•ç»“æ„

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•æ‰§è¡Œ
mkdir -p experiments/{exp_001_rnn_baseline,exp_002_transformer_baseline,exp_003_rnn_attention_comparison}/{checkpoints,logs,results/figures}

# åˆ›å»ºscriptsç›®å½•
mkdir -p scripts

echo "âœ… å®éªŒç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ"
```

### B. å®éªŒé…ç½®æ–‡ä»¶å¿«é€Ÿç”Ÿæˆ

```bash
# ç”Ÿæˆexp_001é…ç½®æ–‡ä»¶
cat > experiments/exp_001_rnn_baseline/config.yaml << 'EOF'
experiment:
  name: "RNN Baseline (Unidirectional)"
  id: "exp_001"
  description: "å•å‘LSTM + ç‚¹ç§¯æ³¨æ„åŠ›"
  date: "2025-12-26"

model:
  type: "rnn_seq2seq"
  embed_dim: 256
  hidden_dim: 256
  n_layers: 2
  dropout: 0.3
  attention_type: "dot"
  bidirectional: false

data:
  train_file: "AP0004_Midterm&Final_translation_dataset_zh_en/train_10k.jsonl"
  valid_file: "AP0004_Midterm&Final_translation_dataset_zh_en/valid.jsonl"
  test_file: "AP0004_Midterm&Final_translation_dataset_zh_en/test.jsonl"
  max_len: 100
  min_freq: 2

training:
  batch_size: 64
  learning_rate: 0.001
  epochs: 15
  grad_clip: 1.0
  teacher_forcing_ratio: 1.0

output:
  checkpoint_dir: "experiments/exp_001_rnn_baseline/checkpoints"
  log_dir: "experiments/exp_001_rnn_baseline/logs"
  results_dir: "experiments/exp_001_rnn_baseline/results"
EOF

# ç”Ÿæˆexp_002é…ç½®æ–‡ä»¶
cat > experiments/exp_002_transformer_baseline/config.yaml << 'EOF'
experiment:
  name: "Transformer Baseline"
  id: "exp_002"
  description: "PyTorchå†…ç½®Transformer + ä½ç½®ç¼–ç "
  date: "2025-12-26"

model:
  type: "transformer"
  d_model: 256
  nhead: 4
  num_encoder_layers: 3
  num_decoder_layers: 3
  dim_feedforward: 1024
  dropout: 0.1

data:
  train_file: "AP0004_Midterm&Final_translation_dataset_zh_en/train_10k.jsonl"
  valid_file: "AP0004_Midterm&Final_translation_dataset_zh_en/valid.jsonl"
  test_file: "AP0004_Midterm&Final_translation_dataset_zh_en/test.jsonl"
  max_len: 100
  min_freq: 2

training:
  batch_size: 64
  learning_rate: 0.0001
  epochs: 15
  grad_clip: 1.0

output:
  checkpoint_dir: "experiments/exp_002_transformer_baseline/checkpoints"
  log_dir: "experiments/exp_002_transformer_baseline/logs"
  results_dir: "experiments/exp_002_transformer_baseline/results"
EOF

echo "âœ… é…ç½®æ–‡ä»¶ç”Ÿæˆå®Œæˆ"
```

### C. æ ¸å¿ƒä»£ç æ–‡ä»¶æ¸…å•

**å¿…é¡»å®ç°çš„æ–‡ä»¶**ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š

```
1. src/data_utils.py          # P0 - æ•°æ®å¤„ç†ï¼ˆ2å°æ—¶ï¼‰
   â”œâ”€â”€ load_data()
   â”œâ”€â”€ Tokenizerç±»
   â”œâ”€â”€ build_vocab()
   â””â”€â”€ TranslationDataset

2. src/models/rnn_seq2seq.py  # P0 - RNNæ¨¡å‹ï¼ˆ3å°æ—¶ï¼‰
   â”œâ”€â”€ Encoderï¼ˆå•å‘LSTMï¼‰
   â”œâ”€â”€ Attentionï¼ˆdot/additiveï¼‰
   â”œâ”€â”€ Decoder
   â””â”€â”€ Seq2Seq

3. src/models/transformer.py  # P0 - Transformerï¼ˆ2å°æ—¶ï¼‰
   â”œâ”€â”€ PositionalEncoding
   â””â”€â”€ TransformerNMTï¼ˆä½¿ç”¨nn.Transformerï¼‰

4. src/train_rnn.py           # P0 - RNNè®­ç»ƒï¼ˆ1å°æ—¶ï¼‰
5. src/train_transformer.py   # P0 - Transformerè®­ç»ƒï¼ˆ1å°æ—¶ï¼‰
6. src/evaluate.py            # P0 - è¯„ä¼°ç³»ç»Ÿï¼ˆ1å°æ—¶ï¼‰
7. inference.py               # P0 - ä¸€é”®æ¨ç†ï¼ˆ1å°æ—¶ï¼‰

æ€»è®¡æ ¸å¿ƒå¼€å‘æ—¶é—´ï¼šçº¦11å°æ—¶
```

### D. æ¯æ—¥ä»»åŠ¡æ£€æŸ¥è¡¨

#### Day 1ï¼ˆä»Šæ™šï¼‰å®Œæˆåº¦æ£€æŸ¥
- [ ] requirements.txtåˆ›å»ºå¹¶å®‰è£…å®Œæˆ
- [ ] é¡¹ç›®ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ
- [ ] å®éªŒé…ç½®æ–‡ä»¶ç”Ÿæˆå®Œæˆ
- [ ] data_utils.pyå®ç°å¹¶æµ‹è¯•é€šè¿‡
- [ ] rnn_seq2seq.pyå®ç°å¹¶å¯ä»¥forward
- [ ] train_rnn.pyå¼€å§‹è®­ç»ƒï¼ˆåå°è¿è¡Œï¼‰
- [ ] Gitåˆå§‹åŒ–å¹¶é¦–æ¬¡æäº¤

#### Day 2ï¼ˆæ˜å¤©ï¼‰å®Œæˆåº¦æ£€æŸ¥
- [ ] RNNæ¨¡å‹è®­ç»ƒå®Œæˆï¼ŒBLEUè¯„ä¼°å®Œæˆ
- [ ] evaluate.pyå’Œinference.pyå®ç°å®Œæˆ
- [ ] transformer.pyå®ç°å¹¶æµ‹è¯•é€šè¿‡
- [ ] Transformerè®­ç»ƒå®Œæˆï¼ŒBLEUè¯„ä¼°å®Œæˆ
- [ ] ä¸¤æ¨¡å‹å¯¹æ¯”æ•°æ®æ”¶é›†å®Œæˆ
- [ ] è‡³å°‘1ä¸ªé¢å¤–å¯¹æ¯”å®éªŒå®Œæˆï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰
- [ ] æŠ¥å‘Šè‰ç¨¿å®Œæˆ80%
- [ ] ä»£ç ä¸Šä¼ GitHub

#### Day 3ï¼ˆæˆªæ­¢æ—¥ï¼‰å®Œæˆåº¦æ£€æŸ¥
- [ ] æŠ¥å‘Šå®Œæˆ100%ï¼ˆ10é¡µä»¥ä¸Šï¼‰
- [ ] æŠ¥å‘Šå‘½åæ­£ç¡®ï¼šå­¦å·_å§“å.pdf
- [ ] é¦–é¡µåŒ…å«GitHub URL
- [ ] inference.pyæµ‹è¯•é€šè¿‡
- [ ] README.mdå®Œå–„
- [ ] æ‰€æœ‰ä»£ç æäº¤Git
- [ ] **æäº¤åˆ°Piazza** âœ…

### E. å…³é”®æŠ€æœ¯å†³ç­–æ€»ç»“

| å†³ç­–ç‚¹ | é€‰æ‹© | åŸå›  |
|--------|------|------|
| RNNç¼–ç å™¨æ–¹å‘ | **å•å‘** | ç¬¦åˆä½œä¸šè¦æ±‚ |
| RNNå•å…ƒç±»å‹ | LSTMæˆ–GRU | éƒ½å¯ä»¥ï¼ŒLSTMæ›´å¸¸è§ |
| æ³¨æ„åŠ›æœºåˆ¶ | å…ˆdotï¼Œå†additive | å¾ªåºæ¸è¿› |
| Transformerå®ç° | **nn.Transformer** | èŠ‚çœæ—¶é—´ï¼Œç¨³å®šå¯é  |
| ä½ç½®ç¼–ç  | sin/cos | æ ‡å‡†å®ç° |
| è®­ç»ƒæ•°æ® | **train_10k** | å¿«é€Ÿè¿­ä»£ |
| è¯„ä¼°æŒ‡æ ‡ | BLEU-4 | ä½œä¸šè¦æ±‚ |
| è§£ç ç­–ç•¥ | å…ˆgreedyï¼Œå†beam | å…ˆä¿è¯èƒ½è·‘ |
| å®éªŒç®¡ç† | YAMLé…ç½® | ç»“æ„åŒ–ã€å¯å¤ç° |

### F. åº”æ€¥è”ç³»èµ„æº

1. **PyTorchæ–‡æ¡£**ï¼šhttps://pytorch.org/docs/stable/index.html
2. **Seq2Seqæ•™ç¨‹**ï¼šhttps://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html
3. **Transformeræ•™ç¨‹**ï¼šhttp://nlp.seas.harvard.edu/annotated-transformer/
4. **BLEUè®¡ç®—**ï¼šhttps://www.nltk.org/api/nltk.translate.bleu_score.html
5. **Jiebaåˆ†è¯**ï¼šhttps://github.com/fxsjy/jieba

---

## ğŸ¯ æœ€åçš„è¡ŒåŠ¨å‘¼å

**ç°åœ¨å°±å¼€å§‹æ‰§è¡Œï¼**

1. âœ… å¤åˆ¶ä¸Šè¿°bashå‘½ä»¤ï¼Œåˆ›å»ºç›®å½•ç»“æ„
2. âœ… ç”Ÿæˆå®éªŒé…ç½®æ–‡ä»¶
3. âœ… å®‰è£…ä¾èµ–åŒ…ï¼ˆpip install -r requirements.txtï¼‰
4. âœ… å¼€å§‹å®ç°data_utils.pyï¼ˆä»Šæ™šçš„æ ¸å¿ƒä»»åŠ¡ï¼‰

**è®°ä½**ï¼š
- â° æ—¶é—´ç´§è¿«ï¼Œä¸è¦è¿½æ±‚å®Œç¾
- ğŸ¯ ä¼˜å…ˆå®ŒæˆP0ä»»åŠ¡
- ğŸ“ åŠæ—¶æäº¤Gitï¼ˆæ¯å®Œæˆä¸€ä¸ªæ¨¡å—ï¼‰
- ğŸ’ª ä¿æŒå†·é™ï¼Œé—®é¢˜æ€»æœ‰è§£å†³æ–¹æ¡ˆ

**ç¥ä½ é¡ºåˆ©å®Œæˆä½œä¸šï¼ğŸš€**

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv2.0ï¼ˆä¿®è®¢ç‰ˆï¼‰  
**æœ€åæ›´æ–°**ï¼š2025-12-26 21:00  
**ä¿®è®¢å†…å®¹**ï¼šä¿®æ­£RNNæ–¹å‘ã€ç®€åŒ–Transformerå®ç°ã€æ–°å¢å®éªŒç®¡ç†ç³»ç»Ÿ

